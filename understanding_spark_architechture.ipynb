{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta import *\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    IntegerType,\n",
    "    LongType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    DateType,\n",
    "    DoubleType,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"learn_spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark Architechture\n",
    "\n",
    "Apache Spark is a distributed computing framework designed for processing large-scale data across a cluster of machines. It provides high-level APIs in Java, Scala, Python, and R, making it accessible to a wide range of users. Spark is known for its in-memory processing capabilities, fault tolerance, and ease of use.\n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/cluster-overview.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Points\n",
    "\n",
    "- **Driver Program**: The application starts with the driver program, which represents the user's Spark application. The driver program is responsible for creating a SparkContext that coordinates the execution of tasks.\n",
    "- **SparkContext**: The SparkContext is the entry point to the Spark cluster. It communicates with the cluster manager to acquire resources (executors) and to schedule tasks.\n",
    "- **Cluster Manager**: The cluster manager is responsible for managing resources across the cluster. It allocates resources to the Spark application and monitors their usage. Examples of cluster managers include `Apache Mesos, Hadoop YARN, and Spark's standalone cluster manager`.\n",
    "- **Executors**: `Executors are worker nodes` in the Spark cluster responsible for executing tasks. Each executor runs in its own Java Virtual Machine (JVM) and communicates with the driver program and the cluster manager. An executor is a Spark process responsible for executing tasks on a specific node in the cluster. Each executor is assigned a fixed number of cores and a certain amount of memory. The number of executors determines the level of parallelism at which Spark can process data.\n",
    "- **RDD (Resilient Distributed Dataset)**: RDD is the fundamental data structure in Spark. It represents a distributed collection of objects that can be processed in parallel. `RDDs are immutable and fault-tolerant`.\n",
    "- **Directed Acyclic Graph (DAG)**: The execution plan of a Spark application is represented as a DAG. The DAG is a sequence of transformations and actions that Spark performs on the data. `Each stage in the DAG consists of transformations and is executed as a set of tasks`.\n",
    "- **Task Scheduler**: The task scheduler, part of the SparkContext, is responsible for scheduling tasks on the executors. It takes into account the dependencies between tasks and aims to maximize parallelism.\n",
    "- **Shuffle**: `A shuffle is an expensive operation where data is redistributed across the cluster`. It occurs when there is a need to exchange data between different partitions of RDDs.\n",
    "- **Broadcast Variables**: Broadcast variables allow efficient distribution of read-only variables to all tasks in a Spark application. This reduces data transfer costs, especially for large variables like lookup tables.\n",
    "- **Caching and Persistence**: Spark allows users to persist (cache) intermediate data in memory across multiple stages. This caching improves performance for iterative algorithms or when the same data is used multiple times.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Executors\n",
    "\n",
    "An executor is a Spark process responsible for executing tasks on a specific node in the cluster. Each executor is assigned a fixed number of cores and a certain amount of memory. The number of executors determines the level of parallelism at which Spark can process data.\n",
    "\n",
    "Generally, Having more executors allows for better parallelism and resource utilization.\n",
    "Each executor can work on a subset of data independently, which can lead to increased processing speed.\n",
    "However, it’s important to strike a balance between the number of executors and the available cluster resources. If the number of executors is too high, it can lead to excessive memory usage and increased overhead due to task scheduling.\n",
    "\n",
    "**Advantages**:\n",
    "\n",
    "1. More executors provide increased parallelism and the ability to process data in parallel.\n",
    "2. Each executor can work on a subset of data independently, leading to improved processing speed.\n",
    "3. It allows for better resource utilization by distributing the workload across multiple executor processes.\n",
    "\n",
    "**Considerations**:\n",
    "\n",
    "1. Allocating too many executors can lead to excessive memory usage and increased overhead due to task scheduling.\n",
    "2. Inefficient executor allocation can result in the underutilization of cluster resources.\n",
    "3. The optimal number of executors depends on factors such as dataset size, computation complexity, and available cluster resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.executor.instances\", 8) # no longer works within spark application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Cores\n",
    "\n",
    "The number of cores refers to the total number of processing units available on the machines in your Spark cluster. It represents the parallelism level at which Spark can execute tasks. Each core can handle one concurrent task.\n",
    "\n",
    "**Increasing the number of cores allows**\n",
    "\n",
    "1. Spark to execute more tasks simultaneously, which can improve the overall throughput of your application.\n",
    "2. However, adding too many cores can also `introduce overhead due to task scheduling and inter-node communication`, especially if the cluster resources are limited.\n",
    "3. The optimal number of cores depends on factors such as the size of your dataset, the complexity of your computations, and the available cluster resources.\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "1. Increasing the number of cores allows for higher parallelism and the ability to execute more tasks simultaneously.\n",
    "2. More cores can lead to improved throughput and faster processing of data.\n",
    "3. It allows better utilization of available computational resources in the cluster.\n",
    "\n",
    "**Considerations**\n",
    "\n",
    "1. Adding too many cores without sufficient resources can lead to resource contention and performance degradation.\n",
    "2. Excessive parallelism can introduce overhead due to task scheduling and inter-node communication, impacting performance.\n",
    "3. The optimal number of cores depends on the size of the dataset, the complexity of computations, and available cluster resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.executor.cores\", 4) # no longer works within spark application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Cluster Hierarchy\n",
    "\n",
    "<img src =\"http://c1.staticflickr.com/9/8854/28256131573_ec8328799b_o.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node -> Worker Node:\n",
    "\n",
    "A node in the context of Spark typically refers to a machine in the cluster. A worker node is a node that runs Spark tasks.\n",
    "\n",
    "### Worker Node -> Executor:\n",
    "\n",
    "A worker node can host multiple executors, each running in its own JVM. The number of executors on a worker node is influenced by the cluster manager's configuration.\n",
    "\n",
    "### Executor -> Cores:\n",
    "\n",
    "An executor can have multiple cores, and each core can run a task concurrently. The number of cores per executor is a configuration parameter.\n",
    "\n",
    "### Executor -> Task:\n",
    "\n",
    "Tasks are executed within an executor. An executor runs multiple tasks concurrently, and `each task processes a partition of the data`.\n",
    "\n",
    "### Parallelism:\n",
    "\n",
    "Parallelism is determined by the number of tasks that can be executed concurrently. It is influenced by the number of partitions in RDDs, the number of cores per executor, and the number of executors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring Spark Number of Executors and its Cores\n",
    "\n",
    "Configuring the number of cores and executors in Apache Spark depends on several factors, including\n",
    "\n",
    "- The characteristics of your workload,\n",
    "- The available cluster resources, and\n",
    "- Specific requirements of your application.\n",
    "\n",
    "While there is no one-size-fits-all approach, here are some general guidelines to help you configure these parameters effectively:\n",
    "\n",
    "- **Number of executors** : The `number of executors should be equal to the number of cores on each node` in the cluster.\n",
    "  If there are more cores than nodes, then the number of executors should be equal to the number of nodes.\n",
    "- **Memory per executor**: The amount of `memory allocated to each executor should be based on the size of the data` that will be processed by that executor. It is important to leave some memory available for the operating system and other processes. A good starting point is to allocate 1GB of memory per executor.\n",
    "- **Number of partitions**: `The number of partitions used for shuffle operations should be equal to the number of executors`.\n",
    "\n",
    "Let’s try to understand how to decide on the Spark number of executors and cores to be configured in a cluster. For our better understanding Let’s say you have a Spark cluster with 16 nodes, each having 8 cores and 32 GB of memory and your dataset size is relatively large, around 1 TB, and you’re running complex computations on it.\n",
    "\n",
    "<img src = \"https://sparkbyexamples.com/ezoimgfmt/i0.wp.com/sparkbyexamples.com/wp-content/uploads/2023/05/Executor-vs-Cores.jpg?w=263&ssl=1&ezimgfmt=ng:webp/ngcb1\">\n",
    "\n",
    "Note: For the above cluster configuration we have:\n",
    "\n",
    "**Available Resources**\n",
    "\n",
    "- Total cores in the cluster = 16 nodes \\* 8 cores per node = 128 cores\n",
    "- Total memory in the cluster = 16 nodes \\* 32 GB per node = 512 GB\n",
    "\n",
    "**Workload Characteristics**\n",
    "Large dataset size and complex computations suggest that you need a high level of parallelism to efficiently process the data. Let’s assume that you want to allocate 80% of the available resources to Spark.\n",
    "Now let’s try to analyze the efficient way to decide Spark’s Number of Executors and Cores.\n",
    "\n",
    "## Tiny Executor Configuration\n",
    "\n",
    "One way of configuring Spark Executor and its core is setting minimal configuration for the executors and incrementing it based on the application performance.\n",
    "\n",
    "### Executor Memory and Cores per Executor (Considering having 1 core per executor)\n",
    "\n",
    "- Number of executors per node=8,\n",
    "- Executor-memory=32/8=4GB\n",
    "\n",
    "### Calculating the Number of Executors (To calculate the number of executors, divide the available memory by the executor memory)\n",
    "\n",
    "- Total memory available for Spark = 80% of 512 GB = 410 GB\n",
    "- Number of executors = Total memory available for Spark / Executor memory = 410 GB / 4 GB ≈ 102 executors\n",
    "- Number of executors per node = Total Number of Executors/ Number of Nodes = 102/16 ≈ 6 Executors/Node\n",
    "\n",
    "So, in this example, you would configure Spark with 102 executors, each executor having 1 core and 4 GB of memory.\n",
    "\n",
    "<img src = \"https://sparkbyexamples.com/ezoimgfmt/i0.wp.com/sparkbyexamples.com/wp-content/uploads/2023/05/Executor-vs-Cores-1.jpg?w=561&ssl=1&ezimgfmt=ng:webp/ngcb1\">\n",
    "\n",
    "### Pros of Spark Tiny Executor Configuration:\n",
    "\n",
    "- **Resource Efficiency** Tiny executors consume less memory and fewer CPU cores compared to larger configurations.\n",
    "- **Increased Task Isolation** With tiny executors, each task runs in a more isolated environment. This isolation can prevent interference between tasks, reducing the chances of resource contention and improving the stability of your Spark application.\n",
    "- **Task Granularity** Tiny executor configurations can be beneficial if your workload consists of a large number of small tasks. With smaller executors, Spark can allocate resources more precisely, ensuring that each task receives sufficient resources without excessive overprovisioning.\n",
    "\n",
    "### Cons of Spark Tiny Executor Configuration:\n",
    "\n",
    "- **Increased Overhead** Using tiny executors can introduce higher overhead due to the increased number of executor processes and task scheduling.\n",
    "- **Limited Parallelism** Tiny executors have fewer cores, limiting the level of parallelism in your Spark application.\n",
    "- **Potential Bottlenecks** In a tiny executor configuration, if a single task takes longer to execute than others, it can become a bottleneck for the entire application.\n",
    "- **Memory Overhead** Although tiny executors consume less memory individually, the overhead of multiple executor processes can add up. This can lead to increased memory usage for managing the executor processes, potentially reducing the available memory for actual data processing.\n",
    "\n",
    "## Fat Executor Configuration\n",
    "\n",
    "The other way of configuring Spark Executor and its core is setting the maximum utility configuration i.e. having only one Executor per node and optimizing it based on the application performance.\n",
    "\n",
    "### Executor Memory and Cores per Executor: Considering having 8 cores per executor,\n",
    "\n",
    "- Number of executors per node= number of cores for a node/ number of cores for an executor = 8/8 = 1,\n",
    "- Executor-memory=32/1= 32GB\n",
    "\n",
    "### Calculating the Number of Executors: To calculate the number of executors, divide the available memory by the executor memory:\n",
    "\n",
    "- Total memory available for Spark = 80% of 512 GB = 410 GB\n",
    "- Number of executors = Total memory available for Spark / Executor memory = 410 GB / 32 GB ≈ 12 executors\n",
    "- Number of executors per node = Total Number of Executors/ Number of Nodes = 12/16 ≈ 1 Executors/Node\n",
    "\n",
    "So, in this example, you would configure Spark with 16 executors, each executor having 8 core and 32 GB of memory.\n",
    "\n",
    "<img src = \"https://sparkbyexamples.com/ezoimgfmt/i0.wp.com/sparkbyexamples.com/wp-content/uploads/2023/05/Executor-vs-Cores-1.jpg?w=561&ssl=1&ezimgfmt=ng:webp/ngcb1\">\n",
    "\n",
    "### Pros of Fat Executor Configuration\n",
    "\n",
    "- **Increased Parallelism** Fat executor configurations allocate more CPU cores and memory to each executor, resulting in improved processing speed and throughput.\n",
    "- **Reduced Overhead** With fewer executor processes to manage, a fat executor configuration can reduce the overhead of task scheduling, inter-node communication, and executor coordination. This can lead to improved overall performance and resource utilization.\n",
    "- **Enhanced Data Locality** `Larger executor memory sizes can accommodate more data partitions in memory`, reducing the need for data shuffling across the cluster.\n",
    "- **Improved Performance for Complex Tasks** By allocating more resources to each executor, you can efficiently handle complex computations and large-scale data processing.\n",
    "\n",
    "### Cons of Fat Executor Configuration\n",
    "\n",
    "- **Resource Overallocation** Using fat executors can result in overallocation of resources, especially if the cluster does not have sufficient memory or CPU cores.\n",
    "- **Reduced Task Isolation** With larger executor configurations, tasks have fewer executor processes to run on. This can increase the chances of resource contention and interference between tasks, potentially impacting the stability and performance of your Spark application.\n",
    "- **Longer Startup Times** Fat executor configurations require more resources and may have longer startup times compared to smaller configurations.\n",
    "- **Difficulty in Resource Sharing** Fat executors may not be efficient when sharing resources with other applications or services running on the same cluster. It can limit the flexibility of resource allocation and hinder the ability to run multiple applications concurrently.\n",
    "\n",
    "## Balanced Executor Configuration\n",
    "\n",
    "Spark founder Databricks after several trail and error testing the spark Executor and cores configuration, they recommends to have 2-5 cores per executor as the best initial efficient configuration for running the application smoothly.\n",
    "\n",
    "### Executor Memory and Cores per Executor: Considering having 3 cores per executor, Leaving 1 core per node for daemon processes\n",
    "\n",
    "- Number of executors per node= (number of cores for a node – core for daemon process)/ number of cores for an executor = 7/3 ≈ 2,\n",
    "- Executor-memory=Total memory per node/ number executors per node = 32/2= 16GB\n",
    "\n",
    "### Calculating the Number of Executors: To calculate the number of executors, divide the available memory by the executor memory:\n",
    "\n",
    "- Total memory available for Spark = 80% of 512 GB = 410 GB\n",
    "- Number of executors = Total memory available for Spark / Executor memory = 410 GB / 16 GB ≈ 32 executors\n",
    "- Number of executors per node = Total Number of Executors/ Number of Nodes = 32/16 = 2 Executors/Node\n",
    "\n",
    "<img src = \"https://sparkbyexamples.com/ezoimgfmt/i0.wp.com/sparkbyexamples.com/wp-content/uploads/2023/05/Executor-vs-Cores-2.jpg?w=561&ssl=1&ezimgfmt=ng:webp/ngcb1\">\n",
    "\n",
    "So, in this example, you would configure Spark with 32 executors, each executor having 3 core and 16 GB of memory.\n",
    "\n",
    "In practice, one size does not fit all. You need to keep tuning as per cluster configuration. `But in general, the number of executor cores should be 2-5.`\n",
    "\n",
    "### Pros of Balanced Executor Configuration:\n",
    "\n",
    "- **Optimal Resource Utilization** A balanced executor configuration aims to evenly distribute resources across the cluster. This allows for efficient utilization of both CPU cores and memory, maximizing the overall performance of your Spark application.\n",
    "- **Reasonable Parallelism** By allocating a moderate number of cores and memory to each executor, a balanced configuration strikes a balance between parallelism and resource efficiency. It can provide a good compromise between the high parallelism of small executors and the resource consumption of large executors.\n",
    "- **Flexibility for Multiple Workloads** A balanced configuration allows for accommodating a variety of workloads. It can handle both small and large datasets, as well as diverse computational requirements, making it suitable for environments where multiple applications or different stages of data processing coexist.\n",
    "- **Reduced Overhead** Compared to larger executor configurations, a balanced configuration typically involves fewer executor processes. This can reduce the overhead of task scheduling, inter-node communication, and executor coordination, leading to improved performance and lower resource consumption.\n",
    "\n",
    "### Cons of Balanced Executor Configuration:\n",
    "\n",
    "- **Limited Scaling** A balanced executor configuration may not scale as effectively as configurations with a higher number of cores or executors. In scenarios where the workload or dataset size significantly increases, a balanced configuration may reach its limit, potentially leading to longer processing times or resource contention.\n",
    "- **Trade-off in Task Isolation** While a balanced configuration can provide a reasonable level of task isolation, it may not offer the same level of isolation as smaller executor configurations. In cases where tasks have distinct resource requirements or strict isolation requirements, a balanced configuration may not be the most suitable choice.\n",
    "- **Task Granularity** In situations where the workload consists of a large number of small tasks, a balanced executor configuration may not offer the same level of fine-grained task allocation as smaller executor configurations. This can lead to suboptimal resource allocation and potentially impact performance.\n",
    "- **Complexity in Resource Management** Maintaining a balanced executor configuration across a dynamic cluster can be challenging. As the cluster size and resource availability change, it may require frequent adjustments to ensure the configuration remains balanced, which can add complexity to cluster management.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing between the three configurations\n",
    "\n",
    "| Configuration              | Tiny Executor                                               | Fat Executor                                                           | Balanced Executor                           |\n",
    "| -------------------------- | ----------------------------------------------------------- | ---------------------------------------------------------------------- | ------------------------------------------- |\n",
    "| **Executor Memory**        | Low memory allocation (e.g., 1-2 GB)                        | High memory allocation (e.g., 16-32 GB)                                | Moderate memory allocation (e.g., 4-8 GB)   |\n",
    "| **Number of Cores**        | Few cores (e.g., 1-2)                                       | Many cores (e.g., 8-16)                                                | Moderate number of cores (e.g., 4-8)        |\n",
    "| **Parallelism**            | Limited parallelism due to fewer cores                      | High parallelism due to many cores                                     | Balanced parallelism                        |\n",
    "| **Task Granularity**       | Smaller tasks due to fewer resources                        | Larger tasks due to more resources                                     | Moderate task granularity                   |\n",
    "| **Use Case**               | Suitable for lightweight tasks or constrained resources     | Suitable for memory-intensive tasks and large datasets                 | Balanced use for general workloads          |\n",
    "| **Resource Utilization**   | Efficient utilization of resources, but limited parallelism | Efficient for parallel processing, but may lead to resource contention | Balanced resource utilization               |\n",
    "| **Memory-Intensive Tasks** | Limited capacity for memory-intensive tasks                 | Well-suited for memory-intensive tasks                                 | Adequate capacity for moderate memory tasks |\n",
    "| **CPU-Intensive Tasks**    | Suitable for tasks with low CPU requirements                | Suitable for CPU-intensive tasks                                       | Balanced for a mix of CPU and memory tasks  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD, DataFrame and Dataset\n",
    "\n",
    "## RDD (Resilient Distributed Dataset):\n",
    "\n",
    "**Definition:** RDD stands for Resilient Distributed Dataset. It is the fundamental data structure in Apache Spark, representing an immutable, distributed collection of objects that can be processed in parallel. RDDs are fault-tolerant and can recover lost data due to node failures by using lineage information.\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "- **Immutability:** RDDs are immutable, meaning their content cannot be changed once created. You can transform an RDD into another RDD through transformations, but the original RDD remains unchanged.\n",
    "- **Partitioning:** RDDs are divided into partitions, each processed on a separate node in the cluster. Partitioning enables parallelism in distributed computing.\n",
    "- **Resilience:** RDDs are fault-tolerant. If a partition is lost due to node failure, Spark can recompute it using the lineage information.\n",
    "\n",
    "## DataFrame:\n",
    "\n",
    "**Definition:** A DataFrame is a higher-level abstraction built on top of RDD. It represents a distributed collection of data organized into named columns. DataFrames provide a more structured and user-friendly API compared to RDDs, and they support various optimizations.\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "- **Schema:** DataFrames have a predefined schema, meaning the data is organized into columns with specified data types. This allows Spark to perform optimizations during execution.\n",
    "- **Lazy Evaluation:** Like RDDs, DataFrames use lazy evaluation, meaning transformations are not executed immediately but are scheduled to be executed later when an action is called.\n",
    "- **Optimization:** DataFrames leverage the Catalyst optimizer and the Tungsten execution engine, leading to performance improvements.\n",
    "\n",
    "## Dataset:\n",
    "\n",
    "**Definition:** A Dataset is a distributed collection of data that provides the benefits of both RDDs and DataFrames. Datasets are strongly typed, allowing for type-safety and improved optimizations, while also providing a functional API similar to RDDs.\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "- **Strong Typing:** Datasets are strongly typed, meaning that the type of data is known at compile-time. This allows for type safety and optimization of queries.\n",
    "- **Functional API:** Datasets provide a functional programming API, similar to RDDs, allowing users to express complex data manipulations using transformations and actions.\n",
    "- **Optimization:** Datasets benefit from the same optimizations as DataFrames, including Catalyst and Tungsten, leading to improved performance.\n",
    "\n",
    "In summary, RDDs are the basic building blocks of Spark, DataFrames provide a higher-level, structured API for data manipulation, and Datasets combine the benefits of both RDDs and DataFrames, offering strong typing and functional programming capabilities. The choice between them depends on the specific requirements and characteristics of the data processing task at hand.\n",
    "\n",
    "## Tabular Comparision\n",
    "\n",
    "| Feature                 | RDD                                           | DataFrame                                         | Dataset                                                 |\n",
    "| ----------------------- | --------------------------------------------- | ------------------------------------------------- | ------------------------------------------------------- |\n",
    "| **Definition**          | Resilient Distributed Dataset                 | Distributed collection of data with named columns | Typed distributed collection of data with named columns |\n",
    "| **Type Safety**         | Not type-safe (untyped)                       | Type-safe (typed)                                 | Type-safe (typed)                                       |\n",
    "| **Performance**         | Lower performance optimizations               | Optimized for performance                         | Optimized for performance                               |\n",
    "| **Ease of Use**         | Lower-level API, more manual control          | Higher-level API, easier to use                   | Higher-level API, easier to use                         |\n",
    "| **Interoperability**    | Supports Java, Scala, Python                  | Supports Java, Scala, Python                      | Supports Java, Scala                                    |\n",
    "| **Compile-Time Safety** | No compile-time type checking                 | Compile-time type checking (statically typed)     | Compile-time type checking (statically typed)           |\n",
    "| **Optimization**        | Limited optimizations                         | Optimized Catalyst and Tungsten engine            | Optimized Catalyst and Tungsten engine                  |\n",
    "| **Schema**              | No predefined schema                          | Predefined schema                                 | Predefined schema                                       |\n",
    "| **Serialization**       | Java Serialization                            | Tungsten binary format                            | Tungsten binary format                                  |\n",
    "| **API**                 | Functional transformations                    | Declarative SQL-like operations                   | Functional transformations                              |\n",
    "| **Immutable**           | Immutable                                     | Immutable                                         | Immutable                                               |\n",
    "| **Lazy Evaluation**     | Supports lazy evaluation                      | Supports lazy evaluation                          | Supports lazy evaluation                                |\n",
    "| **Use Cases**           | Low-level data processing, complex algorithms | Data manipulation, exploratory data analysis      | Type-safe, high-level data processing                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type safety, also known as type checking, is a programming concept that ensures that operations performed on variables or data structures are compatible with their declared types. In the context of Apache Spark and the differentiation between RDD, DataFrame, and Dataset, type safety refers to the level of assurance the system provides regarding the correctness of data types during compile-time.\n",
    "\n",
    "**RDD (Resilient Distributed Dataset):**\n",
    "\n",
    "RDDs are untyped or loosely typed, meaning that Spark does not enforce strict type checking during compile-time. `You can perform operations on RDDs without being explicitly aware of the data types.` This lack of type safety can lead to runtime errors if the data types are not handled correctly.\n",
    "\n",
    "**DataFrame:**\n",
    "\n",
    "DataFrames are designed with a high level of type safety. They have a predefined schema, and operations on DataFrames are checked at compile-time to ensure that they are compatible with the specified schema. This reduces the chances of runtime errors related to type mismatches.\n",
    "\n",
    "**Dataset:**\n",
    "\n",
    "Datasets, like DataFrames, provide a high level of type safety. Datasets are strongly typed, meaning that operations on Datasets are checked at compile-time to ensure type compatibility. They offer the benefits of both RDDs and DataFrames, allowing for strong typing and optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> DataFrames in Apache Spark are not strongly typed in the same sense as Datasets. While DataFrames have a predefined schema that enforces data types for each column, the enforcement is typically done at runtime rather than compile-time. This means that type checking occurs when actions are executed rather than during the compilation phase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = [1, 2, 3, 4, 5]\n",
    "\n",
    "# rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# # Perform transformations and actions on the RDD\n",
    "# squared_rdd = rdd.map(lambda x: x ** 2)\n",
    "# result = squared_rdd.reduce(lambda x, y: x + y)\n",
    "\n",
    "# # Show the result\n",
    "# print(\"RDD Result:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a DataFrame from a list of Row objects\n",
    "# data = [Row(id=1, value=10), Row(id=2, value=20), Row(id=3, value=30)]\n",
    "# df = spark.createDataFrame(data)\n",
    "\n",
    "# # Perform operations on the DataFrame\n",
    "# df_result = df.select(\"id\", \"value\").filter(df[\"value\"] > 15).groupBy(\"id\").sum()\n",
    "# df_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Dataset has `compile-time safety`, it is only supported in a compiled language( Java & Scala ) but not in an interpreted language(R & Python). But Spark Dataframe API is available in all four languages( Java, Scala, Python & R ) supported by Spark.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
