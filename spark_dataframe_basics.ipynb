{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row, DataFrame\n",
    "\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    desc,\n",
    "    asc,\n",
    "\n",
    "    count_distinct,\n",
    "\n",
    "    avg,\n",
    "\n",
    "    stddev,\n",
    "\n",
    "    format_number,\n",
    "    mean,\n",
    "\n",
    "    dayofmonth,\n",
    "    dayofyear,\n",
    "\n",
    "    month,\n",
    "    year,\n",
    "\n",
    "    hour,\n",
    "\n",
    "    weekofyear,\n",
    "\n",
    "    date_format,\n",
    "    spark_partition_id,\n",
    "\n",
    ")\n",
    "\n",
    "from pyspark.sql.types import StringType, StructField, StructType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_records_per_partition(input_df: DataFrame) -> DataFrame:\n",
    "    return (\n",
    "        input_df.withColumn(\"spark_partition_id\", spark_partition_id())\n",
    "        .groupBy(\"spark_partition_id\")\n",
    "        .count()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Spark Session\n",
    "\n",
    "> Builder is a class whereas builder initializes the Builder class\n",
    "\n",
    "## Understanding The Spark Session\n",
    "\n",
    "A SparkSession can be used to create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and read parquet files. To create a SparkSession, use the following builder pattern:\n",
    "\n",
    "## Methods\n",
    "\n",
    "- active() : Returns the active or default SparkSession for the current thread, returned by the builder.\n",
    "- addArtifact(\\*path[, pyfile, archive, file]) : Add artifact(s) to the client session.\n",
    "- addArtifacts(\\*path[, pyfile, archive, file]) : Add artifact(s) to the client session.\n",
    "- addTag(tag) : Add a tag to be assigned to all the operations started by this thread in this session.\n",
    "- clearTags() : Clear the current thread’s operation tags.\n",
    "- copyFromLocalToFs(local_path, dest_path) : Copy file from local to cloud storage file system.\n",
    "- createDataFrame(data[, schema, …]) : Creates a DataFrame from an RDD, a list, a pandas.DataFrame or a numpy.ndarray.\n",
    "- getActiveSession() : Returns the active SparkSession for the current thread, returned by the builder\n",
    "- getTags() : Get the tags that are currently set to be assigned to all the operations started by this thread.\n",
    "- interruptAll() : Interrupt all operations of this session currently running on the connected server.\n",
    "- interruptOperation(op_id) : Interrupt an operation of this session with the given operationId.\n",
    "- interruptTag(tag) : Interrupt all operations of this session with the given operation tag.\n",
    "- newSession() : Returns a new SparkSession as new session, that has separate SQLConf, registered temporary views and UDFs, but shared SparkContext and table cache.\n",
    "- range(start[, end, step, numPartitions]) : Create a DataFrame with single pyspark.sql.types.LongType column named id, containing elements in a range from start to end (exclusive) with step value step.\n",
    "- removeTag(tag) : Remove a tag previously added to be assigned to all the operations started by this thread in this session.\n",
    "- sql(sqlQuery[, args]) : Returns a DataFrame representing the result of the given query.\n",
    "- stop() : Stop the underlying SparkContext.\n",
    "- table(tableName) : Returns the specified table as a DataFrame.\n",
    "\n",
    "## Attributes\n",
    "\n",
    "- builder\n",
    "- catalog : Interface through which the user may create, drop, alter or query underlying databases, tables, functions, etc.\n",
    "- client : Gives access to the Spark Connect client.\n",
    "- conf : Runtime configuration interface for Spark.\n",
    "- read : Returns a DataFrameReader that can be used to read data in as a DataFrame.\n",
    "- readStream : Returns a DataStreamReader that can be used to read data streams as a streaming DataFrame.\n",
    "- sparkContext : Returns the underlying SparkContext.\n",
    "- streams : Returns a StreamingQueryManager that allows managing all the StreamingQuery instances active on this context.\n",
    "- udf : Returns a UDFRegistration for UDF registration.\n",
    "- udtf : Returns a UDTFRegistration for UDTF registration.\n",
    "- version : The version of Spark on which this application is running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Spark Basics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- active() : Returns the active or default SparkSession for the current thread, returned by the builder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-AD3HSBA:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Basics</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x15614de4a70>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.active()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- getActiveSession() : Returns the active SparkSession for the current thread, returned by the builder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-AD3HSBA:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Basics</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x15614de4a70>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.getActiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- newSession() : Returns a new SparkSession as new session, that has separate SQLConf, registered temporary views and UDFs, but shared SparkContext and table cache.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x0000015614EEE6F0>\n"
     ]
    }
   ],
   "source": [
    "spark = spark.newSession()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- range(start[, end, step, numPartitions]) : Create a DataFrame with single pyspark.sql.types.LongType column named id, containing elements in a range from start to end (exclusive) with step value step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "| 20|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1, 1000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.0\n"
     ]
    }
   ],
   "source": [
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storage Levels in Spark\n",
    "\n",
    "## Memory only Storage level\n",
    "\n",
    "StorageLevel.MEMORY_ONLY is the default behavior of the RDD cache() method and stores the RDD or DataFrame as deserialized objects to JVM memory. When there is not enough memory available it will not save DataFrame of some partitions and these will be re-computed as and when required.\n",
    "\n",
    "This takes more memory. but unlike RDD, this would be slower than MEMORY_AND_DISK level as it recomputes the unsaved partitions, and recomputing the in-memory columnar representation of the underlying table is expensive.\n",
    "\n",
    "## Serialize in Memory\n",
    "\n",
    "StorageLevel.MEMORY_ONLY_SER is the same as MEMORY_ONLY but the difference being it stores `RDD as serialized objects to JVM memory.` It takes lesser memory (space-efficient) than MEMORY_ONLY as it saves objects as serialized and takes an additional few more CPU cycles in order to deserialize.\n",
    "\n",
    "## Memory only and Replicate\n",
    "\n",
    "StorageLevel.MEMORY_ONLY_2 is same as MEMORY_ONLY storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "## Serialized in Memory and Replicate\n",
    "\n",
    "StorageLevel.MEMORY_ONLY_SER_2 is same as MEMORY_ONLY_SER storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "## Memory and Disk Storage level\n",
    "\n",
    "StorageLevel.MEMORY_AND_DISK is the default behavior of the DataFrame or Dataset. In this Storage Level, The DataFrame will be stored in JVM memory as deserialized objects. When required storage is greater than available memory, it stores some of the excess partitions into a disk and reads the data from the disk when required. It is slower as there is I/O involved.\n",
    "\n",
    "## Serialize in Memory and Disk\n",
    "\n",
    "StorageLevel.MEMORY_AND_DISK_SER is same as MEMORY_AND_DISK storage level difference being it serializes the DataFrame objects in memory and on disk when space is not available.\n",
    "\n",
    "## Memory, Disk and Replicate\n",
    "\n",
    "StorageLevel.MEMORY_AND_DISK_2 is Same as MEMORY_AND_DISK storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "## Serialize in Memory, Disk and Replicate\n",
    "\n",
    "StorageLevel.MEMORY_AND_DISK_SER_2 is same as MEMORY_AND_DISK_SER storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "## Disk only storage level\n",
    "\n",
    "In StorageLevel.DISK_ONLY storage level, DataFrame is stored only on disk and the CPU computation time is high as I/O involved.\n",
    "\n",
    "## Disk only and Replicate\n",
    "\n",
    "StorageLevel.DISK_ONLY_2 is same as DISK_ONLY storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "| Storage Level       | Space used | CPU time | In memory | On-disk | Serialized | Recompute some partitions |\n",
    "| ------------------- | ---------- | -------- | --------- | ------- | ---------- | ------------------------- |\n",
    "| MEMORY_ONLY         | High       | Low      | Y         | N       | N          | Y                         |\n",
    "| MEMORY_ONLY_SER     | Low        | High     | Y         | N       | Y          | Y                         |\n",
    "| MEMORY_AND_DISK     | High       | Medium   | Some      | Some    | Some       | N                         |\n",
    "| MEMORY_AND_DISK_SER | Low        | High     | Some      | Some    | Y          | N                         |\n",
    "| DISK_ONLY           | Low        | High     | N         | Y       | Y          | N                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading a Dataset\n",
    "\n",
    "## Spark DataFrameReader\n",
    "\n",
    "Use `SparkSession.read` to access this.\n",
    "\n",
    "Quick function review:\n",
    "\n",
    "- `csv(path)`\n",
    "- `jdbc(url, table, ..., connectionProperties)`\n",
    "- `json(path)`\n",
    "- `format(source)`\n",
    "- `load(path)`\n",
    "- `orc(path)`\n",
    "- `parquet(path)`\n",
    "- `table(tableName)`\n",
    "- `text(path)`\n",
    "- `textFile(path)`\n",
    "\n",
    "Configuration methods:\n",
    "\n",
    "- `option(key, value)`\n",
    "- `options(map)`\n",
    "- `schema(schema)`\n",
    "\n",
    "### Header:\n",
    "\n",
    "If the csv file have a header (column names in the first row) then set header=true. This will use the first row in the csv file as the dataframe's column names. Setting header=false (default option) will result in a dataframe with default column names: \\_c0, \\_c1, \\_c2, etc.\n",
    "\n",
    "Setting this to true or false should be based on your input file.\n",
    "\n",
    "### Schema:\n",
    "\n",
    "The schema refered to here are the column types. A column can be of type String, Double, Long, etc. Using inferSchema=false (default option) will give a dataframe where all columns are strings (StringType). Depending on what you want to do, strings may not work. For example, if you want to add numbers from different columns, then those columns should be of some numeric type (strings won't work).\n",
    "\n",
    "By setting inferSchema=true, Spark will automatically go through the csv file and infer the schema of each column. This requires an extra pass over the file which will result in reading a file with inferSchema set to true being slower. But in return the dataframe will most likely have a correct schema given its input.\n",
    "\n",
    "### Faster Method\n",
    "\n",
    "As an alternative to reading a csv with inferSchema you can provide the schema while reading. This have the advantage of being faster than inferring the schema while giving a dataframe with the correct column types. In addition, for csv files without a header row, column names can be given automatically. To provde schema see e.g.: Provide schema while reading csv file as a dataframe\n",
    "\n",
    "### CSV w/InferSchema\n",
    "\n",
    "- we still have three columns\n",
    "- all three columns are still **nullable**\n",
    "- all three columns have their proper names\n",
    "- two jobs were executed (not one as in the previous example)\n",
    "- our three columns now have distinct data types:\n",
    "  - **timestamp** == **timestamp**\n",
    "  - **site** == **string**\n",
    "  - **requests** == **integer**\n",
    "\n",
    "**Question:** Why were there two jobs?\n",
    "\n",
    "inferSchema option tells the reader to infer data types from the source file. This results in an additional pass over the file resulting in two Spark jobs being triggered.\n",
    "\n",
    "**Question:** How long did the last job take?\n",
    "\n",
    "Command took 31.69 seconds -- by kaeshur_pirate@proton.me at 18/11/2023, 3:15:56 pm on My Cluster\n",
    "\n",
    "**Question:** Why did it take so much longer?\n",
    "\n",
    "Bec spark needs to scan the whole file in infer schema\n",
    "\n",
    "### CSV w/ User-Defined Schema\n",
    "\n",
    "- We still have three columns\n",
    "- All three columns are **NOT** nullable because we declared them as such.\n",
    "- All three columns have their proper names\n",
    "- Zero jobs were executed\n",
    "- Our three columns now have distinct data types:\n",
    "  - **timestamp** == **string**\n",
    "  - **site** == **string**\n",
    "  - **requests** == **integer**\n",
    "\n",
    "**Question:** Why were there no jobs?\n",
    "\n",
    "When you define the schema, Spark doesn’t need to perform the costly scan of the entire dataset to infer data types and structures. No need to read the header (line #1) or infer the schema (entire file).\n",
    "That same information is now declared in the user-defined schema.\n",
    "\n",
    "**Question:** What is different about the data types of these columns compared to the previous exercise & why?\n",
    "\n",
    "The timestamp column is now of type string because we declared it as such.\n",
    "\n",
    "**Question:** Do I need to indicate that the file has a header?\n",
    "\n",
    "Yes, otherwise, line #1 will be treated as data and not as a header.\n",
    "\n",
    "**Question:** Do the declared column names need to match the columns in the header of the TSV file?\n",
    "No, and you can demonstrate that by renaming it in the schema to something like capturedAt.\n",
    "When reading a CSV file using Apache Spark and defining well-defined data types, the column definitions specified in your code should align with the actual columns present in the CSV file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer Schema\n",
    "\n",
    "Pros\n",
    "\n",
    "- Simplicity: It’s easy to use, especially for quick data exploration or when the schema is not known in advance.\n",
    "- Less code: You don’t need to manually specify the schema, which reduces the amount of code you need to write.\n",
    "\n",
    "Cons\n",
    "\n",
    "- Performance Overhead: Spark needs to scan the entire dataset to infer the schema, which can be computationally expensive, especially for large datasets.\n",
    "- Data Quality: Inference may lead to incorrect schema deductions if the data has missing or inconsistent values.\n",
    "- Type Inference: Inferencing may not always correctly identify the data types of columns, leading to potential data type mismatches.\n",
    "\n",
    "### Define Schema Explicitly\n",
    "\n",
    "Pros\n",
    "\n",
    "- Performance: Defining the schema explicitly can significantly improve performance because Spark doesn’t need to scan the entire dataset to infer the schema.\n",
    "- Data Quality: You have control over the schema definition, ensuring that it accurately represents your data. This is important for data integrity and consistency.\n",
    "\n",
    "Cons\n",
    "\n",
    "- More Code: You need to write additional code to define the schema, which can be more cumbersome, especially for complex datasets or when the schema evolves over time.\n",
    "\n",
    "### Performance Comparison\n",
    "\n",
    "- In terms of performance, defining the schema explicitly is generally more efficient than inferring it. When you define the schema, Spark doesn’t need to perform the costly scan of the entire dataset to infer data types and structures.\n",
    "\n",
    "- Explicit schema definition is particularly advantageous for large datasets where schema inference can introduce a significant overhead.\n",
    "\n",
    "- If data quality and performance are critical for your application, defining the schema explicitly is often the preferred approach, especially for production-level code.\n",
    "\n",
    "However, it’s essential to strike a balance between performance and development speed. In some scenarios, such as quick data exploration or ad-hoc analysis, inferring the schema might be acceptable. Ultimately, the choice between inferring and defining the schema should consider factors like data quality, development effort, and the specific requirements of your Spark application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reaing JSON Lines\n",
    "\n",
    "JSON Lines is a text file format that stores JSON values one per line, with a line separator of '\\\\n' or '\\\\r\\\\n'.\n",
    "\n",
    "This format is referred to as **JSON Lines** or **newline-delimited JSON**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df = spark.read.json(\"./datasets/people.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading CSV Files\n",
    "\n",
    "The formats CSV [RFC4180] (comma separated values) and TSV [IANA-TSV] (tab separated values) provide simple, easy to process formats for the transmission of tabular data. They are supported as input datat formats to many tools, particularly spreadsheets. This document describes their use for expressing SPARQL query results from SELECT queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "appl_stock_df = spark.read.options(inferSchema=True, header=True).csv(\n",
    "    \"./datasets/appl_stock.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_info_df = (\n",
    "    spark.read.option(\"inferSchema\", True)\n",
    "    .option(\"header\", True)\n",
    "    .csv(\"./datasets/sales_info.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "contains_null_df = (\n",
    "    spark.read.option(\"inferSchema\", True)\n",
    "    .option(\"header\", True)\n",
    "    .csv(\"./datasets/ContainsNull.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Files with custom Schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = [\n",
    "    StructField(name=\"age\", dataType=IntegerType(), nullable=True),\n",
    "    StructField(name=\"name\", dataType=StringType(), nullable=True),\n",
    "]\n",
    "final_structure = StructType(fields=data_schema)\n",
    "\n",
    "people_df_custom_schema = spark.read.json(\n",
    "    \"./datasets/people.json\", schema=final_structure\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data in Parquet Format\n",
    "\n",
    "Apache Parquet is an open source, column-oriented data file format designed for efficient data storage and retrieval. It provides efficient data compression and encoding schemes with enhanced performance to handle complex data in bulk. Parquet is available in multiple languages including Java, C++, Python, etc...\n",
    "\n",
    "#### About Parquet Files\n",
    "\n",
    "- Free & Open Source.\n",
    "- Increased query performance over row-based data stores.\n",
    "- Provides efficient data compression.\n",
    "- Designed for performance on large data sets.\n",
    "- Supports limited schema evolution.\n",
    "- Is a splittable \"file format\".\n",
    "- A <a href=\"https://en.wikipedia.org/wiki/Column-oriented_DBMS\" target=\"_blank\">Column-Oriented</a> data store\n",
    "\n",
    "- We do not need to specify the schema - the column names and data types are stored in the parquet files.\n",
    "- Only one job is required to **read** that schema from the parquet file's metadata.\n",
    "- Unlike the CSV or JSON readers that have to load the entire file and then infer the schema, the parquet reader can \"read\" the schema very quickly because it's reading that schema from the metadata.\n",
    "\n",
    "#### Read in the Parquet Files\n",
    "\n",
    "To read in this files, we will specify the location of the parquet directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_df = spark.read.parquet(\"./datasets/flights.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.show() → None[source]\n",
    "\n",
    "Parameters\n",
    "\n",
    "1. n int, optional Number of rows to show.\n",
    "\n",
    "2. truncate bool or int, optional If set to True, truncate strings longer than 20 chars by default. If set to a number greater than one, truncates long strings to length truncate and align cells right.\n",
    "3. vertical bool, optional If set to True, print output rows vertically (one line per column value).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "| age|height|   name|\n",
      "+----+------+-------+\n",
      "|NULL|  NULL|   NULL|\n",
      "|NULL|  NULL|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|  30|    80|   Andy|\n",
      "|  31|    80|   Andy|\n",
      "|  18|    80| Justin|\n",
      "|  19|    80| Justin|\n",
      "+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------\n",
      " age    | NULL    \n",
      " height | NULL    \n",
      " name   | NULL    \n",
      "-RECORD 1---------\n",
      " age    | NULL    \n",
      " height | NULL    \n",
      " name   | Michael \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.show(truncate=False, n=2, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.printSchema() → None\n",
    "\n",
    "1. level int, optional, default -> None : How many levels to print for nested schemas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_df = spark.createDataFrame([(1, (2, 2))], [\"a\", \"b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: struct (nullable = true)\n",
      "\n",
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: struct (nullable = true)\n",
      " |    |-- _1: long (nullable = true)\n",
      " |    |-- _2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nested_df.printSchema(1)\n",
    "nested_df.printSchema(2)\n",
    "del nested_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.describe() → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "cols str, list, optional Column name or list of column names to describe by (default All columns).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------+-------+\n",
      "|summary|              age|height|   name|\n",
      "+-------+-----------------+------+-------+\n",
      "|  count|                4|     6|      7|\n",
      "|   mean|             24.5|  80.0|   NULL|\n",
      "| stddev|6.952217871538069|   0.0|   NULL|\n",
      "|    min|               18|    80|   Andy|\n",
      "|    max|               31|    80|Michael|\n",
      "+-------+-----------------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|              age|\n",
      "+-------+-----------------+\n",
      "|  count|                4|\n",
      "|   mean|             24.5|\n",
      "| stddev|6.952217871538069|\n",
      "|    min|               18|\n",
      "|    max|               31|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.describe([\"age\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.agg() → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "Aggregate on the entire DataFrame without groups (shorthand for df.groupBy().agg()).\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    exprs Column or dict of key and value strings, Columns or expressions to aggregate DataFrame by.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame: Aggregated DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|max(age)|\n",
      "+--------+\n",
      "|      31|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.agg({\"age\": \"max\"}).show()\n",
    "# people_df.groupBy().agg({\"age\": \"max\"}).show() # same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.alias(alias: str) → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    alias str: an alias name to be set for the DataFrame.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame: Aliased DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+----+------+-------+\n",
      "| age|height|   name| age|height|   name|\n",
      "+----+------+-------+----+------+-------+\n",
      "|NULL|  NULL|Michael|NULL|    80|Michael|\n",
      "|NULL|  NULL|Michael|NULL|    80|Michael|\n",
      "|NULL|  NULL|Michael|NULL|  NULL|Michael|\n",
      "|NULL|    80|Michael|NULL|    80|Michael|\n",
      "|NULL|    80|Michael|NULL|    80|Michael|\n",
      "|NULL|    80|Michael|NULL|  NULL|Michael|\n",
      "|NULL|    80|Michael|NULL|    80|Michael|\n",
      "|NULL|    80|Michael|NULL|    80|Michael|\n",
      "|NULL|    80|Michael|NULL|  NULL|Michael|\n",
      "|  30|    80|   Andy|  31|    80|   Andy|\n",
      "|  30|    80|   Andy|  30|    80|   Andy|\n",
      "|  31|    80|   Andy|  31|    80|   Andy|\n",
      "|  31|    80|   Andy|  30|    80|   Andy|\n",
      "|  18|    80| Justin|  19|    80| Justin|\n",
      "|  18|    80| Justin|  18|    80| Justin|\n",
      "|  19|    80| Justin|  19|    80| Justin|\n",
      "|  19|    80| Justin|  18|    80| Justin|\n",
      "+----+------+-------+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df_alias_1 = people_df.alias(\"people_df_1\")\n",
    "people_df_alias_2 = people_df.alias(\"people_df_2\")\n",
    "\n",
    "people_df_alias_1.join(\n",
    "    people_df_alias_2, col(\"people_df_1.name\") == col(\"people_df_2.name\"), how=\"inner\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.approxQuantile() → Union[List[float], List[List[float]]]\n",
    "\n",
    "Calculates the approximate quantiles of numerical columns of a DataFrame.\n",
    "\n",
    "The result of this algorithm has the following deterministic bound: If the DataFrame has N elements and if we request the quantile at probability p up to error err, then the algorithm will return a sample x from the DataFrame so that the exact rank of x is close to (p \\* N). More precisely,\n",
    "\n",
    "floor((p - err) _ N) <= rank(x) <= ceil((p + err) _ N).\n",
    "\n",
    "This method implements a variation of the Greenwald-Khanna algorithm (with some speed optimizations). The algorithm was first present in [[https://doi.org/10.1145/375663.375670 Space-efficient Online Computation of Quantile Summaries]] by Greenwald and Khanna.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    1. col: str, tuple or list: Can be a single column name, or a list of names for multiple columns.\n",
    "\n",
    "    2. probabilities list or tuple: a list of quantile probabilities Each number must belong to [0, 1]. For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n",
    "\n",
    "    3. relativeError float: The relative target precision to achieve (>= 0). If set to zero, the exact quantiles are computed, which could be very expensive. Note that values greater than 1 are accepted but gives the same result as 1.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    list\n",
    "    the approximate quantiles at the given probabilities.\n",
    "\n",
    "    If the input col is a string, the output is a list of floats.\n",
    "\n",
    "    If the input col is a list or tuple of strings, the output is also a\n",
    "    list, but each element in it is a list of floats, i.e., the output is a list of list of floats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18.0]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.approxQuantile(col=\"age\", probabilities=[0.5], relativeError=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.cache() → pyspark.sql.dataframe.DataFrame[source]\n",
    "\n",
    "Persists the DataFrame with the default storage level (MEMORY_AND_DISK).\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame: Cached DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, height: bigint, name: string]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.persist() → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    storageLevel StorageLevel: Storage level to set for persistence. Default is MEMORY_AND_DISK_DESER.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame: Persisted DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, height: bigint, name: string]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.checkpoint() → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "Returns a checkpointed version of this DataFrame. Checkpointing can be used to truncate the logical plan of this DataFrame, which is especially useful in iterative algorithms where the plan may grow exponentially. It will be saved to files inside the checkpoint directory set with SparkContext.setCheckpointDir().\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    eager bool, optional, default True: Whether to checkpoint this DataFrame immediately.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame: Checkpointed DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.coalesce(numPartitions: int) → pyspark.sql.dataframe.DataFrame[source]\n",
    "\n",
    "Similar to coalesce defined on an RDD, this operation results in a narrow dependency, e.g. if you go from 1000 partitions to 100 partitions, there will not be a shuffle, instead each of the 100 new partitions will claim 10 of the current partitions. If a larger number of partitions is requested, it will stay at the current number of partitions.\n",
    "\n",
    "However, if you’re doing a drastic coalesce, e.g. to numPartitions = 1, this may result in your computation taking place on fewer nodes than you like (e.g. one node in the case of numPartitions = 1). To avoid this, you can call repartition(). This will add a shuffle step, but means the current upstream partitions will be executed in parallel (per whatever the current partitioning is).\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    numPartitions int\n",
    "    specify the target number of partitions\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.coalesce(1).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.colRegex(colName: str) → pyspark.sql.column.Column\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    colName str\n",
    "    string, column name specified as a regex.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    Column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "| age|height|\n",
      "+----+------+\n",
      "|NULL|  NULL|\n",
      "|NULL|  NULL|\n",
      "|NULL|    80|\n",
      "|NULL|    80|\n",
      "|  30|    80|\n",
      "|  31|    80|\n",
      "|  18|    80|\n",
      "|  19|    80|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.select(people_df.colRegex(\"`(name)?+.+`\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.collect() → List[pyspark.sql.types.Row]\n",
    "\n",
    "Returns all the records as a list of Row.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    list\n",
    "    List of rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=None, height=None, name=None),\n",
       " Row(age=None, height=None, name='Michael'),\n",
       " Row(age=None, height=80, name='Michael'),\n",
       " Row(age=None, height=80, name='Michael'),\n",
       " Row(age=30, height=80, name='Andy'),\n",
       " Row(age=31, height=80, name='Andy'),\n",
       " Row(age=18, height=80, name='Justin'),\n",
       " Row(age=19, height=80, name='Justin')]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.corr() → float\n",
    "\n",
    "Calculates the correlation of two columns of a DataFrame as a double value. Currently only supports the Pearson Correlation Coefficient. DataFrame.corr() and DataFrameStatFunctions.corr() are aliases of each other.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    col1 str\n",
    "    The name of the first column\n",
    "\n",
    "    col2 str\n",
    "    The name of the second column\n",
    "\n",
    "    method str, optional\n",
    "    The correlation method. Currently only supports “pearson”\n",
    "\n",
    "### Returns\n",
    "\n",
    "    float\n",
    "    Pearson Correlation Coefficient of two columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# people_df.corr(\"age\", \"name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.createGlobalTempView(name: str) → None\n",
    "\n",
    "Creates a global temporary view with this DataFrame.\n",
    "The lifetime of this temporary view is tied to this `Spark application`. throws TempTableAlreadyExistsException, if the view name already exists in the catalog.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    name str\n",
    "    Name of the view.\n",
    "\n",
    "#### Register a Table/View\n",
    "\n",
    "- Databrick's UI has built in support for working with a number of different data sources\n",
    "- New ones are being added regularly\n",
    "- In our case we are going to upload the file <a href=\"http://files.training.databricks.com/static/data/pageviews_by_second_example.tsv\">pageviews_by_second_example.tsv</a>\n",
    "- .. and then use the UI to create a table.\n",
    "\n",
    "There are several benefits to this strategy:\n",
    "\n",
    "- Once setup, it never has to be done again\n",
    "- It is available for any user on the platform (permissions permitting)\n",
    "- Minimizes exposure of credentials\n",
    "- No real overhead to reading the schema (no infer-schema)\n",
    "- Easier to advertise available datasets to other users\n",
    "\n",
    "#### Review: Reading from Tables\n",
    "\n",
    "- No job is executed - the schema is stored in the table definition on Databricks.\n",
    "- The data types shown here are those we defined when we registered the table.\n",
    "- In our case, the file was uploaded to Databricks and is stored on the DBFS.\n",
    "  - If we used JDBC, it would open the connection to the database and read it in.\n",
    "  - If we used an object store (like what is backing the DBFS), it would read the data from source.\n",
    "- The \"registration\" of the table simply makes future access, or access by multiple users easier.\n",
    "- The users of the notebook cannot see username and passwords, secret keys, tokens, etc.\n",
    "\n",
    "** _Note #1:_ ** _The method createOrReplaceTempView(..) is bound to the SparkSession meaning it will be discarded once the session ends._\n",
    "\n",
    "** _Note #2:_ ** On the other hand, the method createOrReplaceGlobalTempView(..) is bound to the spark application.\\*\n",
    "\n",
    "`_Or to put that another way, I can use createOrReplaceTempView(..) in this notebook only. However, I can call createOrReplaceGlobalTempView(..) in this notebook and then access it from another._`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# people_df.createGlobalTempView(\"people_df_global_temp_view\")\n",
    "# spark.sql(\"select * from global_temp.people_df_global_temp_view\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "| age|height|   name|\n",
      "+----+------+-------+\n",
      "|NULL|  NULL|   NULL|\n",
      "|NULL|  NULL|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|  30|    80|   Andy|\n",
      "|  31|    80|   Andy|\n",
      "|  18|    80| Justin|\n",
      "|  19|    80| Justin|\n",
      "+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.createOrReplaceGlobalTempView(\"people_df_global_temp_view\")\n",
    "spark.sql(\"select * from global_temp.people_df_global_temp_view\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.createTempView(name: str) → None\n",
    "\n",
    "Creates a local temporary view with this DataFrame.\n",
    "\n",
    "The lifetime of this temporary table is tied to the `SparkSession` that was used to create this DataFrame.throws TempTableAlreadyExistsException, if the view name already exists in the catalog.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    name str\n",
    "    Name of the view.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# people_df.createTempView(\"people_df_temp_view\")\n",
    "# spark.sql(\"select * from people_df_temp_view\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "| age|height|   name|\n",
      "+----+------+-------+\n",
      "|NULL|  NULL|   NULL|\n",
      "|NULL|  NULL|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|  30|    80|   Andy|\n",
      "|  31|    80|   Andy|\n",
      "|  18|    80| Justin|\n",
      "|  19|    80| Justin|\n",
      "+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.createOrReplaceTempView(\"people_df_temp_view\")\n",
    "spark.sql(\"select * from people_df_temp_view\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.crossJoin(other: pyspark.sql.dataframe.DataFrame) → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "Returns the cartesian product with another DataFrame.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    other DataFrame\n",
    "    Right side of the cartesian product.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame\n",
    "    Joined DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+----+\n",
      "| age|height|   name| age|\n",
      "+----+------+-------+----+\n",
      "|NULL|  NULL|   NULL|NULL|\n",
      "|NULL|  NULL|   NULL|NULL|\n",
      "|NULL|  NULL|   NULL|NULL|\n",
      "|NULL|  NULL|   NULL|NULL|\n",
      "|NULL|  NULL|   NULL|  30|\n",
      "|NULL|  NULL|   NULL|  31|\n",
      "|NULL|  NULL|   NULL|  18|\n",
      "|NULL|  NULL|   NULL|  19|\n",
      "|NULL|  NULL|Michael|NULL|\n",
      "|NULL|  NULL|Michael|NULL|\n",
      "|NULL|  NULL|Michael|NULL|\n",
      "|NULL|  NULL|Michael|NULL|\n",
      "|NULL|  NULL|Michael|  30|\n",
      "|NULL|  NULL|Michael|  31|\n",
      "|NULL|  NULL|Michael|  18|\n",
      "|NULL|  NULL|Michael|  19|\n",
      "|NULL|    80|Michael|NULL|\n",
      "|NULL|    80|Michael|NULL|\n",
      "|NULL|    80|Michael|NULL|\n",
      "|NULL|    80|Michael|NULL|\n",
      "+----+------+-------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.crossJoin(people_df.select(\"age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.distinct() → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "#### Returns\n",
    "\n",
    "    DataFrame\n",
    "    DataFrame with distinct records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "| age|height|   name|\n",
      "+----+------+-------+\n",
      "|  19|    80| Justin|\n",
      "|  30|    80|   Andy|\n",
      "|  18|    80| Justin|\n",
      "|  31|    80|   Andy|\n",
      "|NULL|  NULL|   NULL|\n",
      "|NULL|  NULL|Michael|\n",
      "|NULL|    80|Michael|\n",
      "+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.drop(\\*cols: ColumnOrName) → DataFrame[source]\n",
    "\n",
    "Returns a new DataFrame without specified columns. This is a no-op if the schema doesn’t contain the given column name(s).\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    cols: str or :class:`Column`\n",
    "    a name of the column, or the Column to drop\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame\n",
    "    DataFrame without given columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|height|   name|\n",
      "+------+-------+\n",
      "|  NULL|   NULL|\n",
      "|  NULL|Michael|\n",
      "|    80|Michael|\n",
      "|    80|Michael|\n",
      "|    80|   Andy|\n",
      "|    80|   Andy|\n",
      "|    80| Justin|\n",
      "|    80| Justin|\n",
      "+------+-------+\n",
      "\n",
      "+----+------+-------+\n",
      "| age|height|   name|\n",
      "+----+------+-------+\n",
      "|NULL|  NULL|   NULL|\n",
      "|NULL|  NULL|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|  30|    80|   Andy|\n",
      "|  31|    80|   Andy|\n",
      "|  18|    80| Justin|\n",
      "|  19|    80| Justin|\n",
      "+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.drop(\"age\").show()\n",
    "\n",
    "people_df.drop(\"agasdasdasde\").show()  ## no op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.dropDuplicates(subset: Optional[List[str]] = None) → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "Return a new DataFrame with duplicate rows removed, optionally only considering certain columns.\n",
    "\n",
    "For a static batch DataFrame, it just drops duplicate rows. For a streaming DataFrame, it will keep all data across triggers as intermediate state to drop duplicates rows. You can use withWatermark() to limit how late the duplicate data can be and the system will accordingly limit the state. In addition, data older than watermark will be dropped to avoid any possibility of duplicates.\n",
    "\n",
    "`drop_duplicates() is an alias for dropDuplicates().`\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    subset List of column names, optional\n",
    "    List of columns to use for duplicate comparison (default All columns).\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame\n",
    "    DataFrame without duplicates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "| age|height|   name|\n",
      "+----+------+-------+\n",
      "|NULL|  NULL|   NULL|\n",
      "|NULL|  NULL|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|  30|    80|   Andy|\n",
      "|  31|    80|   Andy|\n",
      "|  18|    80| Justin|\n",
      "|  19|    80| Justin|\n",
      "+----+------+-------+\n",
      "\n",
      "+----+------+-------+\n",
      "| age|height|   name|\n",
      "+----+------+-------+\n",
      "|  19|    80| Justin|\n",
      "|  30|    80|   Andy|\n",
      "|  18|    80| Justin|\n",
      "|  31|    80|   Andy|\n",
      "|NULL|  NULL|   NULL|\n",
      "|NULL|  NULL|Michael|\n",
      "|NULL|    80|Michael|\n",
      "+----+------+-------+\n",
      "\n",
      "+----+------+-------+\n",
      "| age|height|   name|\n",
      "+----+------+-------+\n",
      "|  30|    80|   Andy|\n",
      "|NULL|  NULL|   NULL|\n",
      "|NULL|  NULL|Michael|\n",
      "|  18|    80| Justin|\n",
      "|NULL|    80|Michael|\n",
      "+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.show()\n",
    "people_df.dropDuplicates().show()\n",
    "people_df.dropDuplicates(subset=[\"name\", \"height\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.dropna() → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "Returns a new DataFrame omitting rows with null values. DataFrame.dropna() and DataFrameNaFunctions.drop() are aliases of each other.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    how str, optional\n",
    "    ‘any’ or ‘all’. If ‘any’, drop a row if it contains any nulls. If ‘all’, drop a row only if all its values are null.\n",
    "\n",
    "    thresh: int, optional\n",
    "    default None If specified, drop rows that have less than thresh non-null values. This overwrites the how parameter.\n",
    "\n",
    "    subset str, tuple or list, optional\n",
    "    optional list of column names to consider.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame\n",
    "    DataFrame with null only rows excluded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "| age|height|   name|\n",
      "+----+------+-------+\n",
      "|NULL|  NULL|   NULL|\n",
      "|NULL|  NULL|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|  30|    80|   Andy|\n",
      "|  31|    80|   Andy|\n",
      "|  18|    80| Justin|\n",
      "|  19|    80| Justin|\n",
      "+----+------+-------+\n",
      "\n",
      "+---+------+------+\n",
      "|age|height|  name|\n",
      "+---+------+------+\n",
      "| 30|    80|  Andy|\n",
      "| 31|    80|  Andy|\n",
      "| 18|    80|Justin|\n",
      "| 19|    80|Justin|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.show()\n",
    "# people_df.dropna(how=\"any\", subset=[\"height\"]).show()\n",
    "people_df.dropna(how=\"any\", subset=[\"height\", \"age\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.fillna() → DataFrame\n",
    "\n",
    "Replace null values, alias for na.fill(). DataFrame.fillna() and DataFrameNaFunctions.fill() are aliases of each other.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    value int, float, string, bool or dict\n",
    "    Value to replace null values with. If the value is a dict, then subset is ignored and value must be a mapping from column name (string) to replacement value. The replacement value must be an int, float, boolean, or string.\n",
    "\n",
    "    subset str, tuple or list, optional\n",
    "    optional list of column names to consider. Columns specified in subset that do not have matching data types are ignored. For example, if value is a string, and subset contains a non-string column, then the non-string column is simply ignored.\n",
    "\n",
    "> The replacement datatype should be same as the column datatype\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame\n",
    "    DataFrame with replaced null values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "| age|height|   name|\n",
      "+----+------+-------+\n",
      "|NULL|  NULL|   NULL|\n",
      "|NULL|  NULL|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|  30|    80|   Andy|\n",
      "|  31|    80|   Andy|\n",
      "|  18|    80| Justin|\n",
      "|  19|    80| Justin|\n",
      "+----+------+-------+\n",
      "\n",
      "+---+------+-------+\n",
      "|age|height|   name|\n",
      "+---+------+-------+\n",
      "| 60|    60|   NULL|\n",
      "| 60|    60|Michael|\n",
      "| 60|    80|Michael|\n",
      "| 60|    80|Michael|\n",
      "| 30|    80|   Andy|\n",
      "| 31|    80|   Andy|\n",
      "| 18|    80| Justin|\n",
      "| 19|    80| Justin|\n",
      "+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.show()\n",
    "people_df.fillna(value=60, subset=[\"age\", \"height\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| NULL|\n",
      "|emp2| NULL| NULL|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "|NULL| NULL| NULL|\n",
      "|emp5| emp5|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.show()\n",
    "# people_df.fillna(value=60, subset=[\"age\", \"height\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+\n",
      "|age|height|   name|\n",
      "+---+------+-------+\n",
      "| 50|     0|unknown|\n",
      "| 50|     0|Michael|\n",
      "| 50|    80|Michael|\n",
      "| 50|    80|Michael|\n",
      "| 30|    80|   Andy|\n",
      "| 31|    80|   Andy|\n",
      "| 18|    80| Justin|\n",
      "| 19|    80| Justin|\n",
      "+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.fillna({\"age\": 50, \"name\": \"unknown\", \"height\": 0}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.filter(condition: ColumnOrName) → DataFrame\n",
    "\n",
    "Filters rows using the given condition.\n",
    "\n",
    "where() is an alias for filter().\n",
    "\n",
    "### Condition Symbols\n",
    "\n",
    "1. `&` : AND operator\n",
    "2. `|` : OR Operator\n",
    "3. `~` : NOT Operator\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    condition Column or str\n",
    "    a Column of types.BooleanType or a string of SQL expressions.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame\n",
    "    Filtered DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by Column instances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "|2010-01-11|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|\n",
      "|2010-01-12|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|\n",
      "|2010-01-13|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|\n",
      "|2010-01-14|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|\n",
      "|2010-01-15|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|\n",
      "|2010-01-19|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|\n",
      "|2010-01-20|        214.910006|        215.549994|        209.500002|            211.73|153038200|         27.431644|\n",
      "|2010-01-21|        212.079994|213.30999599999998|        207.210003|        208.069996|152038600|         26.957455|\n",
      "|2010-01-22|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|\n",
      "|2010-01-25|202.51000200000001|        204.699999|        200.190002|        203.070002|266424900|26.309658000000002|\n",
      "|2010-01-26|205.95000100000001|        213.710005|        202.580004|        205.940001|466777500|         26.681494|\n",
      "|2010-01-27|        206.849995|            210.58|        199.530001|        207.880005|430642100|26.932840000000002|\n",
      "|2010-01-28|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|\n",
      "|2010-02-01|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "appl_stock_df.filter(appl_stock_df.Close < 500).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by SQL expression in a string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "|2010-01-11|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|\n",
      "|2010-01-12|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|\n",
      "|2010-01-13|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|\n",
      "|2010-01-14|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|\n",
      "|2010-01-15|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|\n",
      "|2010-01-19|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|\n",
      "|2010-01-20|        214.910006|        215.549994|        209.500002|            211.73|153038200|         27.431644|\n",
      "|2010-01-21|        212.079994|213.30999599999998|        207.210003|        208.069996|152038600|         26.957455|\n",
      "|2010-01-22|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|\n",
      "|2010-01-25|202.51000200000001|        204.699999|        200.190002|        203.070002|266424900|26.309658000000002|\n",
      "|2010-01-26|205.95000100000001|        213.710005|        202.580004|        205.940001|466777500|         26.681494|\n",
      "|2010-01-27|        206.849995|            210.58|        199.530001|        207.880005|430642100|26.932840000000002|\n",
      "|2010-01-28|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|\n",
      "|2010-02-01|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "appl_stock_df.filter(\"Close < 500\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering based on multiple conditions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+---------+---------+-----------------+---------+-----------------+\n",
      "|      Date|             Open|     High|      Low|            Close|   Volume|        Adj Close|\n",
      "+----------+-----------------+---------+---------+-----------------+---------+-----------------+\n",
      "|2014-06-09|        92.699997|93.879997|    91.75|        93.699997| 75415000|        88.906324|\n",
      "|2014-06-10|        94.730003|95.050003|    93.57|            94.25| 62777000|        89.428189|\n",
      "|2014-06-11|        94.129997|94.760002|93.470001|        93.860001| 45681000|        89.058142|\n",
      "|2014-06-12|        94.040001|94.120003|91.900002|        92.290001| 54749000|        87.568463|\n",
      "|2014-06-13|        92.199997|92.440002|90.879997|        91.279999| 54525000|        86.610132|\n",
      "|2014-06-16|        91.510002|    92.75|91.449997|        92.199997| 35561000|        87.483064|\n",
      "|2014-06-17|        92.309998|92.699997|91.800003|92.08000200000001| 29726000|87.36920699999999|\n",
      "|2014-06-18|        92.269997|92.290001|91.349998|            92.18| 33514000|         87.46409|\n",
      "|2014-06-19|        92.290001|92.300003|91.339996|        91.860001| 35528000|        87.160461|\n",
      "|2014-06-20|        91.849998|92.550003|90.900002|        90.910004|100898000|        86.259066|\n",
      "|2014-06-23|            91.32|91.620003|90.599998|90.83000200000001| 43694000|        86.183157|\n",
      "|2014-06-24|            90.75|91.739998|90.190002|        90.279999| 39036000|        85.661292|\n",
      "|2014-06-25|        90.209999|90.699997|89.650002|        90.360001| 36869000|        85.737201|\n",
      "|2014-06-26|        90.370003|91.050003|89.800003|        90.900002| 32629000|        86.249576|\n",
      "|2014-06-27|            90.82|     92.0|90.769997|        91.980003| 64029000|        87.274325|\n",
      "|2014-06-30|        92.099998|93.730003|92.089996|            92.93| 49482300|         88.17572|\n",
      "|2014-07-01|        93.519997|    94.07|93.129997|        93.519997| 38223000|88.73553199999999|\n",
      "|2014-07-02|        93.870003|94.059998|93.089996|        93.480003| 28465000|        88.697585|\n",
      "|2014-07-03|93.66999799999999|94.099998|93.199997|        94.029999| 22891800|        89.219443|\n",
      "|2014-07-07|        94.139999|95.989998|94.099998|        95.970001| 56468000|        91.060195|\n",
      "+----------+-----------------+---------+---------+-----------------+---------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "appl_stock_df.filter(\"Close < 100 and Open < 200\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+-----------------+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|        Adj Close|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+-----------------+\n",
      "|2012-02-13|        499.529991|503.83000899999996|497.08998899999995|502.60002099999997|129304000|        65.116633|\n",
      "|2012-02-16|        491.500008|        504.890007|         486.62999|502.20999900000004|236138000|        65.066102|\n",
      "|2013-01-16|494.63999900000005|509.44001799999995|492.49997699999994|506.08998099999997|172701200|        66.151072|\n",
      "|2013-01-18|        498.519981|        502.219986|        496.399986|        500.000015|118230700|        65.355052|\n",
      "|2013-10-17|        499.979988|        504.779991|        499.680008|        504.499985| 63398300|        67.207422|\n",
      "|2014-01-31|        495.179985|        501.529984|        493.549988|500.59997599999997|116199300|67.07722700000001|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "appl_stock_df.filter(\n",
    "    (appl_stock_df[\"Open\"] < 500) & (appl_stock_df[\"Close\"] > 500)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "|2010-01-11|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|\n",
      "|2010-01-12|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|\n",
      "|2010-01-13|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|\n",
      "|2010-01-14|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|\n",
      "|2010-01-15|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|\n",
      "|2010-01-19|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|\n",
      "|2010-01-20|        214.910006|        215.549994|        209.500002|            211.73|153038200|         27.431644|\n",
      "|2010-01-21|        212.079994|213.30999599999998|        207.210003|        208.069996|152038600|         26.957455|\n",
      "|2010-01-22|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|\n",
      "|2010-01-25|202.51000200000001|        204.699999|        200.190002|        203.070002|266424900|26.309658000000002|\n",
      "|2010-01-26|205.95000100000001|        213.710005|        202.580004|        205.940001|466777500|         26.681494|\n",
      "|2010-01-27|        206.849995|            210.58|        199.530001|        207.880005|430642100|26.932840000000002|\n",
      "|2010-01-28|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|\n",
      "|2010-02-01|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "appl_stock_df.filter((appl_stock_df.Open < 500) | ~(appl_stock_df.Close > 500)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = appl_stock_df.filter(\n",
    "    appl_stock_df.Low == 197.16\n",
    ").collect()  # returns results in list of row class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstrow = results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Date=datetime.date(2010, 1, 22), Open=206.78000600000001, High=207.499996, Low=197.16, Close=197.75, Volume=220441900, Adj Close=25.620401) <class 'pyspark.sql.types.Row'>\n"
     ]
    }
   ],
   "source": [
    "print(firstrow, type(firstrow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date': datetime.date(2010, 1, 22), 'Open': 206.78000600000001, 'High': 207.499996, 'Low': 197.16, 'Close': 197.75, 'Volume': 220441900, 'Adj Close': 25.620401}\n",
      "206.78000600000001\n"
     ]
    }
   ],
   "source": [
    "firstrowasdict = firstrow.asDict()\n",
    "print(firstrowasdict)\n",
    "print(firstrowasdict[\"Open\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.select(\\*cols: ColumnOrName) → DataFrame\n",
    "\n",
    "Projects a set of expressions and returns a new DataFrame.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    cols str, Column, or list\n",
    "    column names (string) or expressions (Column). If one of the column names is ‘*’, that column is expanded to include all columns in the current DataFrame.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame\n",
    "    A DataFrame with subset (or all) of columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "| age|height|   name|\n",
      "+----+------+-------+\n",
      "|NULL|  NULL|   NULL|\n",
      "|NULL|  NULL|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|  30|    80|   Andy|\n",
      "|  31|    80|   Andy|\n",
      "|  18|    80| Justin|\n",
      "|  19|    80| Justin|\n",
      "+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.select(\"*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+\n",
      "|   name| age|age+10|\n",
      "+-------+----+------+\n",
      "|   NULL|NULL|  NULL|\n",
      "|Michael|NULL|  NULL|\n",
      "|Michael|NULL|  NULL|\n",
      "|Michael|NULL|  NULL|\n",
      "|   Andy|  30|    40|\n",
      "|   Andy|  31|    41|\n",
      "| Justin|  18|    28|\n",
      "| Justin|  19|    29|\n",
      "+-------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.select(\n",
    "    people_df.name, people_df.age, (people_df.age + 10).alias(\"age+10\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The column object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.column.Column'> <class 'pyspark.sql.column.Column'>\n"
     ]
    }
   ],
   "source": [
    "print(type(people_df.age), type(people_df[\"age\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+\n",
      "|   name| age|age+10|\n",
      "+-------+----+------+\n",
      "|   NULL|NULL|  NULL|\n",
      "|Michael|NULL|  NULL|\n",
      "|Michael|NULL|  NULL|\n",
      "|Michael|NULL|  NULL|\n",
      "|   Andy|  30|    40|\n",
      "|   Andy|  31|    41|\n",
      "| Justin|  18|    28|\n",
      "| Justin|  19|    29|\n",
      "+-------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.select(\n",
    "    people_df[\"name\"], people_df[\"age\"], (people_df[\"age\"] + 10).alias(\"age+10\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.withColumn() → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "Returns a new DataFrame by adding a column or replacing the existing column that has the same name.\n",
    "The column expression must be an expression over this DataFrame; attempting to add a column from some other DataFrame will raise an error.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    colName str\n",
    "    string, name of the new column.\n",
    "\n",
    "    col Column\n",
    "    a Column expression for the new column.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame\n",
    "    DataFrame with new or replaced column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+----+\n",
      "| age|height|   name|age2|\n",
      "+----+------+-------+----+\n",
      "|NULL|  NULL|   NULL|NULL|\n",
      "|NULL|  NULL|Michael|NULL|\n",
      "|NULL|    80|Michael|NULL|\n",
      "|NULL|    80|Michael|NULL|\n",
      "|  30|    80|   Andy|  32|\n",
      "|  31|    80|   Andy|  33|\n",
      "|  18|    80| Justin|  20|\n",
      "|  19|    80| Justin|  21|\n",
      "+----+------+-------+----+\n",
      "\n",
      "+----+------+-------+----+\n",
      "| age|height|   name|age2|\n",
      "+----+------+-------+----+\n",
      "|NULL|  NULL|   NULL|NULL|\n",
      "|NULL|  NULL|Michael|NULL|\n",
      "|NULL|    80|Michael|NULL|\n",
      "|NULL|    80|Michael|NULL|\n",
      "|  30|    80|   Andy|  32|\n",
      "|  31|    80|   Andy|  33|\n",
      "|  18|    80| Justin|  20|\n",
      "|  19|    80| Justin|  21|\n",
      "+----+------+-------+----+\n",
      "\n",
      "+----+------+-------+----+\n",
      "| age|height|   name|age2|\n",
      "+----+------+-------+----+\n",
      "|NULL|  NULL|   NULL|NULL|\n",
      "|NULL|  NULL|Michael|NULL|\n",
      "|NULL|    80|Michael|NULL|\n",
      "|NULL|    80|Michael|NULL|\n",
      "|  30|    80|   Andy|  32|\n",
      "|  31|    80|   Andy|  33|\n",
      "|  18|    80| Justin|  20|\n",
      "|  19|    80| Justin|  21|\n",
      "+----+------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.withColumn(colName=\"age2\", col=people_df[\"age\"] + 2).show()\n",
    "people_df.withColumn(colName=\"age2\", col=people_df.age + 2).show()\n",
    "people_df.withColumn(colName=\"age2\", col=col(\"age\") + 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.withColumnRenamed() → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "Returns a new DataFrame by renaming an existing column. This is a no-op if the schema doesn’t contain the given column name.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    existing str\n",
    "    string, name of the existing column to rename.\n",
    "\n",
    "    new str\n",
    "    string, new name of the column.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame\n",
    "    DataFrame with renamed column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "| age|height|newName|\n",
      "+----+------+-------+\n",
      "|NULL|  NULL|   NULL|\n",
      "|NULL|  NULL|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|  30|    80|   Andy|\n",
      "|  31|    80|   Andy|\n",
      "|  18|    80| Justin|\n",
      "|  19|    80| Justin|\n",
      "+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.withColumnRenamed(existing=\"name\", new=\"newName\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.withColumns() → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "Returns a new DataFrame by adding multiple columns or replacing the existing columns that have the same names.\n",
    "The colsMap is a map of column name and column, the column must only refer to attributes supplied by this Dataset. It is an error to add columns that refer to some other Dataset.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    cols Mapdict\n",
    "    a dict of column name and Column. Currently, only a single map is supported.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame\n",
    "    DataFrame with new or replaced columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+----+----+\n",
      "| age|height|   name|age2|age3|\n",
      "+----+------+-------+----+----+\n",
      "|NULL|  NULL|   NULL|NULL|NULL|\n",
      "|NULL|  NULL|Michael|NULL|NULL|\n",
      "|NULL|    80|Michael|NULL|NULL|\n",
      "|NULL|    80|Michael|NULL|NULL|\n",
      "|  30|    80|   Andy|  32|  33|\n",
      "|  31|    80|   Andy|  33|  34|\n",
      "|  18|    80| Justin|  20|  21|\n",
      "|  19|    80| Justin|  21|  22|\n",
      "+----+------+-------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.withColumns({\"age2\": people_df.age + 2, \"age3\": people_df.age + 3}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.withColumnsRenamed() → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "Returns a new DataFrame by renaming multiple columns. This is a no-op if the schema doesn’t contain the given column names.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    cols Mapdict\n",
    "    a dict of existing column names and corresponding desired column names. Currently, only a single map is supported.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame\n",
    "    DataFrame with renamed columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "|age4|height|newName|\n",
      "+----+------+-------+\n",
      "|NULL|  NULL|   NULL|\n",
      "|NULL|  NULL|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|NULL|    80|Michael|\n",
      "|  30|    80|   Andy|\n",
      "|  31|    80|   Andy|\n",
      "|  18|    80| Justin|\n",
      "|  19|    80| Justin|\n",
      "+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.withColumnsRenamed({\"age\": \"age4\", \"name\": \"newName\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.groupBy() → GroupedData\n",
    "\n",
    "Groups the DataFrame using the specified columns, so we can run aggregation on them. See GroupedData for all the available aggregate functions.\n",
    "\n",
    "groupby() is an alias for groupBy().\n",
    "\n",
    "`in momory operation`\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    cols list, str or Column\n",
    "    columns to group by. Each element should be a column name (string) or an expression (Column) or list of them.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    GroupedData\n",
    "    Grouped data by given columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroupedData[grouping expressions: [Company], value: [Company: string, Person: string ... 1 more field], type: GroupBy]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.group.GroupedData"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sales_info_df.groupBy(\"Company\"))\n",
    "type(sales_info_df.groupBy(\"Company\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Company: string, avg(Sales): double]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_info_df.groupBy(\"Company\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|Company|       avg(Sales)|\n",
      "+-------+-----------------+\n",
      "|   APPL|            370.0|\n",
      "|   GOOG|            220.0|\n",
      "|     FB|            610.0|\n",
      "|   MSFT|322.3333333333333|\n",
      "+-------+-----------------+\n",
      "\n",
      "+-------+----------+\n",
      "|Company|min(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|     130.0|\n",
      "|   GOOG|     120.0|\n",
      "|     FB|     350.0|\n",
      "|   MSFT|     124.0|\n",
      "+-------+----------+\n",
      "\n",
      "+-------+----------+\n",
      "|Company|max(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|     750.0|\n",
      "|   GOOG|     340.0|\n",
      "|     FB|     870.0|\n",
      "|   MSFT|     600.0|\n",
      "+-------+----------+\n",
      "\n",
      "+-------+-----------------+\n",
      "|Company|       avg(Sales)|\n",
      "+-------+-----------------+\n",
      "|   APPL|            370.0|\n",
      "|   GOOG|            220.0|\n",
      "|     FB|            610.0|\n",
      "|   MSFT|322.3333333333333|\n",
      "+-------+-----------------+\n",
      "\n",
      "+-------+-----+\n",
      "|Company|count|\n",
      "+-------+-----+\n",
      "|   APPL|    4|\n",
      "|   GOOG|    3|\n",
      "|     FB|    2|\n",
      "|   MSFT|    3|\n",
      "+-------+-----+\n",
      "\n",
      "+-------+-----+\n",
      "|Company|count|\n",
      "+-------+-----+\n",
      "|   APPL|    4|\n",
      "|   GOOG|    3|\n",
      "|     FB|    2|\n",
      "|   MSFT|    3|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_info_df.groupBy(\"Company\").mean().show()\n",
    "sales_info_df.groupBy(\"Company\").min().show()\n",
    "sales_info_df.groupBy(\"Company\").max().show()\n",
    "sales_info_df.groupBy(\"Company\").avg().show()\n",
    "sales_info_df.groupBy(\"Company\").count().show()\n",
    "sales_info_df.groupBy(\"Company\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|count_sales|\n",
      "+-----------+\n",
      "|         11|\n",
      "+-----------+\n",
      "\n",
      "+-----------------+\n",
      "|    average_sales|\n",
      "+-----------------+\n",
      "|360.5833333333333|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------------------------------------+\n",
      "|format_number(stddev(Sales) AS stddev_sales, 2)|\n",
      "+-----------------------------------------------+\n",
      "|                                         250.09|\n",
      "+-----------------------------------------------+\n",
      "\n",
      "+---------------+\n",
      "|formatted sales|\n",
      "+---------------+\n",
      "|         200.00|\n",
      "|         120.00|\n",
      "|         340.00|\n",
      "|         600.00|\n",
      "|         124.00|\n",
      "|         243.00|\n",
      "|         870.00|\n",
      "|         350.00|\n",
      "|         250.00|\n",
      "|         130.00|\n",
      "|         750.00|\n",
      "|         350.00|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_info_df.select(count_distinct(\"Sales\").alias(\"count_sales\")).show()\n",
    "sales_info_df.select(avg(\"Sales\").alias(\"average_sales\")).show()\n",
    "sales_info_df.select(format_number(stddev(\"Sales\").alias(\"stddev_sales\"), 2)).show()\n",
    "\n",
    "\n",
    "sales_info_df.select(format_number(\"sales\", 2).alias(\"formatted sales\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.orderBy() → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "Returns a new DataFrame sorted by the specified column(s).\n",
    "\n",
    "`OrderBy is just an alias for the sort function.`\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    cols str, list, or Column, optional\n",
    "    list of Column or column names to sort by.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame\n",
    "    Sorted DataFrame.\n",
    "\n",
    "### Other Parameters\n",
    "\n",
    "    ascending bool or list, optional, default True\n",
    "    boolean or list of boolean. Sort ascending vs. descending. Specify list for multiple sort orders. If a list is specified, the length of the list must equal the length of the cols.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|     FB|   Carl|870.0|\n",
      "+-------+-------+-----+\n",
      "\n",
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|     FB|   Carl|870.0|\n",
      "+-------+-------+-----+\n",
      "\n",
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|     FB|   Carl|870.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "+-------+-------+-----+\n",
      "\n",
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|     FB|   Carl|870.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_info_df.orderBy(sales_info_df[\"Sales\"], ascending=True).show()\n",
    "sales_info_df.orderBy(sales_info_df[\"Sales\"].asc()).show()\n",
    "sales_info_df.orderBy(sales_info_df[\"Sales\"].desc()).show()\n",
    "sales_info_df.orderBy(sales_info_df.Sales.desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame Attributes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## columns\n",
    "\n",
    "Retrieves the names of all columns in the DataFrame as a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'height', 'name']"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dtypes\n",
    "\n",
    "Returns all column names and their data types as a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('age', 'bigint'), ('height', 'bigint'), ('name', 'string')]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## isStreaming\n",
    "\n",
    "Returns True if this DataFrame contains one or more sources that continuously return data as it arrives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## schema\n",
    "\n",
    "Returns the schema of this DataFrame as a pyspark.sql.types.StructType.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('age', LongType(), True), StructField('height', LongType(), True), StructField('name', StringType(), True)])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## na\n",
    "\n",
    "Returns a DataFrameNaFunctions for handling missing values.\n",
    "\n",
    "> class pyspark.sql.DataFrameNaFunctions(df: pyspark.sql.dataframe.DataFrame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop([how, thresh, subset]): Returns a new DataFrame omitting rows with null values.\n",
    "\n",
    "1.  howstr, optional ‘any’ or ‘all’. If ‘any’, drop a row if it contains any nulls. If ‘all’, drop a row only if all its values are null.\n",
    "2.  thresh: int, optional default None If specified, drop rows that have less than thresh non-null values. This overwrites the how parameter.\n",
    "3.  subset str, tuple or list, optional optional list of column names to consider.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| NULL|\n",
      "|emp2| NULL| NULL|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "|NULL| NULL| NULL|\n",
      "|emp5| emp5|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "|emp5| emp5|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| NULL|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "|emp5| emp5|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.na.drop(\n",
    "    thresh=2\n",
    ").show()  # should have atleast 2 non null values, else drop row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "|emp5| emp5|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.na.drop(how=\"any\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| NULL|\n",
      "|emp2| NULL| NULL|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "|emp5| emp5|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.na.drop(how=\"all\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| NULL|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "|emp5| emp5|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.na.drop(how=\"all\", subset=[\"Name\", \"Sales\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "|emp5| emp5|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.na.drop(how=\"any\", subset=[\"Name\", \"Sales\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| NULL|\n",
      "|emp4|Cindy|456.0|\n",
      "|emp5| emp5|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.na.drop(how=\"any\", subset=[\"Name\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fill(value[, subset]): Replace null values, alias for na.fill().\n",
    "\n",
    "1.  value int, float, string, bool or dict Value to replace null values with. If the value is a dict, then subset is ignored and value must be a mapping from column name (string) to replacement value. The replacement value must be an int, float, boolean, or string.\n",
    "\n",
    "2.  subset str, tuple or list, optional optional list of column names to consider. Columns specified in subset that do not have matching data types are ignored. For example, if value is a string, and subset contains a non-string column, then the non-string column is simply ignored.\n",
    "\n",
    "`Replacement Datatype should be same as the current datatype`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| NULL|\n",
      "|emp2| NULL| NULL|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "|NULL| NULL| NULL|\n",
      "|emp5| emp5|456.0|\n",
      "+----+-----+-----+\n",
      "\n",
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.show()\n",
    "contains_null_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| 50.0|\n",
      "|emp2| NULL| 50.0|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "|NULL| NULL| 50.0|\n",
      "|emp5| emp5|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.na.fill(50).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+\n",
      "|        Id|      Name|Sales|\n",
      "+----------+----------+-----+\n",
      "|      emp1|      John| NULL|\n",
      "|      emp2|fill value| NULL|\n",
      "|      emp3|fill value|345.0|\n",
      "|      emp4|     Cindy|456.0|\n",
      "|fill value|fill value| NULL|\n",
      "|      emp5|      emp5|456.0|\n",
      "+----------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.na.fill(\"fill value\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----+\n",
      "|  Id|      Name|Sales|\n",
      "+----+----------+-----+\n",
      "|emp1|      John| NULL|\n",
      "|emp2|fill value| NULL|\n",
      "|emp3|fill value|345.0|\n",
      "|emp4|     Cindy|456.0|\n",
      "|NULL|fill value| NULL|\n",
      "|emp5|      emp5|456.0|\n",
      "+----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.na.fill(\"fill value\", subset=[\"Name\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|avg(Sales)|\n",
      "+----------+\n",
      "|     419.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.select(mean(contains_null_df.Sales)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mean = contains_null_df.select(mean(contains_null_df.Sales)).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|419.0|\n",
      "|emp2| NULL|419.0|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "|NULL| NULL|419.0|\n",
      "|emp5| emp5|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.na.fill(my_mean, subset=[\"Sales\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|419.0|\n",
      "|emp2| NULL|419.0|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "|NULL| NULL|419.0|\n",
      "|emp5| emp5|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.na.fill(\n",
    "    contains_null_df.select(mean(contains_null_df.Sales)).collect()[0][0],\n",
    "    subset=[\"Sales\"],\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replace(to_replace[, value, subset]): Returns a new DataFrame replacing a value with another value.\n",
    "\n",
    "1.  to_replace bool, int, float, string, list or dict Value to be replaced. If the value is a dict, then value is ignored or can be omitted, and to_replace must be a mapping between a value and a replacement.\n",
    "\n",
    "2.  value bool, int, float, string or None, optional The replacement value must be a bool, int, float, string or None. If value is a list, value should be of the same length and type as to_replace. If value is a scalar and to_replace is a sequence, then value is used as a replacement for each item in to_replace.\n",
    "\n",
    "3.  subset list, optional optional list of column names to consider. Columns specified in subset that do not have matching data types are ignored. For example, if value is a string, and subset contains a non-string column, then the non-string column is simply ignored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| NULL|\n",
      "|emp2| NULL| NULL|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "|NULL| NULL| NULL|\n",
      "|emp5| emp5|456.0|\n",
      "+----+-----+-----+\n",
      "\n",
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| Nhoj| NULL|\n",
      "|emp2| NULL| NULL|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "|NULL| NULL| NULL|\n",
      "|emp5| emp5|456.0|\n",
      "+----+-----+-----+\n",
      "\n",
      "+----+------+-----+\n",
      "|  Id|  Name|Sales|\n",
      "+----+------+-----+\n",
      "|1pme|  John| NULL|\n",
      "|emp2|  NULL| NULL|\n",
      "|emp3|  NULL|345.0|\n",
      "|emp4|Bhindi|456.0|\n",
      "|NULL|  NULL| NULL|\n",
      "|emp5|  emp5|456.0|\n",
      "+----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contains_null_df.show()\n",
    "contains_null_df.na.replace(\"John\", \"Nhoj\").show()\n",
    "contains_null_df.na.replace(\n",
    "    [\"emp1\", \"Cindy\"], [\"1pme\", \"Bhindi\"], subset=[\"Id\", \"Name\"]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing With Dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "appl_stock_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.date(2010, 1, 4), Open=213.429998, High=214.499996, Low=212.38000099999996, Close=214.009998, Volume=123432400, Adj Close=27.727039)]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appl_stock_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+----+-----+----------+----+\n",
      "|Date      |day_of_month|day_of_year|year|month|weekofyear|hour|\n",
      "+----------+------------+-----------+----+-----+----------+----+\n",
      "|2010-01-04|4           |4          |2010|1    |1         |0   |\n",
      "|2010-01-05|5           |5          |2010|1    |1         |0   |\n",
      "|2010-01-06|6           |6          |2010|1    |1         |0   |\n",
      "|2010-01-07|7           |7          |2010|1    |1         |0   |\n",
      "|2010-01-08|8           |8          |2010|1    |1         |0   |\n",
      "|2010-01-11|11          |11         |2010|1    |2         |0   |\n",
      "|2010-01-12|12          |12         |2010|1    |2         |0   |\n",
      "|2010-01-13|13          |13         |2010|1    |2         |0   |\n",
      "|2010-01-14|14          |14         |2010|1    |2         |0   |\n",
      "|2010-01-15|15          |15         |2010|1    |2         |0   |\n",
      "|2010-01-19|19          |19         |2010|1    |3         |0   |\n",
      "|2010-01-20|20          |20         |2010|1    |3         |0   |\n",
      "|2010-01-21|21          |21         |2010|1    |3         |0   |\n",
      "|2010-01-22|22          |22         |2010|1    |3         |0   |\n",
      "|2010-01-25|25          |25         |2010|1    |4         |0   |\n",
      "|2010-01-26|26          |26         |2010|1    |4         |0   |\n",
      "|2010-01-27|27          |27         |2010|1    |4         |0   |\n",
      "|2010-01-28|28          |28         |2010|1    |4         |0   |\n",
      "|2010-01-29|29          |29         |2010|1    |4         |0   |\n",
      "|2010-02-01|1           |32         |2010|2    |5         |0   |\n",
      "|2010-02-02|2           |33         |2010|2    |5         |0   |\n",
      "|2010-02-03|3           |34         |2010|2    |5         |0   |\n",
      "|2010-02-04|4           |35         |2010|2    |5         |0   |\n",
      "|2010-02-05|5           |36         |2010|2    |5         |0   |\n",
      "|2010-02-08|8           |39         |2010|2    |6         |0   |\n",
      "|2010-02-09|9           |40         |2010|2    |6         |0   |\n",
      "|2010-02-10|10          |41         |2010|2    |6         |0   |\n",
      "|2010-02-11|11          |42         |2010|2    |6         |0   |\n",
      "|2010-02-12|12          |43         |2010|2    |6         |0   |\n",
      "|2010-02-16|16          |47         |2010|2    |7         |0   |\n",
      "|2010-02-17|17          |48         |2010|2    |7         |0   |\n",
      "|2010-02-18|18          |49         |2010|2    |7         |0   |\n",
      "|2010-02-19|19          |50         |2010|2    |7         |0   |\n",
      "|2010-02-22|22          |53         |2010|2    |8         |0   |\n",
      "|2010-02-23|23          |54         |2010|2    |8         |0   |\n",
      "|2010-02-24|24          |55         |2010|2    |8         |0   |\n",
      "|2010-02-25|25          |56         |2010|2    |8         |0   |\n",
      "|2010-02-26|26          |57         |2010|2    |8         |0   |\n",
      "|2010-03-01|1           |60         |2010|3    |9         |0   |\n",
      "|2010-03-02|2           |61         |2010|3    |9         |0   |\n",
      "|2010-03-03|3           |62         |2010|3    |9         |0   |\n",
      "|2010-03-04|4           |63         |2010|3    |9         |0   |\n",
      "|2010-03-05|5           |64         |2010|3    |9         |0   |\n",
      "|2010-03-08|8           |67         |2010|3    |10        |0   |\n",
      "|2010-03-09|9           |68         |2010|3    |10        |0   |\n",
      "|2010-03-10|10          |69         |2010|3    |10        |0   |\n",
      "|2010-03-11|11          |70         |2010|3    |10        |0   |\n",
      "|2010-03-12|12          |71         |2010|3    |10        |0   |\n",
      "|2010-03-15|15          |74         |2010|3    |11        |0   |\n",
      "|2010-03-16|16          |75         |2010|3    |11        |0   |\n",
      "+----------+------------+-----------+----+-----+----------+----+\n",
      "only showing top 50 rows\n",
      "\n",
      "+-------------+-------------+\n",
      "|get_only_year|average_close|\n",
      "+-------------+-------------+\n",
      "|         2016|       104.60|\n",
      "|         2015|       120.04|\n",
      "|         2010|       259.84|\n",
      "|         2014|       295.40|\n",
      "|         2011|       364.00|\n",
      "|         2013|       472.63|\n",
      "|         2012|       576.05|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "appl_stock_df.select(\n",
    "    col(\"Date\"),\n",
    "    dayofmonth(col(\"Date\")).alias(\"day_of_month\"),\n",
    "    dayofyear(col(\"Date\")).alias(\"day_of_year\"),\n",
    "    year(col(\"Date\")).alias(\"year\"),\n",
    "    month(col(\"Date\")).alias(\"month\"),\n",
    "    weekofyear(col(\"Date\")).alias(\"weekofyear\"),\n",
    "    hour(col(\"Date\")).alias(\"hour\"),\n",
    ").show(50, truncate=False)\n",
    "\n",
    "appl_stock_df.withColumn(\"get_only_year\", year(appl_stock_df.Date)).groupBy(\n",
    "    \"get_only_year\"\n",
    ").mean().select(\n",
    "    col(\"get_only_year\"), format_number(col(\"avg(Close)\"), 2).alias(\"average_close\")\n",
    ").orderBy(\n",
    "    \"average_close\", ascending=True\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitions and Partitioning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spark.repartition(num_partitions: int) → ps.DataFrame\n",
    "\n",
    "Returns a new DataFrame partitioned by the given partitioning expressions. The resulting DataFrame is hash partitioned.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    numPartitions int\n",
    "    can be an int to specify the target number of partitions or a Column. If it is a Column, it will be used as the first partitioning column. If not specified, the default number of partitions is used.\n",
    "\n",
    "    cols str or Column\n",
    "    partitioning columns.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    Repartitioned DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "+------------------+-----+\n",
      "|spark_partition_id|count|\n",
      "+------------------+-----+\n",
      "|                 0| 1762|\n",
      "+------------------+-----+\n",
      "\n",
      "+------------------+-----+\n",
      "|spark_partition_id|count|\n",
      "+------------------+-----+\n",
      "|                 0|  105|\n",
      "|                 1|   87|\n",
      "|                 2|   73|\n",
      "|                 3|   82|\n",
      "|                 4|   80|\n",
      "|                 5|   80|\n",
      "|                 6|   80|\n",
      "|                 7|  115|\n",
      "|                 8|   72|\n",
      "|                 9|   88|\n",
      "|                10|   77|\n",
      "|                11|   96|\n",
      "|                12|   85|\n",
      "|                13|  104|\n",
      "|                14|   84|\n",
      "|                15|   96|\n",
      "|                16|   76|\n",
      "|                17|   97|\n",
      "|                18|   89|\n",
      "|                19|   96|\n",
      "+------------------+-----+\n",
      "\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(appl_stock_df.rdd.getNumPartitions())\n",
    "get_records_per_partition(appl_stock_df).show()\n",
    "\n",
    "\n",
    "partitioned_appl_stock_df = appl_stock_df.withColumn(\"my_part_year\", year(col(\"Date\")))\n",
    "partitioned_appl_stock_df = partitioned_appl_stock_df.repartition(\n",
    "    20, \"my_part_year\", \"Close\"\n",
    ")\n",
    "\n",
    "\n",
    "get_records_per_partition(partitioned_appl_stock_df).show()\n",
    "\n",
    "\n",
    "print(partitioned_appl_stock_df.rdd.getNumPartitions())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
