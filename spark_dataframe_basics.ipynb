{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.functions import col, desc, asc\n",
    "from pyspark.sql.types import StringType, StructField, StructType, IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Spark Session\n",
    "\n",
    "> Builder is a class whereas builder initializes the Builder class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Spark Basics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storage Levels in Spark\n",
    "\n",
    "## Memory only Storage level\n",
    "\n",
    "StorageLevel.MEMORY_ONLY is the default behavior of the RDD cache() method and stores the RDD or DataFrame as deserialized objects to JVM memory. When there is not enough memory available it will not save DataFrame of some partitions and these will be re-computed as and when required.\n",
    "\n",
    "This takes more memory. but unlike RDD, this would be slower than MEMORY_AND_DISK level as it recomputes the unsaved partitions, and recomputing the in-memory columnar representation of the underlying table is expensive.\n",
    "\n",
    "## Serialize in Memory\n",
    "\n",
    "StorageLevel.MEMORY_ONLY_SER is the same as MEMORY_ONLY but the difference being it stores `RDD as serialized objects to JVM memory.` It takes lesser memory (space-efficient) than MEMORY_ONLY as it saves objects as serialized and takes an additional few more CPU cycles in order to deserialize.\n",
    "\n",
    "## Memory only and Replicate\n",
    "\n",
    "StorageLevel.MEMORY_ONLY_2 is same as MEMORY_ONLY storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "## Serialized in Memory and Replicate\n",
    "\n",
    "StorageLevel.MEMORY_ONLY_SER_2 is same as MEMORY_ONLY_SER storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "## Memory and Disk Storage level\n",
    "\n",
    "StorageLevel.MEMORY_AND_DISK is the default behavior of the DataFrame or Dataset. In this Storage Level, The DataFrame will be stored in JVM memory as deserialized objects. When required storage is greater than available memory, it stores some of the excess partitions into a disk and reads the data from the disk when required. It is slower as there is I/O involved.\n",
    "\n",
    "## Serialize in Memory and Disk\n",
    "\n",
    "StorageLevel.MEMORY_AND_DISK_SER is same as MEMORY_AND_DISK storage level difference being it serializes the DataFrame objects in memory and on disk when space is not available.\n",
    "\n",
    "## Memory, Disk and Replicate\n",
    "\n",
    "StorageLevel.MEMORY_AND_DISK_2 is Same as MEMORY_AND_DISK storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "## Serialize in Memory, Disk and Replicate\n",
    "\n",
    "StorageLevel.MEMORY_AND_DISK_SER_2 is same as MEMORY_AND_DISK_SER storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "## Disk only storage level\n",
    "\n",
    "In StorageLevel.DISK_ONLY storage level, DataFrame is stored only on disk and the CPU computation time is high as I/O involved.\n",
    "\n",
    "## Disk only and Replicate\n",
    "\n",
    "StorageLevel.DISK_ONLY_2 is same as DISK_ONLY storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "| Storage Level       | Space used | CPU time | In memory | On-disk | Serialized | Recompute some partitions |\n",
    "| ------------------- | ---------- | -------- | --------- | ------- | ---------- | ------------------------- |\n",
    "| MEMORY_ONLY         | High       | Low      | Y         | N       | N          | Y                         |\n",
    "| MEMORY_ONLY_SER     | Low        | High     | Y         | N       | Y          | Y                         |\n",
    "| MEMORY_AND_DISK     | High       | Medium   | Some      | Some    | Some       | N                         |\n",
    "| MEMORY_AND_DISK_SER | Low        | High     | Some      | Some    | Y          | N                         |\n",
    "| DISK_ONLY           | Low        | High     | N         | Y       | Y          | N                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading a Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reaing people.json file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df = spark.read.json(\"./datasets/people.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Files with custom Schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = [\n",
    "    StructField(name=\"age\", dataType=IntegerType(), nullable=True),\n",
    "    StructField(name=\"name\", dataType=StringType(), nullable=True),\n",
    "]\n",
    "final_structure = StructType(fields=data_schema)\n",
    "\n",
    "people_df_custom_schema = spark.read.json(\n",
    "    \"./datasets/people.json\", schema=final_structure\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.show() → None[source]\n",
    "\n",
    "Parameters\n",
    "\n",
    "1. n int, optional Number of rows to show.\n",
    "\n",
    "2. truncate bool or int, optional If set to True, truncate strings longer than 20 chars by default. If set to a number greater than one, truncates long strings to length truncate and align cells right.\n",
    "3. vertical bool, optional If set to True, print output rows vertically (one line per column value).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|NULL|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------\n",
      " age  | NULL    \n",
      " name | Michael \n",
      "-RECORD 1-------\n",
      " age  | 30      \n",
      " name | Andy    \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.show(truncate=False, n=2, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.printSchema() → None\n",
    "\n",
    "1. level int, optional, default -> None : How many levels to print for nested schemas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_df = spark.createDataFrame([(1, (2, 2))], [\"a\", \"b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: struct (nullable = true)\n",
      "\n",
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: struct (nullable = true)\n",
      " |    |-- _1: long (nullable = true)\n",
      " |    |-- _2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nested_df.printSchema(1)\n",
    "nested_df.printSchema(2)\n",
    "del nested_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.describe() → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "cols str, list, optional Column name or list of column names to describe by (default All columns).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------+\n",
      "|summary|               age|   name|\n",
      "+-------+------------------+-------+\n",
      "|  count|                 2|      3|\n",
      "|   mean|              24.5|   NULL|\n",
      "| stddev|7.7781745930520225|   NULL|\n",
      "|    min|                19|   Andy|\n",
      "|    max|                30|Michael|\n",
      "+-------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               age|\n",
      "+-------+------------------+\n",
      "|  count|                 2|\n",
      "|   mean|              24.5|\n",
      "| stddev|7.7781745930520225|\n",
      "|    min|                19|\n",
      "|    max|                30|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.describe([\"age\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.agg() → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "Aggregate on the entire DataFrame without groups (shorthand for df.groupBy().agg()).\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    exprs Column or dict of key and value strings, Columns or expressions to aggregate DataFrame by.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame: Aggregated DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|max(age)|\n",
      "+--------+\n",
      "|      30|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.agg({\"age\": \"max\"}).show()\n",
    "# people_df.groupBy().agg({\"age\": \"max\"}).show() # same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.alias(alias: str) → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    alias str: an alias name to be set for the DataFrame.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame: Aliased DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+-------+\n",
      "| age|   name| age|   name|\n",
      "+----+-------+----+-------+\n",
      "|NULL|Michael|NULL|Michael|\n",
      "|  30|   Andy|  30|   Andy|\n",
      "|  19| Justin|  19| Justin|\n",
      "+----+-------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df_alias_1 = people_df.alias(\"people_df_1\")\n",
    "people_df_alias_2 = people_df.alias(\"people_df_2\")\n",
    "\n",
    "people_df_alias_1.join(\n",
    "    people_df_alias_2, col(\"people_df_1.name\") == col(\"people_df_2.name\"), how=\"inner\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.approxQuantile() → Union[List[float], List[List[float]]]\n",
    "\n",
    "Calculates the approximate quantiles of numerical columns of a DataFrame.\n",
    "\n",
    "The result of this algorithm has the following deterministic bound: If the DataFrame has N elements and if we request the quantile at probability p up to error err, then the algorithm will return a sample x from the DataFrame so that the exact rank of x is close to (p \\* N). More precisely,\n",
    "\n",
    "floor((p - err) _ N) <= rank(x) <= ceil((p + err) _ N).\n",
    "\n",
    "This method implements a variation of the Greenwald-Khanna algorithm (with some speed optimizations). The algorithm was first present in [[https://doi.org/10.1145/375663.375670 Space-efficient Online Computation of Quantile Summaries]] by Greenwald and Khanna.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    1. col: str, tuple or list: Can be a single column name, or a list of names for multiple columns.\n",
    "\n",
    "    2. probabilities list or tuple: a list of quantile probabilities Each number must belong to [0, 1]. For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n",
    "\n",
    "    3. relativeError float: The relative target precision to achieve (>= 0). If set to zero, the exact quantiles are computed, which could be very expensive. Note that values greater than 1 are accepted but gives the same result as 1.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    list\n",
    "    the approximate quantiles at the given probabilities.\n",
    "\n",
    "    If the input col is a string, the output is a list of floats.\n",
    "\n",
    "    If the input col is a list or tuple of strings, the output is also a\n",
    "    list, but each element in it is a list of floats, i.e., the output is a list of list of floats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19.0]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.approxQuantile(col=\"age\", probabilities=[0.5], relativeError=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.cache() → pyspark.sql.dataframe.DataFrame[source]\n",
    "\n",
    "Persists the DataFrame with the default storage level (MEMORY_AND_DISK).\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame: Cached DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, name: string]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.persist() → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    storageLevel StorageLevel: Storage level to set for persistence. Default is MEMORY_AND_DISK_DESER.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame: Persisted DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, name: string]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.checkpoint() → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "Returns a checkpointed version of this DataFrame. Checkpointing can be used to truncate the logical plan of this DataFrame, which is especially useful in iterative algorithms where the plan may grow exponentially. It will be saved to files inside the checkpoint directory set with SparkContext.setCheckpointDir().\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    eager bool, optional, default True: Whether to checkpoint this DataFrame immediately.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame: Checkpointed DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.coalesce(numPartitions: int) → pyspark.sql.dataframe.DataFrame[source]\n",
    "\n",
    "Similar to coalesce defined on an RDD, this operation results in a narrow dependency, e.g. if you go from 1000 partitions to 100 partitions, there will not be a shuffle, instead each of the 100 new partitions will claim 10 of the current partitions. If a larger number of partitions is requested, it will stay at the current number of partitions.\n",
    "\n",
    "However, if you’re doing a drastic coalesce, e.g. to numPartitions = 1, this may result in your computation taking place on fewer nodes than you like (e.g. one node in the case of numPartitions = 1). To avoid this, you can call repartition(). This will add a shuffle step, but means the current upstream partitions will be executed in parallel (per whatever the current partitioning is).\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    numPartitions int\n",
    "    specify the target number of partitions\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.coalesce(1).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.colRegex(colName: str) → pyspark.sql.column.Column\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    colName str\n",
    "    string, column name specified as a regex.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    Column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|NULL|\n",
      "|  30|\n",
      "|  19|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.select(people_df.colRegex(\"`(name)?+.+`\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.collect() → List[pyspark.sql.types.Row]\n",
    "\n",
    "Returns all the records as a list of Row.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    list\n",
    "    List of rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=None, name='Michael'),\n",
       " Row(age=30, name='Andy'),\n",
       " Row(age=19, name='Justin')]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.corr() → float\n",
    "\n",
    "Calculates the correlation of two columns of a DataFrame as a double value. Currently only supports the Pearson Correlation Coefficient. DataFrame.corr() and DataFrameStatFunctions.corr() are aliases of each other.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    col1 str\n",
    "    The name of the first column\n",
    "\n",
    "    col2 str\n",
    "    The name of the second column\n",
    "\n",
    "    method str, optional\n",
    "    The correlation method. Currently only supports “pearson”\n",
    "\n",
    "### Returns\n",
    "\n",
    "    float\n",
    "    Pearson Correlation Coefficient of two columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# people_df.corr(\"age\", \"name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.createGlobalTempView(name: str) → None\n",
    "\n",
    "Creates a global temporary view with this DataFrame.\n",
    "The lifetime of this temporary view is tied to this `Spark application`. throws TempTableAlreadyExistsException, if the view name already exists in the catalog.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    name str\n",
    "    Name of the view.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|NULL|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# people_df.createGlobalTempView(\"people_df_global_temp_view\")\n",
    "spark.sql(\"select * from global_temp.people_df_global_temp_view\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|NULL|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.createOrReplaceGlobalTempView(\"people_df_global_temp_view\")\n",
    "spark.sql(\"select * from global_temp.people_df_global_temp_view\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.createTempView(name: str) → None\n",
    "\n",
    "Creates a local temporary view with this DataFrame.\n",
    "\n",
    "The lifetime of this temporary table is tied to the `SparkSession` that was used to create this DataFrame.throws TempTableAlreadyExistsException, if the view name already exists in the catalog.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    name str\n",
    "    Name of the view.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|NULL|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# people_df.createTempView(\"people_df_temp_view\")\n",
    "spark.sql(\"select * from people_df_temp_view\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|NULL|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.createOrReplaceTempView(\"people_df_temp_view\")\n",
    "spark.sql(\"select * from people_df_temp_view\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.crossJoin(other: pyspark.sql.dataframe.DataFrame) → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "Returns the cartesian product with another DataFrame.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    other DataFrame\n",
    "    Right side of the cartesian product.\n",
    "\n",
    "### Returns\n",
    "\n",
    "    DataFrame\n",
    "    Joined DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+\n",
      "| age|   name| age|\n",
      "+----+-------+----+\n",
      "|NULL|Michael|NULL|\n",
      "|NULL|Michael|  30|\n",
      "|NULL|Michael|  19|\n",
      "|  30|   Andy|NULL|\n",
      "|  30|   Andy|  30|\n",
      "|  30|   Andy|  19|\n",
      "|  19| Justin|NULL|\n",
      "|  19| Justin|  30|\n",
      "|  19| Justin|  19|\n",
      "+----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.crossJoin(people_df.select(\"age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame Attributes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## columns\n",
    "\n",
    "Retrieves the names of all columns in the DataFrame as a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'name']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dtypes\n",
    "\n",
    "Returns all column names and their data types as a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('age', 'bigint'), ('name', 'string')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## isStreaming\n",
    "\n",
    "Returns True if this DataFrame contains one or more sources that continuously return data as it arrives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## na\n",
    "\n",
    "Returns a DataFrameNaFunctions for handling missing values.\n",
    "\n",
    "> class pyspark.sql.DataFrameNaFunctions(df: pyspark.sql.dataframe.DataFrame)\n",
    "\n",
    "1. drop([how, thresh, subset]): Returns a new DataFrame omitting rows with null values.\n",
    "   1. howstr, optional ‘any’ or ‘all’. If ‘any’, drop a row if it contains any nulls. If ‘all’, drop a row only if all its values are null.\n",
    "   2. thresh: int, optional default None If specified, drop rows that have less than thresh non-null values. This overwrites the how parameter.\n",
    "   3. subset str, tuple or list, optional optional list of column names to consider.\n",
    "2. fill(value[, subset]): Replace null values, alias for na.fill().\n",
    "\n",
    "   1. value int, float, string, bool or dict Value to replace null values with. If the value is a dict, then subset is ignored and value must be a mapping from column name (string) to replacement value. The replacement value must be an int, float, boolean, or string.\n",
    "\n",
    "   2. subset str, tuple or list, optional optional list of column names to consider. Columns specified in subset that do not have matching data types are ignored. For example, if value is a string, and subset contains a non-string column, then the non-string column is simply ignored.\n",
    "\n",
    "3. replace(to_replace[, value, subset]): Returns a new DataFrame replacing a value with another value.\n",
    "\n",
    "   1. to_replace bool, int, float, string, list or dict Value to be replaced. If the value is a dict, then value is ignored or can be omitted, and to_replace must be a mapping between a value and a replacement.\n",
    "\n",
    "   2. value bool, int, float, string or None, optional The replacement value must be a bool, int, float, string or None. If value is a list, value should be of the same length and type as to_replace. If value is a scalar and to_replace is a sequence, then value is used as a replacement for each item in to_replace.\n",
    "\n",
    "   3. subset list, optional optional list of column names to consider. Columns specified in subset that do not have matching data types are ignored. For example, if value is a string, and subset contains a non-string column, then the non-string column is simply ignored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 50|Michael|\n",
      "| 30|   Andy|\n",
      "| 19| Justin|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.na.fill(50).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## schema\n",
    "\n",
    "Returns the schema of this DataFrame as a pyspark.sql.types.StructType.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('age', LongType(), True), StructField('name', StringType(), True)])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.schema"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
