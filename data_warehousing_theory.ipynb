{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Lake and Delta Lake\n",
    "\n",
    "- Data Lake: A data lake is a centralized repository that allows you to store all your structured and unstructured data at any scale. It can store data in its raw, unprocessed form and supports a variety of data types, including relational data, log files, images, and more.\n",
    "\n",
    "- Delta Lake: Delta Lake is an open-source storage layer that brings ACID (Atomicity, Consistency, Isolation, Durability) transactions to Apache Spark and big data workloads. It is designed to bring reliability to data lakes by providing features like ACID transactions, schema enforcement, and time travel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference between Data Lake and Delta Lake\n",
    "\n",
    "| Parameters                              | Delta Lake                                                                                                                          | Data Lake                                                                                                                                                                                                                                                                |\n",
    "| --------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| Data Consistency and ACID Transactions  | Traditional data lakes often struggle with data consistency, as they lack built-in transactional support.                           | In contrast, Delta Lake provides ACID transactions, ensuring that data changes are either fully applied or fully rolled back, maintaining the integrity of the data.                                                                                                     |\n",
    "| Schema Evolution and Evolution Tracking | In traditional data lakes, schema evolution can be challenging and often requires complex ETL processes.                            | Delta Lake simplifies schema evolution by allowing you to add, modify, or delete columns in a table without disrupting data pipelines.                                                                                                                                   |\n",
    "| Performance and Optimization            | Traditional data lakes may suffer from performance issues as data volumes grow, primarily due to the lack of optimization features. | Delta Lake addresses this challenge by implementing optimization techniques like data compaction and indexing. These optimizations significantly improve query performance, making Delta Lake a compelling choice for organizations with demanding analytical workloads. |\n",
    "| Data Lake Storage Costs                 | The cost of storing data in a data lake can be substantial, especially when dealing with large-scale datasets.                      | Delta Lake adopts a cost-effective approach by using file formats that reduce storage costs and improve compression.                                                                                                                                                     |\n",
    "| Data Quality and Data Governance        | Traditional data lakes may lack robust mechanisms for data quality checks and governance.                                           | Delta Lake incorporates features for data validation and governance, helping organizations maintain data quality and meet regulatory requirements effectively.                                                                                                           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of Data Lake\n",
    "\n",
    "ADLS typically refers to Azure Data Lake Storage, which is a cloud-based storage service provided by Microsoft Azure. Azure Data Lake Storage is designed to enable big data analytics and is integrated with various Azure services, making it a key component in Azure's data ecosystem.\n",
    "\n",
    "| Feature                    | Azure Data Lake Storage Gen1 (ADLS Gen1) | Azure Data Lake Storage Gen2 (ADLS Gen2)                                   |\n",
    "| -------------------------- | ---------------------------------------- | -------------------------------------------------------------------------- |\n",
    "| **Hierarchical Namespace** | Flat namespace; no directory structure   | Hierarchical namespace for efficient data organization                     |\n",
    "| **Security**               | - Authentication: Shared key, Azure AD   | - Authentication: Shared key, Azure AD, Azure AD Bearer Token              |\n",
    "| **Security**               | - Authorization: POSIX-style ACLs        | - Authorization: POSIX-style ACLs, Azure Blob Storage-style RBAC           |\n",
    "| **Performance**            | - Good read and write performance        | - Improved metadata operations, enhanced parallelism                       |\n",
    "| **Integration**            | - Independent service                    | - Built on Azure Blob Storage platform, compatible with Azure Blob Storage |\n",
    "| **Storage Tiers**          | - N/A                                    | - Supports hot and cold storage tiers                                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Lake vs Data Warehouse vs Data Lakehouse\n",
    "\n",
    "| Feature                         | Data Lake                                                                                         | Data Warehouse                                                         | Data Lakehouse                                                                                      |\n",
    "| ------------------------------- | ------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------- |\n",
    "| **Data Storage**                | Stores raw and unstructured data in its native format.                                            | Typically stores structured and processed data in tabular format.      | Stores both raw, unstructured data and structured, processed data.                                  |\n",
    "| **Schema**                      | Schema-on-read; supports schema flexibility.                                                      | Schema-on-write; enforces a predefined schema.                         | Supports both schema-on-read and schema-on-write.                                                   |\n",
    "| **Data Processing**             | Suited for big data processing and analytics using distributed computing frameworks.              | Optimized for complex SQL queries and analytics.                       | Combines big data processing capabilities with SQL analytics.                                       |\n",
    "| **Query Performance**           | May have slower query performance due to schema-on-read and raw data storage.                     | Offers fast query performance for structured data.                     | Combines the advantages of both Data Lake and Data Warehouse for balanced performance.              |\n",
    "| **Use Cases**                   | Ideal for storing large volumes of raw data for diverse analytics and machine learning use cases. | Best for structured, business-critical analytics and reporting.        | Suitable for both big data analytics and structured, ad-hoc queries.                                |\n",
    "| **Integration**                 | Integrates well with big data processing frameworks like Apache Spark, Hadoop.                    | Integrates with business intelligence tools and SQL-based analytics.   | Integrates with both big data processing frameworks and SQL analytics tools.                        |\n",
    "| **Cost Considerations**         | Generally more cost-effective for storing large volumes of raw data.                              | May have higher storage costs but optimized for query performance.     | Aims for a balance between cost-effective storage and optimized query performance.                  |\n",
    "| **Data Quality and Governance** | May lack built-in governance features; data quality checks may be challenging.                    | Typically includes features for data governance and quality assurance. | Incorporates data governance features, addressing concerns of both Data Lake and Data Warehouse.    |\n",
    "| **Transaction Support**         | Lacks built-in support for transactions.                                                          | Supports transactions for maintaining data consistency.                | Often includes transaction support, providing a middle ground between Data Lake and Data Warehouse. |\n",
    "| **Examples**                    | Azure Data Lake Storage, Amazon S3.                                                               | Amazon Redshift, Google BigQuery.                                      | Delta Lake, Databricks Delta, Snowflake.                                                            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference Between Schema on Read vs Schema on Write\n",
    "\n",
    "| Feature                 | Schema on Read                                                                                           | Schema on Write                                                                                                  |\n",
    "| ----------------------- | -------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------- |\n",
    "| **Definition**          | Defines the schema when the data is read.                                                                | Requires defining the schema before writing data.                                                                |\n",
    "| **Flexibility**         | Offers flexibility to read different schemas from the same data.                                         | Less flexible as the schema is enforced during the write operation.                                              |\n",
    "| **Data Storage**        | Raw data is stored without a predefined structure.                                                       | Data is stored in a structured format with a predefined schema.                                                  |\n",
    "| **Processing Overhead** | Minimal processing overhead during data ingestion.                                                       | Higher processing overhead during data ingestion to enforce schema.                                              |\n",
    "| **Query Performance**   | May experience slower query performance since the schema is interpreted during read operations.          | Typically provides faster query performance as the schema is predefined and optimized for queries.               |\n",
    "| **Use Cases**           | Suited for scenarios where data formats may evolve, and flexibility in data interpretation is essential. | Ideal for scenarios where data consistency and query performance are critical, such as in business intelligence. |\n",
    "| **Data Evolution**      | Adapts well to changes in data structures over time.                                                     | May require additional steps to handle changes in data structures, potentially involving ETL processes.          |\n",
    "| **Data Quality**        | Offers less control over data quality during ingestion.                                                  | Provides better control over data quality by enforcing a predefined schema.                                      |\n",
    "| **Examples**            | Apache Parquet, JSON, Avro.                                                                              | Relational databases, SQL-based storage systems.                                                                 |\n",
    "| **Complexity**          | Generally simpler in terms of schema management.                                                         | Can be more complex due to schema enforcement and management.                                                    |\n",
    "| **Scalability**         | Suited for scalable storage of diverse data formats.                                                     | May face challenges with diverse data formats and evolving schemas at scale.                                     |\n",
    "| **Best Suited For**     | Data lakes with diverse, raw, and evolving data.                                                         | Data warehouses with structured, business-critical data.                                                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delta Tables Hands on\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, substring, input_file_name, current_date, year\n",
    "\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    IntegerType,\n",
    "    LongType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    DateType,\n",
    "    DoubleType,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a spark session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = (\n",
    "    SparkSession.builder.appName(\"learn_delta_lake\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Cluster Information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Get the SparkContext\n",
    "sc = spark._jsc.sc()\n",
    "n_workers = (\n",
    "    len([executor.host() for executor in sc.statusTracker().getExecutorInfos()]) - 1\n",
    ")\n",
    "\n",
    "print(n_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_custom_schema = StructType(\n",
    "    [\n",
    "        StructField(\"my_date\", DateType(), nullable=True),\n",
    "        StructField(\"open\", DoubleType(), nullable=True),\n",
    "        StructField(\"high\", DoubleType(), nullable=True),\n",
    "        StructField(\"low\", DoubleType(), nullable=True),\n",
    "        StructField(\"close\", DoubleType(), nullable=True),\n",
    "        StructField(\"volume\", IntegerType(), nullable=True),\n",
    "        StructField(\"adj_close\", DoubleType(), nullable=True),\n",
    "    ]\n",
    ")\n",
    "my_test_data_df = (\n",
    "    spark.read.schema(my_custom_schema)\n",
    "    .options(header=True)\n",
    "    .csv(\"./datasets/appl_stock.csv\")\n",
    ")\n",
    "\n",
    "\n",
    "my_test_data_df = my_test_data_df.withColumn(\"year_only\", year(col(\"my_date\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write DataFrame as a Delta table\n",
    "\n",
    "| Feature                       | Delta Table                                      | Parquet Format                                          |\n",
    "| ----------------------------- | ------------------------------------------------ | ------------------------------------------------------- |\n",
    "| **ACID Transactions**         | Supports ACID transactions for data integrity.   | Does not support ACID transactions natively.            |\n",
    "| **Schema Evolution**          | Supports schema evolution for table evolution.   | Schema evolution is possible but limited.               |\n",
    "| **Time Travel**               | Supports time travel for querying past versions. | No built-in support for time travel.                    |\n",
    "| **Concurrency Control**       | Optimistic concurrency control for data writes.  | No built-in concurrency control for writes.             |\n",
    "| **Metadata Management**       | Maintains metadata for improved reliability.     | Limited metadata management compared to Delta.          |\n",
    "| **Data Storage Optimization** | Provides features like data compaction.          | Stores data efficiently but lacks Delta features.       |\n",
    "| **Compatibility with Spark**  | Designed for seamless integration with Spark.    | Can be used with Spark but lacks Delta features.        |\n",
    "| **Performance Optimization**  | Optimized for high-performance read and write.   | Efficient storage but may lack Delta's features.        |\n",
    "| **Open Source**               | Open-source Delta Lake is available on GitHub.   | Parquet is an open standard but lacks Delta's features. |\n",
    "| **Use Cases**                 | Best suited for data lakes, data engineering.    | Suitable for efficient storage and processing.          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrameWriter.format(source: str) → pyspark.sql.readwriter.DataFrameWriter\n",
    "\n",
    "Specifies the underlying output data source.\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "    source str\n",
    "    string, name of the data source, e.g. ‘json’, ‘parquet’.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_test_data_df.write.format(\"delta\").option(\"overwriteSchema\", \"true\").mode(\n",
    "    \"overwrite\"\n",
    ").partitionBy(\"year_only\").save(\"./output/my_test_data_df_delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Delta table by specific partition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+---------+\n",
      "|   my_date|              open|              high|               low|             close|   volume|         adj_close|year_only|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+---------+\n",
      "|2010-01-04|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|     2010|\n",
      "|2010-01-05|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|     2010|\n",
      "|2010-01-06|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|     2010|\n",
      "|2010-01-07|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|     2010|\n",
      "|2010-01-08|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|     2010|\n",
      "|2010-01-11|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|     2010|\n",
      "|2010-01-12|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|     2010|\n",
      "|2010-01-13|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|     2010|\n",
      "|2010-01-14|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|     2010|\n",
      "|2010-01-15|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|     2010|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samp1 = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .load(\"./output/my_test_data_df_delta\")\n",
    "    .where(\"year_only=2010\")\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "samp1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPEND Using Delta Lake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1762"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(\"./output/my_test_data_df_delta\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp1.write.format(\"delta\").mode(\"append\").save(\"./output/my_test_data_df_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1772"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(\"./output/my_test_data_df_delta\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data from delta tables via SQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+-----------------+----------+-----------------+---------+---------+---------+\n",
      "|   my_date|             open|             high|       low|            close|   volume|adj_close|year_only|\n",
      "+----------+-----------------+-----------------+----------+-----------------+---------+---------+---------+\n",
      "|2014-01-02|       555.680008|       557.029999|552.020004|        553.12999| 58671200|74.115916|     2014|\n",
      "|2014-01-03|552.8600230000001|       553.699989|540.429993|540.9800190000001| 98116900|72.487897|     2014|\n",
      "|2014-01-06|       537.450005|       546.800018|533.599983|       543.929993|103152700|72.883175|     2014|\n",
      "|2014-01-07|       544.320015|       545.959999|537.919975|       540.040024| 79302300|72.361944|     2014|\n",
      "|2014-01-08|       538.809982|545.5599900000001| 538.68998|       543.460022| 64632400|72.820202|     2014|\n",
      "+----------+-----------------+-----------------+----------+-----------------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"SELECT * FROM delta.`{}` LIMIT 5;\".format(\n",
    "        \"F:\\\\development\\\\learn_spark\\\\output\\\\my_test_data_df_delta\"\n",
    "    )\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACID Transactions on Delta Tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|num_affected_rows|\n",
      "+-----------------+\n",
      "|                2|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"UPDATE delta.`{}` SET high = 12345 WHERE my_date = '2010-01-04';\".format(\n",
    "        \"F:\\\\development\\\\learn_spark\\\\output\\\\my_test_data_df_delta\"\n",
    "    )\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Evolution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = my_test_data_df.withColumn(\"new_column\", my_test_data_df[\"high\"] * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.write.option(\"overwriteSchema\", \"true\").format(\"delta\").mode(\"overwrite\").save(\n",
    "    \"./output/my_test_data_df_delta\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Travel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+----------+----------+-----------------+--------+---------+---------+\n",
      "|   my_date|             open|      high|       low|            close|  volume|adj_close|year_only|\n",
      "+----------+-----------------+----------+----------+-----------------+--------+---------+---------+\n",
      "|2014-01-02|       555.680008|557.029999|552.020004|        553.12999|58671200|74.115916|     2014|\n",
      "|2014-01-03|552.8600230000001|553.699989|540.429993|540.9800190000001|98116900|72.487897|     2014|\n",
      "+----------+-----------------+----------+----------+-----------------+--------+---------+---------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------+-----------------+----------+----------+-----------------+--------+---------+---------+\n",
      "|   my_date|             open|      high|       low|            close|  volume|adj_close|year_only|\n",
      "+----------+-----------------+----------+----------+-----------------+--------+---------+---------+\n",
      "|2014-01-02|       555.680008|557.029999|552.020004|        553.12999|58671200|74.115916|     2014|\n",
      "|2014-01-03|552.8600230000001|553.699989|540.429993|540.9800190000001|98116900|72.487897|     2014|\n",
      "+----------+-----------------+----------+----------+-----------------+--------+---------+---------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------+----------+----------+------------------+----------+---------+------------------+---------+\n",
      "|   my_date|      open|      high|               low|     close|   volume|         adj_close|year_only|\n",
      "+----------+----------+----------+------------------+----------+---------+------------------+---------+\n",
      "|2010-01-04|213.429998|   12345.0|212.38000099999996|214.009998|123432400|         27.727039|     2010|\n",
      "|2010-01-05|214.599998|215.589994|        213.249994|214.379993|150476200|27.774976000000002|     2010|\n",
      "+----------+----------+----------+------------------+----------+---------+------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query the Delta table as it appeared at a specific version\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\n",
    "    \"./output/my_test_data_df_delta\"\n",
    ").show(2)\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\n",
    "    \"./output/my_test_data_df_delta\"\n",
    ").show(2)\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", 2).load(\n",
    "    \"./output/my_test_data_df_delta\"\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all available versions of current delta table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------+-------+\n",
      "|file_path                                                                         |version|\n",
      "+----------------------------------------------------------------------------------+-------+\n",
      "|year_only=2010/part-00000-aa884108-5b5d-4d78-b316-6ac2fa5e5320.c000.snappy.parquet|0      |\n",
      "|year_only=2011/part-00000-7c7b6ecc-c11f-46ac-906a-0eb3fd13d4b7.c000.snappy.parquet|0      |\n",
      "|year_only=2012/part-00000-3ac43812-3fa8-4329-823e-23756512dd60.c000.snappy.parquet|0      |\n",
      "|year_only=2013/part-00000-9de2350f-b533-4af3-9a76-e9ce6f14e214.c000.snappy.parquet|0      |\n",
      "|year_only=2014/part-00000-8113355e-9aa1-4605-be6c-33c39675e5af.c000.snappy.parquet|0      |\n",
      "|year_only=2015/part-00000-3ab6ee04-ab19-4293-ac2d-d7444c1d3ef3.c000.snappy.parquet|0      |\n",
      "|year_only=2016/part-00000-53496ff8-c145-46b2-a2f6-fa4f8a899d95.c000.snappy.parquet|0      |\n",
      "|year_only=2010/part-00000-fcfaf48d-9c27-4635-86c9-a0fd99ad54f9.c000.snappy.parquet|1      |\n",
      "|year_only=2010/part-00000-b43faf2e-8e6b-4f89-9c32-cab6fecb332a.c000.snappy.parquet|2      |\n",
      "|year_only=2010/part-00001-a4dd9adc-4b5b-4557-9227-1248c4f44989.c000.snappy.parquet|2      |\n",
      "|part-00000-3a9ca227-4f14-46a7-915b-2cf2b1cf5dfb-c000.snappy.parquet               |3      |\n",
      "+----------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "details = spark.read.json(\"./output/my_test_data_df_delta/_delta_log/*.json\")\n",
    "details = details.select(col(\"add\")[\"path\"].alias(\"file_path\"))\n",
    "details = (\n",
    "    details.withColumn(\"version\", substring(input_file_name(), -6, 1))\n",
    "    .filter(col(\"file_path\").isNotNull() == True)\n",
    "    .orderBy(col(\"version\"), ascending=True)\n",
    "    .show(truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concurrency Control\n",
    "\n",
    "Delta Lake uses optimistic concurrency control to handle concurrent writes, preventing conflicts and ensuring consistency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata Management\n",
    "\n",
    "Delta tables store metadata, providing information about the table and its transactions, which enhances reliability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------------+----+-----------+-------------------------------------------------------------+-----------------------+-----------------------+----------------+--------+-----------+----------+----------------+----------------+------------------------+\n",
      "|format|id                                  |name|description|location                                                     |createdAt              |lastModified           |partitionColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion|tableFeatures           |\n",
      "+------+------------------------------------+----+-----------+-------------------------------------------------------------+-----------------------+-----------------------+----------------+--------+-----------+----------+----------------+----------------+------------------------+\n",
      "|delta |ec57ab81-42a0-4c69-bd73-98f75e3c95dc|NULL|NULL       |file:/F:/development/learn_spark/output/my_test_data_df_delta|2024-01-16 13:05:14.062|2024-01-16 13:05:19.811|[]              |1       |79274      |{}        |1               |2               |[appendOnly, invariants]|\n",
      "+------+------------------------------------+----+-----------+-------------------------------------------------------------+-----------------------+-----------------------+----------------+--------+-----------+----------+----------------+----------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"DESCRIBE DETAIL delta.`{}`;\".format(\n",
    "        \"F:\\\\development\\\\learn_spark\\\\output\\\\my_test_data_df_delta\"\n",
    "    )\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Delta Table Upsert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert into delta tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = spark.read.options(header=True, inferSchema=True).csv(\"./datasets/user.csv\")\n",
    "users_updated_df = spark.read.options(header=True, inferSchema=True).csv(\n",
    "    \"./datasets/user_updated.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.write.format(\"delta\").mode(\"overwrite\").save(\"./output/users_df\")\n",
    "users_updated_df.write.format(\"delta\").mode(\"overwrite\").save(\n",
    "    \"./output/users_updated_df\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Delta Tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_delta = DeltaTable.forPath(spark, \"./output/users_df\")\n",
    "users_updated_delta = DeltaTable.forPath(spark, \"./output/users_updated_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example SQL\n",
    "\n",
    "#   MERGE INTO people10m\n",
    "#   USING people10mupdates\n",
    "#   ON people10m.id = people10mupdates.id\n",
    "#   WHEN MATCHED THEN\n",
    "#     UPDATE SET\n",
    "#       id = people10mupdates.id,\n",
    "#       firstName = people10mupdates.firstName,\n",
    "#       middleName = people10mupdates.middleName,\n",
    "#       lastName = people10mupdates.lastName,\n",
    "#       gender = people10mupdates.gender,\n",
    "#       birthDate = people10mupdates.birthDate,\n",
    "#       ssn = people10mupdates.ssn,\n",
    "#       salary = people10mupdates.salary\n",
    "#   WHEN NOT MATCHED\n",
    "#     THEN INSERT (\n",
    "#       id,\n",
    "#       firstName,\n",
    "#       middleName,\n",
    "#       lastName,\n",
    "#       gender,\n",
    "#       birthDate,\n",
    "#       ssn,\n",
    "#       salary\n",
    "#     )\n",
    "#     VALUES (\n",
    "#       people10mupdates.id,\n",
    "#       people10mupdates.firstName,\n",
    "#       people10mupdates.middleName,\n",
    "#       people10mupdates.lastName,\n",
    "#       people10mupdates.gender,\n",
    "#       people10mupdates.birthDate,\n",
    "#       people10mupdates.ssn,\n",
    "#       people10mupdates.salary\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfUpdates = users_updated_delta.toDF()\n",
    "\n",
    "users_delta.alias(\"users\").merge(\n",
    "    dfUpdates.alias(\"users_updated\"), \"users.id = users_updated.id\"\n",
    ").whenMatchedUpdate(\n",
    "    set={\n",
    "        \"username\": \"users_updated.username\",\n",
    "        \"email\": \"users_updated.email\",\n",
    "        \"phone\": \"users_updated.phone\",\n",
    "    }\n",
    ").whenNotMatchedInsert(\n",
    "    values={\n",
    "        \"id\": \"users_updated.id\",\n",
    "        \"username\": \"users_updated.username\",\n",
    "        \"email\": \"users_updated.email\",\n",
    "        \"phone\": \"users_updated.phone\",\n",
    "    }\n",
    ").execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------------+-----+\n",
      "|id |username|email           |phone|\n",
      "+---+--------+----------------+-----+\n",
      "|1  |john    |john@gmail.com  |0    |\n",
      "|2  |jane    |jane02@gmail.com|12345|\n",
      "|3  |rick    |rick@gmail.com  |78678|\n",
      "|4  |mike    |mike@gmail.com  |9787 |\n",
      "+---+--------+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"SELECT * FROM delta.`{}`;\".format(\"F:\\\\development\\\\learn_spark\\\\output\\\\users_df\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------------+----+-----------+------------------------------------------------+----------------------+-----------------------+----------------+--------+-----------+----------+----------------+----------------+------------------------+\n",
      "|format|id                                  |name|description|location                                        |createdAt             |lastModified           |partitionColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion|tableFeatures           |\n",
      "+------+------------------------------------+----+-----------+------------------------------------------------+----------------------+-----------------------+----------------+--------+-----------+----------+----------------+----------------+------------------------+\n",
      "|delta |590cc3dd-a8ef-442b-a90e-4c8bf72f58bd|NULL|NULL       |file:/F:/development/learn_spark/output/users_df|2024-01-02 15:20:29.39|2024-01-16 13:05:29.309|[]              |1       |1277       |{}        |1               |2               |[appendOnly, invariants]|\n",
      "+------+------------------------------------+----+-----------+------------------------------------------------+----------------------+-----------------------+----------------+--------+-----------+----------+----------------+----------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"DESCRIBE DETAIL delta.`{}`;\".format(\n",
    "        \"F:\\\\development\\\\learn_spark\\\\output\\\\users_df\"\n",
    "    )\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of versions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation|operationParameters                                                                                                                                                            |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|11     |2024-01-16 13:05:29.309|NULL  |NULL    |MERGE    |{predicate -> [\"(id#21931 = id#21939)\"], matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|NULL|NULL    |NULL     |10         |Serializable  |false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1277, numTargetBytesRemoved -> 1245, numTargetRowsMatchedUpdated -> 3, executionTimeMs -> 1484, numTargetRowsInserted -> 1, numTargetRowsMatchedDeleted -> 0, scanTimeMs -> 1113, numTargetRowsUpdated -> 3, numOutputRows -> 4, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 4, numTargetFilesRemoved -> 1, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 366} |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|10     |2024-01-16 13:05:24.488|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}                                                                                                                                         |NULL|NULL    |NULL     |9          |Serializable  |false        |{numFiles -> 1, numOutputRows -> 3, numOutputBytes -> 1245}                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|9      |2024-01-15 14:00:44.922|NULL  |NULL    |MERGE    |{predicate -> [\"(id#15545 = id#15553)\"], matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|NULL|NULL    |NULL     |8          |Serializable  |false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1277, numTargetBytesRemoved -> 1245, numTargetRowsMatchedUpdated -> 3, executionTimeMs -> 900, numTargetRowsInserted -> 1, numTargetRowsMatchedDeleted -> 0, scanTimeMs -> 520, numTargetRowsUpdated -> 3, numOutputRows -> 4, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 4, numTargetFilesRemoved -> 1, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 375}   |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|8      |2024-01-15 14:00:41.488|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}                                                                                                                                         |NULL|NULL    |NULL     |7          |Serializable  |false        |{numFiles -> 1, numOutputRows -> 3, numOutputBytes -> 1245}                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|7      |2024-01-06 15:44:41.255|NULL  |NULL    |MERGE    |{predicate -> [\"(id#14684 = id#14692)\"], matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|NULL|NULL    |NULL     |6          |Serializable  |false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1277, numTargetBytesRemoved -> 1245, numTargetRowsMatchedUpdated -> 3, executionTimeMs -> 2842, numTargetRowsInserted -> 1, numTargetRowsMatchedDeleted -> 0, scanTimeMs -> 1456, numTargetRowsUpdated -> 3, numOutputRows -> 4, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 4, numTargetFilesRemoved -> 1, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 1378}|NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|6      |2024-01-06 15:44:31.743|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}                                                                                                                                         |NULL|NULL    |NULL     |5          |Serializable  |false        |{numFiles -> 1, numOutputRows -> 3, numOutputBytes -> 1245}                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|5      |2024-01-02 16:06:30.303|NULL  |NULL    |MERGE    |{predicate -> [\"(id#37378 = id#37386)\"], matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|NULL|NULL    |NULL     |4          |Serializable  |false        |{numTargetRowsCopied -> 1, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1355, numTargetBytesRemoved -> 1355, numTargetRowsMatchedUpdated -> 4, executionTimeMs -> 606, numTargetRowsInserted -> 0, numTargetRowsMatchedDeleted -> 0, scanTimeMs -> 350, numTargetRowsUpdated -> 4, numOutputRows -> 5, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 4, numTargetFilesRemoved -> 1, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 253}   |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|4      |2024-01-02 16:06:21.966|NULL  |NULL    |MERGE    |{predicate -> [\"(id#37378 = id#37386)\"], matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|NULL|NULL    |NULL     |3          |Serializable  |false        |{numTargetRowsCopied -> 1, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1355, numTargetBytesRemoved -> 1273, numTargetRowsMatchedUpdated -> 3, executionTimeMs -> 729, numTargetRowsInserted -> 1, numTargetRowsMatchedDeleted -> 0, scanTimeMs -> 426, numTargetRowsUpdated -> 3, numOutputRows -> 5, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 4, numTargetFilesRemoved -> 1, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 301}   |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|3      |2024-01-02 16:04:45.596|NULL  |NULL    |MERGE    |{predicate -> [\"(id#37378 = id#37386)\"], matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|NULL|NULL    |NULL     |2          |Serializable  |false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1273, numTargetBytesRemoved -> 1245, numTargetRowsMatchedUpdated -> 3, executionTimeMs -> 998, numTargetRowsInserted -> 1, numTargetRowsMatchedDeleted -> 0, scanTimeMs -> 569, numTargetRowsUpdated -> 3, numOutputRows -> 4, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 4, numTargetFilesRemoved -> 1, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 424}   |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|2      |2024-01-02 16:03:54.029|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}                                                                                                                                         |NULL|NULL    |NULL     |1          |Serializable  |false        |{numFiles -> 1, numOutputRows -> 3, numOutputBytes -> 1245}                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|1      |2024-01-02 16:01:05.143|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}                                                                                                                                         |NULL|NULL    |NULL     |0          |Serializable  |false        |{numFiles -> 1, numOutputRows -> 3, numOutputBytes -> 1245}                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|0      |2024-01-02 15:20:29.552|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}                                                                                                                                         |NULL|NULL    |NULL     |NULL       |Serializable  |false        |{numFiles -> 1, numOutputRows -> 3, numOutputBytes -> 1277}                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "+-------+-----------------------+------+--------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"DESCRIBE HISTORY delta.`{}`;\".format(\n",
    "        \"F:\\\\development\\\\learn_spark\\\\output\\\\users_df\"\n",
    "    )\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|version|\n",
      "+-------+\n",
      "|     11|\n",
      "|     10|\n",
      "|      9|\n",
      "|      8|\n",
      "|      7|\n",
      "|      6|\n",
      "|      5|\n",
      "|      4|\n",
      "|      3|\n",
      "|      2|\n",
      "|      1|\n",
      "|      0|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_delta.history().select(\"version\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Z Order and OPTIMIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+---------+-----------------+\n",
      "|my_date   |open              |high              |low               |close             |volume   |adj_close         |year_only|new_column       |\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+---------+-----------------+\n",
      "|2010-01-04|213.429998        |214.499996        |212.38000099999996|214.009998        |123432400|27.727039         |2010     |643.499988       |\n",
      "|2010-01-05|214.599998        |215.589994        |213.249994        |214.379993        |150476200|27.774976000000002|2010     |646.769982       |\n",
      "|2010-01-06|214.379993        |215.23            |210.750004        |210.969995        |138040000|27.333178000000004|2010     |645.6899999999999|\n",
      "|2010-01-07|211.75            |212.000006        |209.050005        |210.58            |119282800|27.28265          |2010     |636.0000180000001|\n",
      "|2010-01-08|210.299994        |212.000006        |209.06000500000002|211.98000499999998|111902700|27.464034         |2010     |636.0000180000001|\n",
      "|2010-01-11|212.79999700000002|213.000002        |208.450005        |210.11000299999998|115557400|27.221758         |2010     |639.000006       |\n",
      "|2010-01-12|209.18999499999998|209.76999500000002|206.419998        |207.720001        |148614900|26.91211          |2010     |629.3099850000001|\n",
      "|2010-01-13|207.870005        |210.92999500000002|204.099998        |210.650002        |151473000|27.29172          |2010     |632.7899850000001|\n",
      "|2010-01-14|210.11000299999998|210.45999700000002|209.020004        |209.43            |108223500|27.133657         |2010     |631.379991       |\n",
      "|2010-01-15|210.92999500000002|211.59999700000003|205.869999        |205.93            |148516900|26.680197999999997|2010     |634.7999910000001|\n",
      "|2010-01-19|208.330002        |215.18999900000003|207.240004        |215.039995        |182501900|27.860484999999997|2010     |645.5699970000001|\n",
      "|2010-01-20|214.910006        |215.549994        |209.500002        |211.73            |153038200|27.431644         |2010     |646.649982       |\n",
      "|2010-01-21|212.079994        |213.30999599999998|207.210003        |208.069996        |152038600|26.957455         |2010     |639.929988       |\n",
      "|2010-01-22|206.78000600000001|207.499996        |197.16            |197.75            |220441900|25.620401         |2010     |622.499988       |\n",
      "|2010-01-25|202.51000200000001|204.699999        |200.190002        |203.070002        |266424900|26.309658000000002|2010     |614.099997       |\n",
      "|2010-01-26|205.95000100000001|213.710005        |202.580004        |205.940001        |466777500|26.681494         |2010     |641.130015       |\n",
      "|2010-01-27|206.849995        |210.58            |199.530001        |207.880005        |430642100|26.932840000000002|2010     |631.74           |\n",
      "|2010-01-28|204.930004        |205.500004        |198.699995        |199.289995        |293375600|25.819922000000002|2010     |616.500012       |\n",
      "|2010-01-29|201.079996        |202.199995        |190.250002        |192.060003        |311488100|24.883208         |2010     |606.5999850000001|\n",
      "|2010-02-01|192.36999699999998|196.0             |191.29999899999999|194.729998        |187469100|25.229131         |2010     |588.0            |\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+---------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"SELECT * FROM delta.`{}`;\".format(\n",
    "        \"F:\\\\development\\\\learn_spark\\\\output\\\\my_test_data_df_delta\"\n",
    "    )\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z Ordering\n",
    "\n",
    "Z-Ordering in PySpark is a technique to colocate related information in the same set of files, automatically used by Delta Lake in data-skipping algorithms. This co-locality reduces the amount of data that Delta Lake on Apache Spark needs to read, thus improving query performance. To Z-Order data, you specify the columns to order on in the ZORDER BY clause using the OPTIMIZE command. For example, OPTIMIZE events ZORDER BY (eventType). This feature is available in Delta Lake 2.0.0 and above 3.\n",
    "\n",
    "### OPTIMIZE\n",
    "\n",
    "Delta Lake supports the `OPTIMIZE` operation, which performs file compaction.\n",
    "\n",
    "<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Small files are compacted together into new larger files up to 1GB.\n",
    "Thus, at this point the number of files increases!\n",
    "\n",
    "The 1GB size was determined by the Databricks optimization team as a trade-off between query speed and run-time performance when running Optimize.\n",
    "\n",
    "`OPTIMIZE` is not run automatically because you must collect many small files first.\n",
    "\n",
    "- Run `OPTIMIZE` more often if you want better end-user query performance\n",
    "- Since `OPTIMIZE` is a time consuming step, run it less often if you want to optimize cost of compute hours\n",
    "- To start with, run `OPTIMIZE` on a daily basis (preferably at night when spot prices are low), and determine the right frequency for your particular business case\n",
    "- In the end, the frequency at which you run `OPTIMIZE` is a business decision\n",
    "\n",
    "The easiest way to see what `OPTIMIZE` does is to perform a simple `count(*)` query before and after and compare the timing!\n",
    "\n",
    "## Data Skipping\n",
    "\n",
    "Data skipping is a feature of Delta Lake that uses Z-Ordering to colocate related information in the same set of files. This technique reduces the amount of data that needs to be read by Delta Lake on Apache Spark, thus improving query performance. Data skipping information is collected automatically when you write data into a Delta table, and Delta Lake takes advantage of this information at query time to provide faster queries. You must have statistics collected for columns that are used in ZORDER statements. Z-Ordering is not idempotent but aims to be an incremental operation. The time it takes for Z-Ordering is not guaranteed to reduce over multiple runs. To Z-Order data, you specify the columns to order on in the ZORDER BY clause using the OPTIMIZE command. Data skipping is an automatic feature of the OPTIMIZE command and works well when combined with Z-Ordering.\n",
    "\n",
    "### Data Skipping and ZORDER\n",
    "\n",
    "Delta Lake uses two mechanisms to speed up queries.\n",
    "\n",
    "<b>Data Skipping</b> is a performance optimization that aims at speeding up queries that contain filters (WHERE clauses).\n",
    "\n",
    "For example, we have a data set that is partitioned by `date`.\n",
    "\n",
    "A query using `WHERE date > 2016-07-26` would not access data that resides in partitions that correspond to dates prior to `2016-07-26`.\n",
    "\n",
    "<b>ZOrdering</b> is a technique to colocate related information in the same set of files.\n",
    "\n",
    "ZOrdering maps multidimensional data to one dimension while preserving locality of the data points.\n",
    "\n",
    "Given a column that you want to perform ZORDER on, say `OrderColumn`, Delta\n",
    "\n",
    "- takes existing parquet files within a partition\n",
    "- maps the rows within the parquet files according to `OrderColumn` using the algorithm described <a href=\"https://en.wikipedia.org/wiki/Z-order_curve\" target=\"_blank\">here</a>\n",
    "- (in the case of only one column, the mapping above becomes a linear sort)\n",
    "- rewrites the sorted data into new parquet files\n",
    "\n",
    "<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> You cannot use the partition column also as a ZORDER column.\n",
    "\n",
    "#### ZORDER Technical Overview\n",
    "\n",
    "A brief example of how this algorithm works (refer to [this blog](https://databricks.com/blog/2018/07/31/processing-petabytes-of-data-in-seconds-with-databricks-delta.html) for more details):\n",
    "\n",
    "![](https://files.training.databricks.com/images/adbcore/zorder.png)\n",
    "\n",
    "Legend:\n",
    "\n",
    "- Gray dot = data point e.g., chessboard square coordinates\n",
    "- Gray box = data file; in this example, we aim for files of 4 points each\n",
    "- Yellow box = data file that’s read for the given query\n",
    "- Green dot = data point that passes the query’s filter and answers the query\n",
    "- Red dot = data point that’s read, but doesn’t satisfy the filter; “false positive”\n",
    "\n",
    "#### ZORDER example\n",
    "\n",
    "In the image below, table `Students` has 4 columns:\n",
    "\n",
    "- `gender` with 2 distinct values\n",
    "- `Pass-Fail` with 2 distinct values\n",
    "- `Class` with 4 distinct values\n",
    "- `Student` with many distinct values\n",
    "\n",
    "Suppose you wish to perform the following query:\n",
    "\n",
    "`SELECT Name FROM Students WHERE gender = 'M' AND Pass_Fail = 'P' AND Class = 'Junior'`\n",
    "\n",
    "`ORDER BY Gender, Pass_Fail`\n",
    "\n",
    "The most effective way of performing that search is to order the data starting with the largest set, which is `Gender` in this case.\n",
    "\n",
    "If you're searching for `gender = 'M'`, then you don't even have to look at students with `gender = 'F'`.\n",
    "\n",
    "Note that this technique only works if all `gender = 'M'` values are co-located.\n",
    "\n",
    "<div><img src=\"https://files.training.databricks.com/images/eLearning/Delta/zorder.png\" style=\"height: 300px\"/></div><br/>\n",
    "\n",
    "Here's the comparison between Z-Order and Optimize in a tabular format, generated in Markdown text:\n",
    "\n",
    "|           Feature |                                                                                                                             Z-Order |                                                                                                                                             Optimize |\n",
    "| ----------------: | ----------------------------------------------------------------------------------------------------------------------------------: | ---------------------------------------------------------------------------------------------------------------------------------------------------: |\n",
    "|        Definition | Z-Ordering is a technique to colocate related information in the same set of files, used by Delta Lake in data-skipping algorithms. | OPTIMIZE is a command used to improve the performance of Delta Lake tables, which can be used to perform Z-Ordering and Data Skipping optimizations. |\n",
    "|           Purpose |                                                                     Improves query performance by reducing the amount of data read. |                                              Improves the performance of Delta Lake tables by performing Z-Ordering and Data Skipping optimizations. |\n",
    "|        Z-Ordering |                                                                       Automatically used by Delta Lake in data-skipping algorithms. |                                                                                              Not idempotent but aims to be an incremental operation. |\n",
    "|     Data Skipping |                                                Automatic feature of the OPTIMIZE command, works well when combined with Z-Ordering. |                                                                           Can be run during normal business hours and does not require any downtime. |\n",
    "| Z-Order BY Clause |                                                 Specify the columns to order on in the ZORDER BY clause using the OPTIMIZE command. |                                                                        Can be applied incrementally to partitions and queries after the initial run. |\n",
    "|        Trade-offs |                      Z-Ordering is not idempotent, and its effectiveness drops with each additional column in the ZORDER BY clause. |                            The trade-offs between Z-Ordering and Hive-style partitioning are complex and should be analyzed based on query patterns. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------+------+--------+---------+---------+----------+\n",
      "|   my_date|      open|      high|   low| close|  volume|adj_close|year_only|new_column|\n",
      "+----------+----------+----------+------+------+--------+---------+---------+----------+\n",
      "|2016-12-30|116.650002|117.199997|115.43|115.82|30586300|115.32002|     2016|351.599991|\n",
      "+----------+----------+----------+------+------+--------+---------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"select * from delta.`F:\\\\development\\\\learn_spark\\\\output\\\\my_test_data_df_delta` where my_date = '2016-12-30' and volume > 30586250;\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|path                                                         |metrics                                                                                                                                                                                            |\n",
      "+-------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|file:/F:/development/learn_spark/output/my_test_data_df_delta|{1, 1, {79274, 79274, 79274.0, 1, 79274}, {79274, 79274, 79274.0, 1, 79274}, 1, {all, {0, 0}, {1, 79274}, 0, {1, 79274}, 1, NULL}, 1, 1, 0, false, 0, 0, 1705390533426, 0, 20, 0, NULL, NULL, 9, 9}|\n",
      "+-------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testdt = DeltaTable.forPath(spark, \"F:\\\\development\\\\learn_spark\\\\output\\\\my_test_data_df_delta\")\n",
    "# testdt.optimize().executeZOrderBy(\"my_date\")\n",
    "\n",
    "spark.sql(\n",
    "    \"OPTIMIZE delta.`F:\\\\development\\\\learn_spark\\\\output\\\\my_test_data_df_delta` ZORDER by (my_date);\"\n",
    ").show(truncate=False)\n",
    "\n",
    "# # OPTIMIZE command with Z-Ordering and Data Skipping\n",
    "# testdt.optimize(\"my_date\").execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------+------+--------+---------+---------+----------+\n",
      "|   my_date|      open|      high|   low| close|  volume|adj_close|year_only|new_column|\n",
      "+----------+----------+----------+------+------+--------+---------+---------+----------+\n",
      "|2016-12-30|116.650002|117.199997|115.43|115.82|30586300|115.32002|     2016|351.599991|\n",
      "+----------+----------+----------+------+------+--------+---------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"select * from delta.`F:\\\\development\\\\learn_spark\\\\output\\\\my_test_data_df_delta` where my_date = '2016-12-30' and volume > 30586250;\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------+-------------------------------------------------+----+--------+---------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation|operationParameters                              |job |notebook|clusterId|readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                               |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------+-------------------------------------------------+----+--------+---------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|4      |2024-01-16 13:05:32.358|NULL  |NULL    |OPTIMIZE |{predicate -> [], zOrderBy -> [\"my_date\"]}       |NULL|NULL    |NULL     |3          |SnapshotIsolation|false        |{numRemovedFiles -> 1, numRemovedBytes -> 79274, p25FileSize -> 79274, numDeletionVectorsRemoved -> 0, minFileSize -> 79274, numAddedFiles -> 1, maxFileSize -> 79274, p75FileSize -> 79274, p50FileSize -> 79274, numAddedBytes -> 79274}                                                                                     |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|3      |2024-01-16 13:05:19.811|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}           |NULL|NULL    |NULL     |2          |Serializable     |false        |{numFiles -> 1, numOutputRows -> 1762, numOutputBytes -> 79274}                                                                                                                                                                                                                                                                |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|2      |2024-01-16 13:05:18.165|NULL  |NULL    |UPDATE   |{predicate -> [\"(my_date#11709 = 2010-01-04)\"]}  |NULL|NULL    |NULL     |1          |Serializable     |false        |{numRemovedFiles -> 2, numRemovedBytes -> 15793, numCopiedRows -> 260, numDeletionVectorsAdded -> 0, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 291, numDeletionVectorsUpdated -> 0, scanTimeMs -> 204, numAddedFiles -> 2, numUpdatedRows -> 2, numAddedBytes -> 15809, rewriteTimeMs -> 86}|NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|1      |2024-01-16 13:05:15.842|NULL  |NULL    |WRITE    |{mode -> Append, partitionBy -> []}              |NULL|NULL    |NULL     |0          |Serializable     |false        |{numFiles -> 1, numOutputRows -> 10, numOutputBytes -> 2434}                                                                                                                                                                                                                                                                   |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|0      |2024-01-16 13:05:14.355|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> [\"year_only\"]}|NULL|NULL    |NULL     |NULL       |Serializable     |false        |{numFiles -> 7, numOutputRows -> 1762, numOutputBytes -> 87511}                                                                                                                                                                                                                                                                |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "+-------+-----------------------+------+--------+---------+-------------------------------------------------+----+--------+---------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"describe history delta.`F:\\\\development\\\\learn_spark\\\\output\\\\my_test_data_df_delta`;\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding VACCUM\n",
    "\n",
    "Vacuum in Delta tables is a command that removes all files from directories not managed by Delta Lake, ignoring directories beginning with an underscore `_`. It is also used to remove data files that are no longer in the latest state of the table[1][2]. Here are some key points about Vacuum in Delta tables:\n",
    "\n",
    "- Vacuum is not triggered automatically, and you need to run it manually to remove unused data files[4].\n",
    "- The default retention threshold for Vacuum is 7 days[2][5].\n",
    "- Running Vacuum regularly is important for cost and compliance, as it helps reduce cloud storage costs and ensures data privacy[3][5].\n",
    "- Delta Lake has a safety check to prevent you from running a dangerous Vacuum command. If you are certain that there are no operations being performed on the table that take longer than you can turn off this safety check, you can set the Spark configuration property `spark.databricks.delta.retentionDurationCheck.enabled` to false[5].\n",
    "- Vacuum commits to the Delta transaction log contain audit information, and you can query the audit events using `DESCRIBE HISTORY`[5].\n",
    "\n",
    "To use Vacuum in PySpark, you can use the `vacuum()` method on a Delta table, specifying the retention threshold in hours if needed[1][2]. For example:\n",
    "\n",
    "```python\n",
    "from delta.tables import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, pathToTable)\n",
    "deltaTable.vacuum()  # vacuum files not required by versions older than the default retention period\n",
    "deltaTable.vacuum(100)  # vacuum files not required by versions more than 100 hours old\n",
    "```\n",
    "\n",
    "Remember to avoid updating or appending data files during the Vacuum process, as it can lead to data loss or corruption[4].\n",
    "\n",
    "> Vacuuming in the context of Delta tables is not a reversible operation. Once data files are vacuumed and deleted, they cannot be restored to the table.\n",
    "\n",
    "Citations:\n",
    "\n",
    "1. https://docs.delta.io/latest/delta-utility.html\n",
    "2. https://docs.databricks.com/en/sql/language-manual/delta-vacuum.html\n",
    "3. https://docs.databricks.com/en/delta/vacuum.html\n",
    "4. https://docs.delta.io/0.4.0/delta-utility.html\n",
    "5. https://learn.microsoft.com/en-us/azure/databricks/delta/vacuum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testdt = DeltaTable.forPath(spark, \"F:\\\\development\\\\learn_spark\\\\output\\\\my_test_data_df_delta\")\n",
    "# testdt.vacuum(0)\n",
    "\n",
    "# ^^ Gives error IllegalArgumentException: requirement failed: Are you sure you would like to vacuum files with such a low retention period? If you have\n",
    "# writers that are currently writing to this table, there is a risk that you may corrupt the\n",
    "# state of your Delta table.\n",
    "\n",
    "# If you are certain that there are no operations being performed on this table, such as\n",
    "# insert/upsert/delete/optimize, then you may turn off this check by setting:\n",
    "# spark.databricks.delta.retentionDurationCheck.enabled = false\n",
    "\n",
    "# If you are not sure, please use a value not less than \"168 hours\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before Vaccum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------+-------------------------------------------------+----+--------+---------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation|operationParameters                              |job |notebook|clusterId|readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                               |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------+-------------------------------------------------+----+--------+---------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|4      |2024-01-16 13:05:32.358|NULL  |NULL    |OPTIMIZE |{predicate -> [], zOrderBy -> [\"my_date\"]}       |NULL|NULL    |NULL     |3          |SnapshotIsolation|false        |{numRemovedFiles -> 1, numRemovedBytes -> 79274, p25FileSize -> 79274, numDeletionVectorsRemoved -> 0, minFileSize -> 79274, numAddedFiles -> 1, maxFileSize -> 79274, p75FileSize -> 79274, p50FileSize -> 79274, numAddedBytes -> 79274}                                                                                     |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|3      |2024-01-16 13:05:19.811|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}           |NULL|NULL    |NULL     |2          |Serializable     |false        |{numFiles -> 1, numOutputRows -> 1762, numOutputBytes -> 79274}                                                                                                                                                                                                                                                                |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|2      |2024-01-16 13:05:18.165|NULL  |NULL    |UPDATE   |{predicate -> [\"(my_date#11709 = 2010-01-04)\"]}  |NULL|NULL    |NULL     |1          |Serializable     |false        |{numRemovedFiles -> 2, numRemovedBytes -> 15793, numCopiedRows -> 260, numDeletionVectorsAdded -> 0, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 291, numDeletionVectorsUpdated -> 0, scanTimeMs -> 204, numAddedFiles -> 2, numUpdatedRows -> 2, numAddedBytes -> 15809, rewriteTimeMs -> 86}|NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|1      |2024-01-16 13:05:15.842|NULL  |NULL    |WRITE    |{mode -> Append, partitionBy -> []}              |NULL|NULL    |NULL     |0          |Serializable     |false        |{numFiles -> 1, numOutputRows -> 10, numOutputBytes -> 2434}                                                                                                                                                                                                                                                                   |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|0      |2024-01-16 13:05:14.355|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> [\"year_only\"]}|NULL|NULL    |NULL     |NULL       |Serializable     |false        |{numFiles -> 7, numOutputRows -> 1762, numOutputBytes -> 87511}                                                                                                                                                                                                                                                                |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "+-------+-----------------------+------+--------+---------+-------------------------------------------------+----+--------+---------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"describe history delta.`F:\\\\development\\\\learn_spark\\\\output\\\\my_test_data_df_delta`;\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\n",
    "#     \"VACUUM delta.`F:\\\\development\\\\learn_spark\\\\output\\\\my_test_data_df_delta` RETAIN 0 HOURS;\"\n",
    "# ).show(truncate=False)\n",
    "\n",
    "# Gives error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bypass\n",
    "\n",
    "# spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------+\n",
      "|path                                                         |\n",
      "+-------------------------------------------------------------+\n",
      "|file:/F:/development/learn_spark/output/my_test_data_df_delta|\n",
      "+-------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Safe\n",
    "\n",
    "spark.sql(\n",
    "    \"VACUUM delta.`F:\\\\development\\\\learn_spark\\\\output\\\\my_test_data_df_delta` RETAIN 169 HOURS;\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+------------+-----------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation   |operationParameters                                                                                        |job |notebook|clusterId|readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                               |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+------------+-----------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|6      |2024-01-16 13:06:40.207|NULL  |NULL    |VACUUM END  |{status -> COMPLETED}                                                                                      |NULL|NULL    |NULL     |5          |SnapshotIsolation|true         |{numDeletedFiles -> 0, numVacuumedDirectories -> 8}                                                                                                                                                                                                                                                                            |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|5      |2024-01-16 13:06:38.776|NULL  |NULL    |VACUUM START|{retentionCheckEnabled -> true, defaultRetentionMillis -> 604800000, specifiedRetentionMillis -> 608400000}|NULL|NULL    |NULL     |4          |SnapshotIsolation|true         |{numFilesToDelete -> 0, sizeOfDataToDelete -> 0}                                                                                                                                                                                                                                                                               |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|4      |2024-01-16 13:05:32.358|NULL  |NULL    |OPTIMIZE    |{predicate -> [], zOrderBy -> [\"my_date\"]}                                                                 |NULL|NULL    |NULL     |3          |SnapshotIsolation|false        |{numRemovedFiles -> 1, numRemovedBytes -> 79274, p25FileSize -> 79274, numDeletionVectorsRemoved -> 0, minFileSize -> 79274, numAddedFiles -> 1, maxFileSize -> 79274, p75FileSize -> 79274, p50FileSize -> 79274, numAddedBytes -> 79274}                                                                                     |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|3      |2024-01-16 13:05:19.811|NULL  |NULL    |WRITE       |{mode -> Overwrite, partitionBy -> []}                                                                     |NULL|NULL    |NULL     |2          |Serializable     |false        |{numFiles -> 1, numOutputRows -> 1762, numOutputBytes -> 79274}                                                                                                                                                                                                                                                                |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|2      |2024-01-16 13:05:18.165|NULL  |NULL    |UPDATE      |{predicate -> [\"(my_date#11709 = 2010-01-04)\"]}                                                            |NULL|NULL    |NULL     |1          |Serializable     |false        |{numRemovedFiles -> 2, numRemovedBytes -> 15793, numCopiedRows -> 260, numDeletionVectorsAdded -> 0, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 291, numDeletionVectorsUpdated -> 0, scanTimeMs -> 204, numAddedFiles -> 2, numUpdatedRows -> 2, numAddedBytes -> 15809, rewriteTimeMs -> 86}|NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|1      |2024-01-16 13:05:15.842|NULL  |NULL    |WRITE       |{mode -> Append, partitionBy -> []}                                                                        |NULL|NULL    |NULL     |0          |Serializable     |false        |{numFiles -> 1, numOutputRows -> 10, numOutputBytes -> 2434}                                                                                                                                                                                                                                                                   |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|0      |2024-01-16 13:05:14.355|NULL  |NULL    |WRITE       |{mode -> Overwrite, partitionBy -> [\"year_only\"]}                                                          |NULL|NULL    |NULL     |NULL       |Serializable     |false        |{numFiles -> 7, numOutputRows -> 1762, numOutputBytes -> 87511}                                                                                                                                                                                                                                                                |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "+-------+-----------------------+------+--------+------------+-----------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"describe history delta.`F:\\\\development\\\\learn_spark\\\\output\\\\my_test_data_df_delta`;\"\n",
    ").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
