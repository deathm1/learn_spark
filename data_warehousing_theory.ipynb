{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Lake and Delta Lake\n",
    "\n",
    "- Data Lake: A data lake is a centralized repository that allows you to store all your structured and unstructured data at any scale. It can store data in its raw, unprocessed form and supports a variety of data types, including relational data, log files, images, and more.\n",
    "\n",
    "- Delta Lake: Delta Lake is an open-source storage layer that brings ACID (Atomicity, Consistency, Isolation, Durability) transactions to Apache Spark and big data workloads. It is designed to bring reliability to data lakes by providing features like ACID transactions, schema enforcement, and time travel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference between Data Lake and Delta Lake\n",
    "\n",
    "| Parameters                              | Delta Lake                                                                                                                          | Data Lake                                                                                                                                                                                                                                                                |\n",
    "| --------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| Data Consistency and ACID Transactions  | Traditional data lakes often struggle with data consistency, as they lack built-in transactional support.                           | In contrast, Delta Lake provides ACID transactions, ensuring that data changes are either fully applied or fully rolled back, maintaining the integrity of the data.                                                                                                     |\n",
    "| Schema Evolution and Evolution Tracking | In traditional data lakes, schema evolution can be challenging and often requires complex ETL processes.                            | Delta Lake simplifies schema evolution by allowing you to add, modify, or delete columns in a table without disrupting data pipelines.                                                                                                                                   |\n",
    "| Performance and Optimization            | Traditional data lakes may suffer from performance issues as data volumes grow, primarily due to the lack of optimization features. | Delta Lake addresses this challenge by implementing optimization techniques like data compaction and indexing. These optimizations significantly improve query performance, making Delta Lake a compelling choice for organizations with demanding analytical workloads. |\n",
    "| Data Lake Storage Costs                 | The cost of storing data in a data lake can be substantial, especially when dealing with large-scale datasets.                      | Delta Lake adopts a cost-effective approach by using file formats that reduce storage costs and improve compression.                                                                                                                                                     |\n",
    "| Data Quality and Data Governance        | Traditional data lakes may lack robust mechanisms for data quality checks and governance.                                           | Delta Lake incorporates features for data validation and governance, helping organizations maintain data quality and meet regulatory requirements effectively.                                                                                                           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of Data Lake\n",
    "\n",
    "ADLS typically refers to Azure Data Lake Storage, which is a cloud-based storage service provided by Microsoft Azure. Azure Data Lake Storage is designed to enable big data analytics and is integrated with various Azure services, making it a key component in Azure's data ecosystem.\n",
    "\n",
    "| Feature                    | Azure Data Lake Storage Gen1 (ADLS Gen1) | Azure Data Lake Storage Gen2 (ADLS Gen2)                                   |\n",
    "| -------------------------- | ---------------------------------------- | -------------------------------------------------------------------------- |\n",
    "| **Hierarchical Namespace** | Flat namespace; no directory structure   | Hierarchical namespace for efficient data organization                     |\n",
    "| **Security**               | - Authentication: Shared key, Azure AD   | - Authentication: Shared key, Azure AD, Azure AD Bearer Token              |\n",
    "| **Security**               | - Authorization: POSIX-style ACLs        | - Authorization: POSIX-style ACLs, Azure Blob Storage-style RBAC           |\n",
    "| **Performance**            | - Good read and write performance        | - Improved metadata operations, enhanced parallelism                       |\n",
    "| **Integration**            | - Independent service                    | - Built on Azure Blob Storage platform, compatible with Azure Blob Storage |\n",
    "| **Storage Tiers**          | - N/A                                    | - Supports hot and cold storage tiers                                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Lake vs Data Warehouse vs Data Lakehouse\n",
    "\n",
    "| Feature                         | Data Lake                                                                                         | Data Warehouse                                                         | Data Lakehouse                                                                                      |\n",
    "| ------------------------------- | ------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------- |\n",
    "| **Data Storage**                | Stores raw and unstructured data in its native format.                                            | Typically stores structured and processed data in tabular format.      | Stores both raw, unstructured data and structured, processed data.                                  |\n",
    "| **Schema**                      | Schema-on-read; supports schema flexibility.                                                      | Schema-on-write; enforces a predefined schema.                         | Supports both schema-on-read and schema-on-write.                                                   |\n",
    "| **Data Processing**             | Suited for big data processing and analytics using distributed computing frameworks.              | Optimized for complex SQL queries and analytics.                       | Combines big data processing capabilities with SQL analytics.                                       |\n",
    "| **Query Performance**           | May have slower query performance due to schema-on-read and raw data storage.                     | Offers fast query performance for structured data.                     | Combines the advantages of both Data Lake and Data Warehouse for balanced performance.              |\n",
    "| **Use Cases**                   | Ideal for storing large volumes of raw data for diverse analytics and machine learning use cases. | Best for structured, business-critical analytics and reporting.        | Suitable for both big data analytics and structured, ad-hoc queries.                                |\n",
    "| **Integration**                 | Integrates well with big data processing frameworks like Apache Spark, Hadoop.                    | Integrates with business intelligence tools and SQL-based analytics.   | Integrates with both big data processing frameworks and SQL analytics tools.                        |\n",
    "| **Cost Considerations**         | Generally more cost-effective for storing large volumes of raw data.                              | May have higher storage costs but optimized for query performance.     | Aims for a balance between cost-effective storage and optimized query performance.                  |\n",
    "| **Data Quality and Governance** | May lack built-in governance features; data quality checks may be challenging.                    | Typically includes features for data governance and quality assurance. | Incorporates data governance features, addressing concerns of both Data Lake and Data Warehouse.    |\n",
    "| **Transaction Support**         | Lacks built-in support for transactions.                                                          | Supports transactions for maintaining data consistency.                | Often includes transaction support, providing a middle ground between Data Lake and Data Warehouse. |\n",
    "| **Examples**                    | Azure Data Lake Storage, Amazon S3.                                                               | Amazon Redshift, Google BigQuery.                                      | Delta Lake, Databricks Delta, Snowflake.                                                            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference Between Schema on Read vs Schema on Write\n",
    "\n",
    "| Feature                 | Schema on Read                                                                                           | Schema on Write                                                                                                  |\n",
    "| ----------------------- | -------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------- |\n",
    "| **Definition**          | Defines the schema when the data is read.                                                                | Requires defining the schema before writing data.                                                                |\n",
    "| **Flexibility**         | Offers flexibility to read different schemas from the same data.                                         | Less flexible as the schema is enforced during the write operation.                                              |\n",
    "| **Data Storage**        | Raw data is stored without a predefined structure.                                                       | Data is stored in a structured format with a predefined schema.                                                  |\n",
    "| **Processing Overhead** | Minimal processing overhead during data ingestion.                                                       | Higher processing overhead during data ingestion to enforce schema.                                              |\n",
    "| **Query Performance**   | May experience slower query performance since the schema is interpreted during read operations.          | Typically provides faster query performance as the schema is predefined and optimized for queries.               |\n",
    "| **Use Cases**           | Suited for scenarios where data formats may evolve, and flexibility in data interpretation is essential. | Ideal for scenarios where data consistency and query performance are critical, such as in business intelligence. |\n",
    "| **Data Evolution**      | Adapts well to changes in data structures over time.                                                     | May require additional steps to handle changes in data structures, potentially involving ETL processes.          |\n",
    "| **Data Quality**        | Offers less control over data quality during ingestion.                                                  | Provides better control over data quality by enforcing a predefined schema.                                      |\n",
    "| **Examples**            | Apache Parquet, JSON, Avro.                                                                              | Relational databases, SQL-based storage systems.                                                                 |\n",
    "| **Complexity**          | Generally simpler in terms of schema management.                                                         | Can be more complex due to schema enforcement and management.                                                    |\n",
    "| **Scalability**         | Suited for scalable storage of diverse data formats.                                                     | May face challenges with diverse data formats and evolving schemas at scale.                                     |\n",
    "| **Best Suited For**     | Data lakes with diverse, raw, and evolving data.                                                         | Data warehouses with structured, business-critical data.                                                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delta Tables Hands on\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, substring, input_file_name, current_date, year\n",
    "\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    IntegerType,\n",
    "    LongType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    DateType,\n",
    "    DoubleType,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a spark session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = (\n",
    "    SparkSession.builder.appName(\"learn_delta_lake\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Cluster Information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Get the SparkContext\n",
    "sc = spark._jsc.sc()\n",
    "n_workers = (\n",
    "    len([executor.host() for executor in sc.statusTracker().getExecutorInfos()]) - 1\n",
    ")\n",
    "\n",
    "print(n_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_custom_schema = StructType(\n",
    "    [\n",
    "        StructField(\"my_date\", DateType(), nullable=True),\n",
    "        StructField(\"open\", DoubleType(), nullable=True),\n",
    "        StructField(\"high\", DoubleType(), nullable=True),\n",
    "        StructField(\"low\", DoubleType(), nullable=True),\n",
    "        StructField(\"close\", DoubleType(), nullable=True),\n",
    "        StructField(\"volume\", IntegerType(), nullable=True),\n",
    "        StructField(\"adj_close\", DoubleType(), nullable=True),\n",
    "    ]\n",
    ")\n",
    "my_test_data_df = (\n",
    "    spark.read.schema(my_custom_schema)\n",
    "    .options(header=True)\n",
    "    .csv(\"./datasets/appl_stock.csv\")\n",
    ")\n",
    "\n",
    "\n",
    "my_test_data_df = my_test_data_df.withColumn(\"year_only\", year(col(\"my_date\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write DataFrame as a Delta table\n",
    "\n",
    "| Feature                       | Delta Table                                      | Parquet Format                                          |\n",
    "| ----------------------------- | ------------------------------------------------ | ------------------------------------------------------- |\n",
    "| **ACID Transactions**         | Supports ACID transactions for data integrity.   | Does not support ACID transactions natively.            |\n",
    "| **Schema Evolution**          | Supports schema evolution for table evolution.   | Schema evolution is possible but limited.               |\n",
    "| **Time Travel**               | Supports time travel for querying past versions. | No built-in support for time travel.                    |\n",
    "| **Concurrency Control**       | Optimistic concurrency control for data writes.  | No built-in concurrency control for writes.             |\n",
    "| **Metadata Management**       | Maintains metadata for improved reliability.     | Limited metadata management compared to Delta.          |\n",
    "| **Data Storage Optimization** | Provides features like data compaction.          | Stores data efficiently but lacks Delta features.       |\n",
    "| **Compatibility with Spark**  | Designed for seamless integration with Spark.    | Can be used with Spark but lacks Delta features.        |\n",
    "| **Performance Optimization**  | Optimized for high-performance read and write.   | Efficient storage but may lack Delta's features.        |\n",
    "| **Open Source**               | Open-source Delta Lake is available on GitHub.   | Parquet is an open standard but lacks Delta's features. |\n",
    "| **Use Cases**                 | Best suited for data lakes, data engineering.    | Suitable for efficient storage and processing.          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrameWriter.format(source: str) → pyspark.sql.readwriter.DataFrameWriter\n",
    "\n",
    "Specifies the underlying output data source.\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "    source str\n",
    "    string, name of the data source, e.g. ‘json’, ‘parquet’.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_test_data_df.write.format(\"delta\").option(\"overwriteSchema\", \"true\").mode(\n",
    "    \"overwrite\"\n",
    ").partitionBy(\"year_only\").save(\"./output/my_test_data_df_delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Delta table by specific partition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+---------+\n",
      "|   my_date|              open|              high|               low|             close|   volume|         adj_close|year_only|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+---------+\n",
      "|2010-01-04|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|     2010|\n",
      "|2010-01-05|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|     2010|\n",
      "|2010-01-06|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|     2010|\n",
      "|2010-01-07|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|     2010|\n",
      "|2010-01-08|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|     2010|\n",
      "|2010-01-11|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|     2010|\n",
      "|2010-01-12|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|     2010|\n",
      "|2010-01-13|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|     2010|\n",
      "|2010-01-14|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|     2010|\n",
      "|2010-01-15|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|     2010|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samp1 = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .load(\"./output/my_test_data_df_delta\")\n",
    "    .where(\"year_only=2010\")\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "samp1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPEND Using Delta Lake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1762"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(\"./output/my_test_data_df_delta\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp1.write.format(\"delta\").mode(\"append\").save(\"./output/my_test_data_df_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1772"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(\"./output/my_test_data_df_delta\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data from delta tables via SQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------------------+------------------+---------+------------------+---------+\n",
      "|   my_date|      open|      high|               low|             close|   volume|         adj_close|year_only|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+---------+\n",
      "|2010-01-04|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|     2010|\n",
      "|2010-01-05|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|     2010|\n",
      "|2010-01-06|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|     2010|\n",
      "|2010-01-07|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|     2010|\n",
      "|2010-01-08|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|     2010|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"SELECT * FROM delta.`{}` LIMIT 5;\".format(\n",
    "        \"F:\\\\development\\\\learn_spark\\\\output\\\\my_test_data_df_delta\"\n",
    "    )\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACID Transactions on Delta Tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|num_affected_rows|\n",
      "+-----------------+\n",
      "|                2|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"UPDATE delta.`{}` SET high = 12345 WHERE my_date = '2010-01-04';\".format(\n",
    "        \"F:\\\\development\\\\learn_spark\\\\output\\\\my_test_data_df_delta\"\n",
    "    )\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Evolution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = my_test_data_df.withColumn(\"new_column\", my_test_data_df[\"high\"] * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.write.option(\"overwriteSchema\", \"true\").format(\"delta\").mode(\"overwrite\").save(\n",
    "    \"./output/my_test_data_df_delta\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Travel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+---------+---------+---------+\n",
      "|   my_date|      open|      high|       low|     close|   volume|adj_close|year_only|\n",
      "+----------+----------+----------+----------+----------+---------+---------+---------+\n",
      "|2011-01-03|325.640003|330.260002|324.840012|    329.57|111284600|42.698941|     2011|\n",
      "|2011-01-04|332.439999|     332.5|328.149994|331.290012| 77270200|42.921785|     2011|\n",
      "+----------+----------+----------+----------+----------+---------+---------+---------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------+----------+----------+----------+----------+---------+---------+---------+\n",
      "|   my_date|      open|      high|       low|     close|   volume|adj_close|year_only|\n",
      "+----------+----------+----------+----------+----------+---------+---------+---------+\n",
      "|2011-01-03|325.640003|330.260002|324.840012|    329.57|111284600|42.698941|     2011|\n",
      "|2011-01-04|332.439999|     332.5|328.149994|331.290012| 77270200|42.921785|     2011|\n",
      "+----------+----------+----------+----------+----------+---------+---------+---------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------+----------+----------+------------------+----------+---------+------------------+---------+----------+\n",
      "|   my_date|      open|      high|               low|     close|   volume|         adj_close|year_only|new_column|\n",
      "+----------+----------+----------+------------------+----------+---------+------------------+---------+----------+\n",
      "|2010-01-04|213.429998|214.499996|212.38000099999996|214.009998|123432400|         27.727039|     2010|643.499988|\n",
      "|2010-01-05|214.599998|215.589994|        213.249994|214.379993|150476200|27.774976000000002|     2010|646.769982|\n",
      "+----------+----------+----------+------------------+----------+---------+------------------+---------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query the Delta table as it appeared at a specific version\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\n",
    "    \"./output/my_test_data_df_delta\"\n",
    ").show(2)\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\n",
    "    \"./output/my_test_data_df_delta\"\n",
    ").show(2)\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", 2).load(\n",
    "    \"./output/my_test_data_df_delta\"\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all available versions of current delta table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------+-------+\n",
      "|file_path                                                                         |version|\n",
      "+----------------------------------------------------------------------------------+-------+\n",
      "|year_only=2010/part-00000-cc0db3e3-ef48-454e-b14b-1b4c8e694587.c000.snappy.parquet|0      |\n",
      "|year_only=2010/part-00000-96a2c349-dcc3-4ace-bbca-b92de5c8fa12.c000.snappy.parquet|0      |\n",
      "|year_only=2011/part-00000-6d0f3e44-b91f-44c7-bf34-755fc3c07041.c000.snappy.parquet|0      |\n",
      "|year_only=2010/part-00001-c02114c9-659a-427d-ab46-625bb7916cf8.c000.snappy.parquet|0      |\n",
      "|year_only=2012/part-00000-1acd2d02-558b-4168-bff5-086c21137e6e.c000.snappy.parquet|0      |\n",
      "|year_only=2013/part-00000-0d5e096c-7e27-4312-b9dc-b2d210b23d2e.c000.snappy.parquet|0      |\n",
      "|year_only=2014/part-00000-b4cb60be-d7a0-4824-b2c4-704799173738.c000.snappy.parquet|0      |\n",
      "|year_only=2015/part-00000-d457ea1a-b90d-4b89-8a7a-58701e007dbb.c000.snappy.parquet|0      |\n",
      "|year_only=2016/part-00000-4b8183a1-87ba-482c-b5e7-098546de9d9f.c000.snappy.parquet|0      |\n",
      "|year_only=2010/part-00000-7a2885a4-2130-4fbf-b18b-c4b4136fb374.c000.snappy.parquet|1      |\n",
      "|part-00000-e0189646-2e13-4974-80fa-142500cecf0e-c000.snappy.parquet               |1      |\n",
      "|year_only=2010/part-00000-33ce225f-6876-4cf1-8eb5-2ae22ead5ce3.c000.snappy.parquet|2      |\n",
      "|part-00000-1027c735-6ea9-4d63-bfe4-0925730a2e51-c000.snappy.parquet               |2      |\n",
      "|year_only=2011/part-00000-a4c5b8e3-cefc-474a-8813-1bc6efd44b69.c000.snappy.parquet|2      |\n",
      "|year_only=2012/part-00000-7aeb3a79-bc11-4101-8607-1cb0adefd5b3.c000.snappy.parquet|2      |\n",
      "|year_only=2013/part-00000-9edcfd87-1cbe-4907-adda-b950dc3b16d9.c000.snappy.parquet|2      |\n",
      "|year_only=2014/part-00000-0375ba1d-5dcb-44aa-a127-9d40480588b5.c000.snappy.parquet|2      |\n",
      "|year_only=2015/part-00000-c0863a0d-e544-47aa-b594-bc59a217e4f8.c000.snappy.parquet|2      |\n",
      "|year_only=2016/part-00000-813f5acc-8006-4160-bc84-6e871df0b7d8.c000.snappy.parquet|2      |\n",
      "|year_only=2010/part-00000-a7f46a47-e071-461d-b528-06c47ed81606.c000.snappy.parquet|3      |\n",
      "+----------------------------------------------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "details = spark.read.json(\"./output/my_test_data_df_delta/_delta_log/*.json\")\n",
    "details = details.select(col(\"add\")[\"path\"].alias(\"file_path\"))\n",
    "details = (\n",
    "    details.withColumn(\"version\", substring(input_file_name(), -6, 1))\n",
    "    .filter(col(\"file_path\").isNotNull() == True)\n",
    "    .orderBy(col(\"version\"), ascending=True)\n",
    "    .show(truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concurrency Control\n",
    "\n",
    "Delta Lake uses optimistic concurrency control to handle concurrent writes, preventing conflicts and ensuring consistency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata Management\n",
    "\n",
    "Delta tables store metadata, providing information about the table and its transactions, which enhances reliability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------------+----+-----------+-------------------------------------------------------------+-----------------------+-----------------------+----------------+--------+-----------+----------+----------------+----------------+------------------------+\n",
      "|format|id                                  |name|description|location                                                     |createdAt              |lastModified           |partitionColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion|tableFeatures           |\n",
      "+------+------------------------------------+----+-----------+-------------------------------------------------------------+-----------------------+-----------------------+----------------+--------+-----------+----------+----------------+----------------+------------------------+\n",
      "|delta |581e4b25-a247-49ec-bba3-eaa7170f0a8d|NULL|NULL       |file:/F:/development/learn_spark/output/my_test_data_df_delta|2024-01-01 16:30:26.605|2024-01-06 15:44:16.728|[]              |1       |79274      |{}        |1               |2               |[appendOnly, invariants]|\n",
      "+------+------------------------------------+----+-----------+-------------------------------------------------------------+-----------------------+-----------------------+----------------+--------+-----------+----------+----------------+----------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"DESCRIBE DETAIL delta.`{}`;\".format(\n",
    "        \"F:\\\\development\\\\learn_spark\\\\output\\\\my_test_data_df_delta\"\n",
    "    )\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Delta Table Upsert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert into delta tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = spark.read.options(header=True, inferSchema=True).csv(\"./datasets/user.csv\")\n",
    "users_updated_df = spark.read.options(header=True, inferSchema=True).csv(\n",
    "    \"./datasets/user_updated.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.write.format(\"delta\").mode(\"overwrite\").save(\"./output/users_df\")\n",
    "users_updated_df.write.format(\"delta\").mode(\"overwrite\").save(\n",
    "    \"./output/users_updated_df\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Delta Tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_delta = DeltaTable.forPath(spark, \"./output/users_df\")\n",
    "users_updated_delta = DeltaTable.forPath(spark, \"./output/users_updated_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example SQL\n",
    "\n",
    "#   MERGE INTO people10m\n",
    "#   USING people10mupdates\n",
    "#   ON people10m.id = people10mupdates.id\n",
    "#   WHEN MATCHED THEN\n",
    "#     UPDATE SET\n",
    "#       id = people10mupdates.id,\n",
    "#       firstName = people10mupdates.firstName,\n",
    "#       middleName = people10mupdates.middleName,\n",
    "#       lastName = people10mupdates.lastName,\n",
    "#       gender = people10mupdates.gender,\n",
    "#       birthDate = people10mupdates.birthDate,\n",
    "#       ssn = people10mupdates.ssn,\n",
    "#       salary = people10mupdates.salary\n",
    "#   WHEN NOT MATCHED\n",
    "#     THEN INSERT (\n",
    "#       id,\n",
    "#       firstName,\n",
    "#       middleName,\n",
    "#       lastName,\n",
    "#       gender,\n",
    "#       birthDate,\n",
    "#       ssn,\n",
    "#       salary\n",
    "#     )\n",
    "#     VALUES (\n",
    "#       people10mupdates.id,\n",
    "#       people10mupdates.firstName,\n",
    "#       people10mupdates.middleName,\n",
    "#       people10mupdates.lastName,\n",
    "#       people10mupdates.gender,\n",
    "#       people10mupdates.birthDate,\n",
    "#       people10mupdates.ssn,\n",
    "#       people10mupdates.salary\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfUpdates = users_updated_delta.toDF()\n",
    "\n",
    "users_delta.alias(\"users\").merge(\n",
    "    dfUpdates.alias(\"users_updated\"), \"users.id = users_updated.id\"\n",
    ").whenMatchedUpdate(\n",
    "    set={\n",
    "        \"username\": \"users_updated.username\",\n",
    "        \"email\": \"users_updated.email\",\n",
    "        \"phone\": \"users_updated.phone\",\n",
    "    }\n",
    ").whenNotMatchedInsert(\n",
    "    values={\n",
    "        \"id\": \"users_updated.id\",\n",
    "        \"username\": \"users_updated.username\",\n",
    "        \"email\": \"users_updated.email\",\n",
    "        \"phone\": \"users_updated.phone\",\n",
    "    }\n",
    ").execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------------+-----+\n",
      "|id |username|email           |phone|\n",
      "+---+--------+----------------+-----+\n",
      "|1  |john    |john@gmail.com  |0    |\n",
      "|2  |jane    |jane02@gmail.com|12345|\n",
      "|3  |rick    |rick@gmail.com  |78678|\n",
      "|4  |mike    |mike@gmail.com  |9787 |\n",
      "+---+--------+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"SELECT * FROM delta.`{}`;\".format(\"F:\\\\development\\\\learn_spark\\\\output\\\\users_df\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------------+----+-----------+------------------------------------------------+----------------------+-----------------------+----------------+--------+-----------+----------+----------------+----------------+------------------------+\n",
      "|format|id                                  |name|description|location                                        |createdAt             |lastModified           |partitionColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion|tableFeatures           |\n",
      "+------+------------------------------------+----+-----------+------------------------------------------------+----------------------+-----------------------+----------------+--------+-----------+----------+----------------+----------------+------------------------+\n",
      "|delta |590cc3dd-a8ef-442b-a90e-4c8bf72f58bd|NULL|NULL       |file:/F:/development/learn_spark/output/users_df|2024-01-02 15:20:29.39|2024-01-06 15:44:41.255|[]              |1       |1277       |{}        |1               |2               |[appendOnly, invariants]|\n",
      "+------+------------------------------------+----+-----------+------------------------------------------------+----------------------+-----------------------+----------------+--------+-----------+----------+----------------+----------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"DESCRIBE DETAIL delta.`{}`;\".format(\n",
    "        \"F:\\\\development\\\\learn_spark\\\\output\\\\users_df\"\n",
    "    )\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of versions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation|operationParameters                                                                                                                                                            |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|7      |2024-01-06 15:44:41.255|NULL  |NULL    |MERGE    |{predicate -> [\"(id#14684 = id#14692)\"], matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|NULL|NULL    |NULL     |6          |Serializable  |false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1277, numTargetBytesRemoved -> 1245, numTargetRowsMatchedUpdated -> 3, executionTimeMs -> 2842, numTargetRowsInserted -> 1, numTargetRowsMatchedDeleted -> 0, scanTimeMs -> 1456, numTargetRowsUpdated -> 3, numOutputRows -> 4, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 4, numTargetFilesRemoved -> 1, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 1378}|NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|6      |2024-01-06 15:44:31.743|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}                                                                                                                                         |NULL|NULL    |NULL     |5          |Serializable  |false        |{numFiles -> 1, numOutputRows -> 3, numOutputBytes -> 1245}                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|5      |2024-01-02 16:06:30.303|NULL  |NULL    |MERGE    |{predicate -> [\"(id#37378 = id#37386)\"], matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|NULL|NULL    |NULL     |4          |Serializable  |false        |{numTargetRowsCopied -> 1, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1355, numTargetBytesRemoved -> 1355, numTargetRowsMatchedUpdated -> 4, executionTimeMs -> 606, numTargetRowsInserted -> 0, numTargetRowsMatchedDeleted -> 0, scanTimeMs -> 350, numTargetRowsUpdated -> 4, numOutputRows -> 5, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 4, numTargetFilesRemoved -> 1, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 253}   |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|4      |2024-01-02 16:06:21.966|NULL  |NULL    |MERGE    |{predicate -> [\"(id#37378 = id#37386)\"], matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|NULL|NULL    |NULL     |3          |Serializable  |false        |{numTargetRowsCopied -> 1, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1355, numTargetBytesRemoved -> 1273, numTargetRowsMatchedUpdated -> 3, executionTimeMs -> 729, numTargetRowsInserted -> 1, numTargetRowsMatchedDeleted -> 0, scanTimeMs -> 426, numTargetRowsUpdated -> 3, numOutputRows -> 5, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 4, numTargetFilesRemoved -> 1, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 301}   |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|3      |2024-01-02 16:04:45.596|NULL  |NULL    |MERGE    |{predicate -> [\"(id#37378 = id#37386)\"], matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|NULL|NULL    |NULL     |2          |Serializable  |false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1273, numTargetBytesRemoved -> 1245, numTargetRowsMatchedUpdated -> 3, executionTimeMs -> 998, numTargetRowsInserted -> 1, numTargetRowsMatchedDeleted -> 0, scanTimeMs -> 569, numTargetRowsUpdated -> 3, numOutputRows -> 4, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 4, numTargetFilesRemoved -> 1, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 424}   |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|2      |2024-01-02 16:03:54.029|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}                                                                                                                                         |NULL|NULL    |NULL     |1          |Serializable  |false        |{numFiles -> 1, numOutputRows -> 3, numOutputBytes -> 1245}                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|1      |2024-01-02 16:01:05.143|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}                                                                                                                                         |NULL|NULL    |NULL     |0          |Serializable  |false        |{numFiles -> 1, numOutputRows -> 3, numOutputBytes -> 1245}                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|0      |2024-01-02 15:20:29.552|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}                                                                                                                                         |NULL|NULL    |NULL     |NULL       |Serializable  |false        |{numFiles -> 1, numOutputRows -> 3, numOutputBytes -> 1277}                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "+-------+-----------------------+------+--------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"DESCRIBE HISTORY delta.`{}`;\".format(\n",
    "        \"F:\\\\development\\\\learn_spark\\\\output\\\\users_df\"\n",
    "    )\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|version|\n",
      "+-------+\n",
      "|      7|\n",
      "|      6|\n",
      "|      5|\n",
      "|      4|\n",
      "|      3|\n",
      "|      2|\n",
      "|      1|\n",
      "|      0|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_delta.history().select(\"version\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
