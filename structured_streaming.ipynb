{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Streaming\n",
    "\n",
    "Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. You can express your streaming computation the same way you would express a batch computation on static data. The Spark SQL engine will take care of running it incrementally and continuously and updating the final result as streaming data continues to arrive. You can use the Dataset/DataFrame API in Scala, Java, Python or R to express streaming aggregations, event-time windows, stream-to-batch joins, etc. The computation is executed on the same optimized Spark SQL engine. Finally, the system ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write-Ahead Logs. In short, Structured Streaming provides fast, scalable, fault-tolerant, end-to-end exactly-once stream processing without the user having to reason about streaming.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\n",
    "    \"spark.sql.repl.eagerEval.enabled\", True\n",
    ").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read from localhost:9999 by running nc -lk 9999\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataFrame representing the stream of input lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = (\n",
    "    spark.readStream.format(\"socket\")\n",
    "    .option(\"host\", \"192.168.1.19\")\n",
    "    .option(\"port\", 9999)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the lines into words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.select(explode(split(lines.value, \" \")).alias(\"word\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate running word count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCounts = words.groupBy(\"word\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. This lines DataFrame represents an unbounded table containing the streaming text data.\n",
    "2. This table contains one column of strings named “value”, and each line in the streaming text data becomes a row in the table. Note, that this is not currently receiving any data as we are just setting up the transformation, and have not yet started it.\n",
    "3. Next, we have used two built-in SQL functions - split and explode, to split each line into multiple rows with a word each. In addition, we use the function alias to name the new column as “word”.\n",
    "4. Finally, we have defined the wordCounts DataFrame by grouping by the unique values in the Dataset and counting them. Note that this is a streaming DataFrame which represents the running word counts of the stream.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start running the query that prints the running counts to the console\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = (\n",
    "#     wordCounts.writeStream.outputMode(\"complete\")\n",
    "#     .format(\"console\")\n",
    "#     .trigger(processingTime=\"2 seconds\")\n",
    "#     .start()\n",
    "# )\n",
    "\n",
    "\n",
    "# query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this code is executed, the streaming computation will have started in the background. The query object is a handle to that active streaming query, and we have decided to wait for the termination of the query using awaitTermination() to prevent the process from exiting while the query is active.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working of Spark Structured Streaming\n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png\">\n",
    "\n",
    "A query on the input will generate the “Result Table”. Every trigger interval (say, every 1 second), new rows get appended to the Input Table, which eventually updates the Result Table. Whenever the result table gets updated, we would want to write the changed result rows to an external sink.\n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/structured-streaming-model.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The “Output” is defined as what gets written out to the external storage. The output can be defined in a different mode:\n",
    "\n",
    "1. Complete Mode - The entire updated Result Table will be written to the external storage. It is up to the storage connector to decide how to handle writing of the entire table.\n",
    "2. Append Mode - Only the new rows appended in the Result Table since the last trigger will be written to the external storage. This is applicable only on the queries where existing rows in the Result Table are not expected to change.\n",
    "3. Update Mode - Only the rows that were updated in the Result Table since the last trigger will be written to the external storage (available since Spark 2.1.1). Note that this is different from the Complete Mode in that this mode only outputs the rows that have changed since the last trigger. If the query doesn’t contain aggregations, it will be equivalent to Append mode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the use of this model, let’s understand the model in context of the Quick Example above. The first lines DataFrame is the input table, and the final wordCounts DataFrame is the result table. Note that the query on streaming lines DataFrame to generate wordCounts is exactly the same as it would be a static DataFrame. However, when this query is started, Spark will continuously check for new data from the socket connection. If there is new data, Spark will run an “incremental” query that combines the previous running counts with the new data to compute updated counts, as shown below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://spark.apache.org/docs/latest/img/structured-streaming-example-model.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Structured Streaming does not materialize the entire table. It reads the latest available data from the streaming data source, processes it incrementally to update the result, and then discards the source data. It only keeps around the minimal intermediate state data as required to update the result (e.g. intermediate counts in the earlier example).\n",
    "\n",
    "This model is significantly different from many other stream processing engines. Many streaming systems require the user to maintain running aggregations themselves, thus having to reason about fault-tolerance, and data consistency (at-least-once, or at-most-once, or exactly-once). In this model, Spark is responsible for updating the Result Table when there is new data, thus relieving the users from reasoning about it. As an example, let’s see how this model handles event-time based processing and late arriving data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Event-time and Late Data\n",
    "\n",
    "Event-time is the time embedded in the data itself. For many applications, you may want to operate on this event-time. For example, if you want to get the number of events generated by IoT devices every minute, then you probably want to use the time when the data was generated (that is, event-time in the data), rather than the time Spark receives them. This event-time is very naturally expressed in this model – each event from the devices is a row in the table, and event-time is a column value in the row. This allows window-based aggregations (e.g. number of events every minute) to be just a special type of grouping and aggregation on the event-time column – each time window is a group and each row can belong to multiple windows/groups. Therefore, such event-time-window-based aggregation queries can be defined consistently on both a static dataset (e.g. from collected device events logs) as well as on a data stream, making the life of the user much easier.\n",
    "\n",
    "Furthermore, this model naturally handles data that has arrived later than expected based on its event-time. Since Spark is updating the Result Table, it has full control over updating old aggregates when there is late data, as well as cleaning up old aggregates to limit the size of intermediate state data. Since Spark 2.1, we have support for watermarking which allows the user to specify the threshold of late data, and allows the engine to accordingly clean up old state. These are explained later in more detail in the Window Operations section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fault Tolerance Semantics\n",
    "\n",
    "Delivering end-to-end exactly-once semantics was one of key goals behind the design of Structured Streaming. To achieve that, we have designed the Structured Streaming sources, the sinks and the execution engine to reliably track the exact progress of the processing so that it can handle any kind of failure by restarting and/or reprocessing. Every streaming source is assumed to have offsets (similar to Kafka offsets, or Kinesis sequence numbers) to track the read position in the stream. The engine uses checkpointing and write-ahead logs to record the offset range of the data being processed in each trigger. The streaming sinks are designed to be idempotent for handling reprocessing. Together, using replayable sources and idempotent sinks, Structured Streaming can ensure end-to-end exactly-once semantics under any failure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating streaming DataFrames and streaming Datasets\n",
    "\n",
    "Streaming DataFrames can be created through the DataStreamReader interface (Scala/Java/Python docs) returned by SparkSession.readStream(). In R, with the read.stream() method. Similar to the read interface for creating static DataFrame, you can specify the details of the source – data format, schema, options, etc.\n",
    "\n",
    "Input Sources\n",
    "There are a few built-in sources.\n",
    "\n",
    "1. File source - Reads files written in a directory as a stream of data. Files will be processed in the order of file modification time. If latestFirst is set, order will be reversed. Supported file formats are text, CSV, JSON, ORC, Parquet. See the docs of the DataStreamReader interface for a more up-to-date list, and supported options for each file format. Note that the files must be atomically placed in the given directory, which in most file systems, can be achieved by file move operations.\n",
    "   Kafka source - Reads data from Kafka. It’s compatible with Kafka broker versions 0.10.0 or higher. See the Kafka Integration Guide for more details.\n",
    "\n",
    "2. Socket source (for testing) - Reads UTF8 text data from a socket connection. The listening server socket is at the driver. Note that this should be used only for testing as this does not provide end-to-end fault-tolerance guarantees.\n",
    "\n",
    "3. Rate source (for testing) - Generates data at the specified number of rows per second, each output row contains a timestamp and value. Where timestamp is a Timestamp type containing the time of message dispatch, and value is of Long type containing the message count, starting from 0 as the first row. This source is intended for testing and benchmarking.\n",
    "\n",
    "Rate Per Micro-Batch source (for testing) - Generates data at the specified number of rows per micro-batch, each output row contains a timestamp and value. Where timestamp is a Timestamp type containing the time of message dispatch, and value is of Long type containing the message count, starting from 0 as the first row. Unlike rate data source, this data source provides a consistent set of input rows per micro-batch regardless of query execution (configuration of trigger, query being lagging, etc.), say, batch 0 will produce 0~999 and batch 1 will produce 1000~1999, and so on. Same applies to the generated time. This source is intended for testing and benchmarking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading a Stream in Spark\n",
    "\n",
    "The method `SparkSession.readStream` returns a `DataStreamReader` used to configure the stream.\n",
    "\n",
    "There are a number of key points to the configuration of a `DataStreamReader`:\n",
    "\n",
    "- The schema\n",
    "- The type of stream: Files, Kafka, TCP/IP, etc\n",
    "- Configuration specific to the type of stream\n",
    "  - For files, the file type, the path to the files, max files, etc...\n",
    "  - For TCP/IP the server's address, port number, etc...\n",
    "  - For Kafka the server's address, port, topics, partitions, etc...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Schema\n",
    "\n",
    "Every streaming DataFrame must have a schema - the definition of column names and data types.\n",
    "\n",
    "Some sources such as Pub/Sub sources like Kafka and Event Hubs define the schema for you.\n",
    "\n",
    "For file-based streaming sources, the schema must be user-defined.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why must a schema be specified for a streaming DataFrame?\n",
    "\n",
    "To say that another way...\n",
    "\n",
    "### Why are streaming DataFrames unable to infer/read a schema?\n",
    "\n",
    "If you have enough data, you can infer the schema.\n",
    "<br><br>\n",
    "If you don't have enough data you run the risk of miss-inferring the schema.\n",
    "<br><br>\n",
    "For example, you think you have all integers but the last value contains \"1.123\" (a float) or \"snoopy\" (a string).\n",
    "<br><br>\n",
    "With a stream, we have to assume we don't have enough data because we are starting with zero records.\n",
    "<br><br>\n",
    "And unlike reading from a table or parquet file, there is nowhere from which to \"read\" the stream's schema.\n",
    "<br><br>\n",
    "For this reason, we must specify the schema manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema = \"Arrival_Time timestamp, Creation_Time timestamp, Device string, Index integer, Model string, User String, gt string, x double, y double, z double\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring a File Stream\n",
    "\n",
    "In our example below, we will be consuming files written continuously to a pre-defined directory.\n",
    "\n",
    "To control how much data is pulled into Spark at once, we can specify the option `maxFilesPerTrigger`.\n",
    "\n",
    "In our example below, we will be reading in only one file for every trigger interval:\n",
    "\n",
    "`.option(\"maxFilesPerTrigger\", 1)`\n",
    "\n",
    "Both the location and file type are specified with the following call, which itself returns a `DataFrame`:\n",
    "\n",
    "`.json(dataPath)`\n",
    "\n",
    "> Supported formats\n",
    "\n",
    "1. csv(path[, schema, sep, encoding, quote, …]): Loads a CSV file stream and returns the result as a DataFrame.\n",
    "2. format(source): Specifies the input data source format.\n",
    "3. json(path[, schema, primitivesAsString, …]): Loads a JSON file stream and returns the results as a DataFrame.\n",
    "4. load([path, format, schema]): Loads a data stream from a data source and returns it as a DataFrame.\n",
    "5. option(key, value): Adds an input option for the underlying data source.\n",
    "6. options(\\*\\*options): Adds input options for the underlying data source.\n",
    "7. orc(path[, mergeSchema, pathGlobFilter, …]): Loads a ORC file stream, returning the result as a DataFrame.\n",
    "8. parquet(path[, mergeSchema, pathGlobFilter, …]): Loads a Parquet file stream, returning the result as a DataFrame.\n",
    "9. schema(schema): Specifies the input schema.\n",
    "10. table(tableName): Define a Streaming DataFrame on a Table.\n",
    "11. text(path[, wholetext, lineSep, …]): Loads a text file stream and returns a DataFrame whose schema starts with a string column named “value”, and followed by partitioned columns if there are any.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialDF = (\n",
    "    spark.readStream.option(  # Returns DataStreamReader\n",
    "        \"maxFilesPerTrigger\", 1\n",
    "    )  # Force processing of only 1 file per trigger\n",
    "    .schema(dataSchema)  # Required for all streaming DataFrames\n",
    "    .json(\"./datasets/activity-data\")  # The stream's source directory and file type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with the initial `DataFrame`, we can apply some transformations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingDF = initialDF.withColumnRenamed(\"Index\", \"User_ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiate b/w a normal and a streaming DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamingDF.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupported Operations\n",
    "\n",
    "Most operations on a \"streaming\" DataFrame are identical to a \"static\" DataFrame.\n",
    "\n",
    "There are some exceptions to this.\n",
    "\n",
    "One such example would be to sort our never-ending stream by `Recorded_At`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting is not supported on an unaggregated stream\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sortedDF = streamingDF.orderBy(col(\"Recorded_At\").desc())\n",
    "    display(sortedDF)\n",
    "except:\n",
    "    print(\"Sorting is not supported on an unaggregated stream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting is one of a handful of operations that is either too complex or logically not possible to do with a stream.\n",
    "\n",
    "#### Unsupported Operations\n",
    "\n",
    "There are a few DataFrame/Dataset operations that are not supported with streaming DataFrames/Datasets. Some of them are as follows.\n",
    "\n",
    "1. Limit and take the first N rows are not supported on streaming Datasets.\n",
    "2. Distinct operations on streaming Datasets are not supported.\n",
    "3. Sorting operations are supported on streaming Datasets only after an aggregation and in Complete Output Mode.\n",
    "\n",
    "Few types of outer joins on streaming Datasets are not supported. See the support matrix in the Join Operations section for more details.\n",
    "\n",
    "Chaining multiple stateful operations on streaming Datasets is not supported with Update and Complete mode.\n",
    "\n",
    "In addition, mapGroupsWithState/flatMapGroupsWithState operation followed by other stateful operation is not supported in Append mode.\n",
    "A known workaround is to split your streaming query into multiple queries having a single stateful operation per each query, and ensure end-to-end exactly once per query. Ensuring end-to-end exactly once for the last query is optional.\n",
    "In addition, there are some Dataset methods that will not work on streaming Datasets. They are actions that will immediately run queries and return results, which does not make sense on a streaming Dataset. Rather, those functionalities can be done by explicitly starting a streaming query (see the next section regarding that).\n",
    "\n",
    "1. count() - Cannot return a single count from a streaming Dataset. Instead, use ds.groupBy().count() which returns a streaming Dataset containing a running count.\n",
    "2. foreach() - Instead use ds.writeStream.foreach(...) (see next section).\n",
    "3. show() - Instead use the console sink (see next section).\n",
    "\n",
    "If you try any of these operations, you will see an AnalysisException like “operation XYZ is not supported with streaming DataFrames/Datasets”. While some of them may be supported in future releases of Spark, there are others which are fundamentally hard to implement on streaming data efficiently. For example, sorting on the input stream is not supported, as it requires keeping track of all the data received in the stream. This is therefore fundamentally hard to execute efficiently.\n",
    "\n",
    "> We will see in the following module how we can sort an **aggregated** stream.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Writing a Stream</h2>\n",
    "\n",
    "The method `DataFrame.writeStream` returns a `DataStreamWriter` used to configure the output of the stream.\n",
    "\n",
    "There are a number of parameters to the `DataStreamWriter` configuration:\n",
    "\n",
    "- Query's name (optional) - This name must be unique among all the currently active queries in the associated SQLContext.\n",
    "- Trigger (optional) - Default value is `ProcessingTime(0`) and it will run the query as fast as possible.\n",
    "- Checkpointing directory (optional for pub/sub sinks)\n",
    "- Output mode\n",
    "- Output sink\n",
    "- Configuration specific to the output sink, such as:\n",
    "  - The host, port and topic of the receiving Kafka server\n",
    "  - The file format and final destination of files\n",
    "  - A <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=foreach#pyspark.sql.streaming.DataStreamWriter.foreach\"target=\"_blank\">custom sink</a> via `writeStream.foreach(...)`\n",
    "\n",
    "Once the configuration is completed, we can trigger the job with a call to `.start()`\n",
    "\n",
    "### Triggers\n",
    "\n",
    "The trigger specifies when the system should process the next set of data.\n",
    "\n",
    "| Trigger Type                           | Example                                       | Notes                                                                                                                                                                                                                                  |\n",
    "| -------------------------------------- | --------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| Unspecified                            |                                               | _DEFAULT_- The query will be executed as soon as the system has completed processing the previous query                                                                                                                                |\n",
    "| Fixed interval micro-batches           | `.trigger(Trigger.ProcessingTime(\"6 hours\"))` | The query will be executed in micro-batches and kicked off at the user-specified intervals                                                                                                                                             |\n",
    "| One-time micro-batch                   | `.trigger(Trigger.Once())`                    | The query will execute _only one_ micro-batch to process all the available data and then stop on its own                                                                                                                               |\n",
    "| Continuous w/fixed checkpoint interval | `.trigger(Trigger.Continuous(\"1 second\"))`    | The query will be executed in a low-latency, <a href=\"http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#continuous-processing\" target = \"_blank\">continuous processing mode</a>. _EXPERIMENTAL_ in 2.3.2 |\n",
    "\n",
    "In the example below, you will be using a fixed interval of 3 seconds:\n",
    "\n",
    "`.trigger(Trigger.ProcessingTime(\"3 seconds\"))`\n",
    "\n",
    "### Checkpointing\n",
    "\n",
    "A <b>checkpoint</b> stores the current state of your streaming job to a reliable storage system such as Azure Blob Storage or HDFS. It does not store the state of your streaming job to the local file system of any node in your cluster.\n",
    "\n",
    "Together with write ahead logs, a terminated stream can be restarted and it will continue from where it left off.\n",
    "\n",
    "To enable this feature, you only need to specify the location of a checkpoint directory:\n",
    "\n",
    "`.option(\"checkpointLocation\", checkpointPath)`\n",
    "\n",
    "Points to consider:\n",
    "\n",
    "- If you do not have a checkpoint directory, when the streaming job stops, you lose all state around your streaming job and upon restart, you start from scratch.\n",
    "- For some sinks, you will get an error if you do not specify a checkpoint directory:<br/>\n",
    "  `analysisException: 'checkpointLocation must be specified either through option(\"checkpointLocation\", ...)..`\n",
    "- Also note that every streaming job should have its own checkpoint directory: no sharing.\n",
    "\n",
    "### Output Modes\n",
    "\n",
    "| Mode         | Example                   | Notes                                                                                                                                      |\n",
    "| ------------ | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| **Complete** | `.outputMode(\"complete\")` | The entire updated Result Table is written to the sink. The individual sink implementation decides how to handle writing the entire table. |\n",
    "| **Append**   | `.outputMode(\"append\")`   | Only the new rows appended to the Result Table since the last trigger are written to the sink.                                             |\n",
    "| **Update**   | `.outputMode(\"update\")`   | Only the rows in the Result Table that were updated since the last trigger will be outputted to the sink. Since Spark 2.1.1                |\n",
    "\n",
    "In the example below, we are writing to a Parquet directory which only supports the `append` mode:\n",
    "\n",
    "`dsw.outputMode(\"append\")`\n",
    "\n",
    "### Output Sinks\n",
    "\n",
    "`DataStreamWriter.format` accepts the following values, among others:\n",
    "\n",
    "| Output Sink | Example                                         | Notes                                                                                   |\n",
    "| ----------- | ----------------------------------------------- | --------------------------------------------------------------------------------------- |\n",
    "| **File**    | `dsw.format(\"parquet\")`, `dsw.format(\"csv\")`... | Dumps the Result Table to a file. Supports Parquet, json, csv, etc.                     |\n",
    "| **Kafka**   | `dsw.format(\"kafka\")`                           | Writes the output to one or more topics in Kafka                                        |\n",
    "| **Console** | `dsw.format(\"console\")`                         | Prints data to the console (useful for debugging)                                       |\n",
    "| **Memory**  | `dsw.format(\"memory\")`                          | Updates an in-memory table, which can be queried through Spark SQL or the DataFrame API |\n",
    "| **foreach** | `dsw.foreach(writer: ForeachWriter)`            | This is your \"escape hatch\", allowing you to write your own type of sink.               |\n",
    "| **Delta**   | `dsw.format(\"delta\")`                           | A proprietary sink                                                                      |\n",
    "\n",
    "In the example below, we will be appending files to a Parquet directory and specifying its location with this call:\n",
    "\n",
    "`.format(\"parquet\").start(outputPathDir)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working file streaming example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputPathDir = \"./streaming/outputdir/streamop.json\"\n",
    "checkpointPath = (\n",
    "    \"./streaming/checkpointdir\"  # A subdirectory for our checkpoint & W-A logs\n",
    ")\n",
    "\n",
    "streamingQuery = (\n",
    "    streamingDF.writeStream.queryName(  # Start with our \"streaming\" DataFrame  # Get the DataStreamWriter\n",
    "        \"stream_1p\"\n",
    "    )  # Name the query\n",
    "    .trigger(processingTime=\"1 seconds\")  # Configure for a 3-second micro-batch\n",
    "    .format(\"json\")  # Specify the sink type, a Parquet file\n",
    "    .option(\n",
    "        \"checkpointLocation\", checkpointPath\n",
    "    )  # Specify the location of checkpoint files & W-A logs\n",
    "    .outputMode(\"append\")  # Write only new data to the \"file\"\n",
    "    .start(outputPathDir)  # Start the job, writing to the specified directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Managing Streaming Queries</h2>\n",
    "\n",
    "When a query is started, the `StreamingQuery` object can be used to monitor and manage the query.\n",
    "\n",
    "| Method               | Description                                                                                   |\n",
    "| -------------------- | --------------------------------------------------------------------------------------------- |\n",
    "| `id`                 | get unique identifier of the running query that persists across restarts from checkpoint data |\n",
    "| `runId`              | get unique id of this run of the query, which will be generated at every start/restart        |\n",
    "| `name`               | get name of the auto-generated or user-specified name                                         |\n",
    "| `explain()`          | print detailed explanations of the query                                                      |\n",
    "| `stop()`             | stop query                                                                                    |\n",
    "| `awaitTermination()` | block until query is terminated, with stop() or with error                                    |\n",
    "| `exception`          | exception if query terminated with error                                                      |\n",
    "| `recentProgress`     | array of most recent progress updates for this query                                          |\n",
    "| `lastProgress`       | most recent progress update of this streaming query                                           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamingQuery.recentProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6f8d599e-0a71-45c4-94e5-925dcb089c96: stream_1p\n"
     ]
    }
   ],
   "source": [
    "for s in spark.streams.active:  # Iterate over all streams\n",
    "    print(\"{}: {}\".format(s.id, s.name))  # Print the stream's id and name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below stops the `streamingQuery` defined above and introduces `awaitTermination()`\n",
    "\n",
    "`awaitTermination()` will block the current thread\n",
    "\n",
    "- Until the stream stops or\n",
    "- Until the specified timeout elapses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingQuery.awaitTermination(\n",
    "    5\n",
    ")  # Stream for another 5 seconds while the current thread blocks\n",
    "streamingQuery.stop()  # Stop the stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> The Display function</h2>\n",
    "\n",
    "Within the Databricks notebooks, we can use the `display()` function to render a live plot\n",
    "\n",
    "When you pass a \"streaming\" `DataFrame` to `display()`:\n",
    "\n",
    "- A \"memory\" sink is being used\n",
    "- The output mode is complete\n",
    "- The query name is specified with the `streamName` parameter\n",
    "- The trigger is specified with the `trigger` parameter\n",
    "- The checkpointing location is specified with the `checkpointLocation`\n",
    "\n",
    "`display(myDF, streamName = \"myQuery\")`\n",
    "\n",
    "> We just programmatically stopped our only streaming query in the previous cell. In the cell below, `display` will automatically start our streaming DataFrame, `streamingDF`. We are passing `stream_2p` as the name for this newly started stream.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myStream = \"stream_2p\"\n",
    "# display(streamingDF, streamName=myStream) only for databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the value passed to `streamName` in the call to `display`, we can programatically access this specific stream:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for stream_2p\n"
     ]
    }
   ],
   "source": [
    "print(\"Looking for {}\".format(\"stream_2p\"))\n",
    "\n",
    "for stream in spark.streams.active:  # Loop over all active streams\n",
    "    if stream.name == \"stream_2p\":  # Single out \"streamWithTimestamp\"\n",
    "        print(\"Found {} ({})\".format(stream.name, stream.id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the `streamName` get's registered as a temporary table pointing to the memory sink, we can use SQL to query the sink.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop all remaining streams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in spark.streams.active:\n",
    "    s.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
