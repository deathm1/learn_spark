{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, col, window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Aggregations\n",
    "\n",
    "Continuous applications often require near real-time decisions on real-time, aggregated statistics.\n",
    "\n",
    "Some examples include\n",
    "\n",
    "- Aggregating errors in data from IoT devices by type\n",
    "- Detecting anomalous behavior in a server's log file by aggregating by country.\n",
    "- Doing behavior analysis on instant messages via hash tags.\n",
    "\n",
    "However, in the case of streams, you generally don't want to run aggregations over the entire dataset.\n",
    "\n",
    "> What problems might you encounter if you aggregate over a stream's entire dataset?\n",
    "\n",
    "While streams have a definitive start, there conceptually is no end to the flow of data.\n",
    "\n",
    "Because there is no \"end\" to a stream, the size of the dataset grows in perpetuity.\n",
    "\n",
    "This means that your cluster will eventually run out of resources.\n",
    "\n",
    "Instead of aggregating over the entire dataset, you can aggregate over data grouped by windows of time (say, every 5 minutes or every hour).\n",
    "\n",
    "This is referred to as `windowing`\n",
    "\n",
    "## Windowing\n",
    "\n",
    "If we were using a static DataFrame to produce an aggregate count, we could use `groupBy()` and `count()`.\n",
    "\n",
    "Instead we accumulate counts within a sliding window, answering questions like \"How many records are we getting every second?\"\n",
    "\n",
    "- **Sliding windows** : The windows overlap and a single event may be aggregated into multiple windows.\n",
    "\n",
    "- **Tumbling Windows**: The windows do not overlap and a single event will be aggregated into only one window.\n",
    "\n",
    "The diagram below shows sliding windows.\n",
    "\n",
    "The following illustration, from the <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\" target=\"_blank\">Structured Streaming Programming Guide</a> guide, helps us understanding how it works:\n",
    "\n",
    "<img src=\"http://spark.apache.org/docs/latest/img/structured-streaming-window.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event Time vs Receipt Time\n",
    "\n",
    "- **Event Time** is the time at which the event occurred in the real world.\n",
    "\n",
    "- **Event Time** is **NOT** something maintained by the Structured Streaming framework.\n",
    "\n",
    "At best, Structured Streaming only knows about **Receipt Time** - the time a piece of data arrived in Spark.\n",
    "\n",
    "### What are some examples of **Event Time**? **of Receipt Time**?\n",
    "\n",
    "#### Examples of _Event Time_:\n",
    "\n",
    "- The timestamp recorded in each record of a log file\n",
    "- The instant at which an IoT device took a measurement\n",
    "- The moment a REST API received a request\n",
    "\n",
    "#### Examples of _Receipt Time_:\n",
    "\n",
    "- A timestamp added to a DataFrame the moment it was processed by Spark\n",
    "- The timestamp extracted from an hourly log file's file name\n",
    "- The time at which an IoT hub received a report of a device's measurement\n",
    "  - Presumably offset by some delay from when the measurement was taken\n",
    "\n",
    "### What are some of the inherent problems with using **Receipt Time**?\n",
    "\n",
    "The main problem with using **Receipt Time** is going to be with accuracy. For example:\n",
    "\n",
    "- The time between when an IoT device takes a measurement vs when it is reported can be off by several minutes.\n",
    "  - This could have significant ramifications to security and health devices, for example\n",
    "- The timestamp embedded in an hourly log file can be off by up to one hour making correlations to other events extremely difficult\n",
    "- The timestamp added by Spark as part of a DataFrame transformation can be off by hours to weeks to months depending on when the event occurred and when the job ran.\n",
    "\n",
    "### When might it be OK to use **Receipt Time** instead of **Event Time**?\n",
    "\n",
    "When accuracy is not a significant concern - that is **Receipt Time** is close enough to **Event Time**\n",
    "\n",
    "One example would be for IoT events that can be delayed by minutes but the resolution of your query is by days or months (close enough)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windowed Streaming Example\n",
    "\n",
    "Each line in the file contains a JSON record with two fields: `time` and `action`\n",
    "New files are being written to this directory continuously (aka streaming).\n",
    "Theoretically, there is no end to this process.\n",
    "Let's start by looking at the head of one such file:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the FileStream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "myschema = \"time timestamp, action string\"\n",
    "\n",
    "inputDF = (\n",
    "    spark.readStream.schema(myschema)\n",
    "    .option(\"maxFilesPerTrigger\", 10)\n",
    "    .json(\"./datasets/time-series/\")\n",
    ")\n",
    "\n",
    "countsDF = inputDF.groupBy(col(\"action\"), window(col(\"time\"), \"1 hour\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streamingQuery = (\n",
    "#     countsDF.writeStream.format(\"json\")\n",
    "#     .trigger(processingTime=\"1 seconds\")\n",
    "#     .option(\"checkpointLocation\", \"chkpt\")\n",
    "#     .outputMode(\"append\")\n",
    "#     .start(\"test\")\n",
    "# )\n",
    "# streamingQuery.awaitTermination()\n",
    "\n",
    "\n",
    "# query = countsDF.writeStream \\\n",
    "#     .format(\"console\") \\\n",
    "#     .outputMode(\"update\") \\\n",
    "#     .start()\n",
    "\n",
    "\n",
    "streamingQuery = (\n",
    "    countsDF.writeStream.queryName(  # Start with our \"streaming\" DataFrame  # Get the DataStreamWriter\n",
    "        \"stream_1p\"\n",
    "    )  # Name the query\n",
    "    .trigger(processingTime=\"1 seconds\")  # Configure for a 3-second micro-batch\n",
    "    .format(\"json\")  # Specify the sink type, a Parquet file\n",
    "    .option(\n",
    "        \"checkpointLocation\", \"./streaming/checkpointdir/\"\n",
    "    )  # Specify the location of checkpoint files & W-A logs\n",
    "    .outputMode(\"append\")  # Write only new data to the \"file\"\n",
    "    .start(\n",
    "        \"./streaming/outputdir/\"\n",
    "    )  # Start the job, writing to the specified directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingQuery.awaitTermination(\n",
    "    6\n",
    ")  # Stream for another 5 seconds while the current thread blocks\n",
    "streamingQuery.stop()  # Stop the stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------------------------------------------------+\n",
      "|action|count|window                                                        |\n",
      "+------+-----+--------------------------------------------------------------+\n",
      "|Scroll|1215 |{2016-07-26T08:30:00.000+05:30, 2016-07-26T07:30:00.000+05:30}|\n",
      "|Click |1216 |{2016-07-26T08:30:00.000+05:30, 2016-07-26T07:30:00.000+05:30}|\n",
      "|Close |1224 |{2016-07-26T08:30:00.000+05:30, 2016-07-26T07:30:00.000+05:30}|\n",
      "|Open  |1220 |{2016-07-26T08:30:00.000+05:30, 2016-07-26T07:30:00.000+05:30}|\n",
      "+------+-----+--------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.json(\"./streaming/outputdir/\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Considerations\n",
    "\n",
    "If you run that query, as is, it will take a surprisingly long time to start generating data. What's the cause of the delay?\n",
    "\n",
    "If you expand the **Spark Jobs** component, you'll see something like this:\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/structured-streaming-shuffle-partitions-200.png\"/>\n",
    "\n",
    "It's our `groupBy()`. `groupBy()` causes a _shuffle_, and, by default, Spark SQL shuffles to 200 partitions. In addition, we're doing a _stateful_ aggregation: one that requires Structured Streaming to maintain and aggregate data over time.\n",
    "\n",
    "When doing a stateful aggregation, Structured Streaming must maintain an in-memory _state map_ for each window within each partition. For fault tolerance reasons, the state map has to be saved after a partition is processed, and it needs to be saved somewhere fault-tolerant. To meet those requirements, the Streaming API saves the maps to a distributed store. On some clusters, that will be HDFS. Azure Databricks uses the DBFS.\n",
    "\n",
    "That means that every time it finishes processing a window, the Streaming API writes its internal map to disk. The write has some overhead, typically between 1 and 2 seconds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the cause of the delay?\n",
    "\n",
    "- `groupBy()` causes a **shuffle**\n",
    "- By default, this produces **200 partitions**\n",
    "- Plus a **stateful aggregation** to be maintained **over time**\n",
    "\n",
    "This results in :\n",
    "\n",
    "- Maintenance of an **in-memory state map** for **each window** within **each partition**\n",
    "- Writing of the state map to a fault-tolerant store\n",
    "  - On some clusters, that will be HDFS\n",
    "  - Azure Databricks uses the DBFS\n",
    "- Around 1 to 2 seconds overhead\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Partition Best Practices\n",
    "\n",
    "One way to reduce this overhead is to reduce the number of partitions Spark shuffles to.\n",
    "In most cases, you want a 1-to-1 mapping of partitions to cores for streaming applications.\n",
    "\n",
    "## Run query with proper setting for shuffle partitions\n",
    "\n",
    "Rerun the query below and notice the performance improvement.\n",
    "Once the data is loaded, render a line graph with\n",
    "\n",
    "- **Keys** is set to `start`\n",
    "- **Series groupings** is set to `action`\n",
    "- **Values** is set to `count`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)\n",
    "\n",
    "\n",
    "streamingQuery = (\n",
    "    countsDF.writeStream.queryName(  # Start with our \"streaming\" DataFrame  # Get the DataStreamWriter\n",
    "        \"stream_3p\"\n",
    "    )  # Name the query\n",
    "    .trigger(processingTime=\"1 seconds\")  # Configure for a 3-second micro-batch\n",
    "    .format(\"json\")  # Specify the sink type, a Parquet file\n",
    "    .option(\n",
    "        \"checkpointLocation\", \"./streaming/checkpointdir2/\"\n",
    "    )  # Specify the location of checkpoint files & W-A logs\n",
    "    .outputMode(\"append\")  # Write only new data to the \"file\"\n",
    "    .start(\n",
    "        \"./streaming/outputdir2/\"\n",
    "    )  # Start the job, writing to the specified directory\n",
    ")\n",
    "\n",
    "streamingQuery.awaitTermination(\n",
    "    6\n",
    ")  # Stream for another 5 seconds while the current thread blocks\n",
    "streamingQuery.stop()  # Stop the stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------------------------------------------------+\n",
      "|action|count|window                                                        |\n",
      "+------+-----+--------------------------------------------------------------+\n",
      "|Scroll|1215 |{2016-07-26T08:30:00.000+05:30, 2016-07-26T07:30:00.000+05:30}|\n",
      "|Click |1216 |{2016-07-26T08:30:00.000+05:30, 2016-07-26T07:30:00.000+05:30}|\n",
      "|Close |1224 |{2016-07-26T08:30:00.000+05:30, 2016-07-26T07:30:00.000+05:30}|\n",
      "|Open  |1220 |{2016-07-26T08:30:00.000+05:30, 2016-07-26T07:30:00.000+05:30}|\n",
      "+------+-----+--------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.json(\"./streaming/outputdir2/\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop all streams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in spark.streams.active:  # Iterate over all active streams\n",
    "    s.stop()  # Stop the stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem with Generating Many Windows\n",
    "\n",
    "We are generating a window for every 1 hour aggregate.\n",
    "_Every window_ has to be separately persisted and maintained.\n",
    "Over time, this aggregated data will build up in the driver.\n",
    "The end result being a massive slowdown if not an OOM Error.\n",
    "\n",
    "## How do we fix that problem?\n",
    "\n",
    "One simple solution is to increase the size of our window (say, to 2 hours).\n",
    "That way, we're generating fewer windows.\n",
    "But if the job runs for a long time, we're still building up an unbounded set of windows.\n",
    "Eventually, we could hit resource limits.\n",
    "\n",
    "# Watermarking\n",
    "\n",
    "A better solution to the problem is to define a cut-off.\n",
    "A point after which Structured Streaming will commit windowed data to sink, or throw it away if the sink is console or memory as `display()` mimics.\n",
    "That's what _watermarking_ allows us to do.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------------------------------------------------+\n",
      "|action|count|window                                                        |\n",
      "+------+-----+--------------------------------------------------------------+\n",
      "|Scroll|1215 |{2016-07-26T08:30:00.000+05:30, 2016-07-26T07:30:00.000+05:30}|\n",
      "|Click |1216 |{2016-07-26T08:30:00.000+05:30, 2016-07-26T07:30:00.000+05:30}|\n",
      "|Close |1224 |{2016-07-26T08:30:00.000+05:30, 2016-07-26T07:30:00.000+05:30}|\n",
      "|Open  |1220 |{2016-07-26T08:30:00.000+05:30, 2016-07-26T07:30:00.000+05:30}|\n",
      "+------+-----+--------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "watermarkedDF = (\n",
    "    inputDF.withWatermark(\"time\", \"2 hours\")  # Specify a 2-hour watermark\n",
    "    .groupBy(\n",
    "        col(\"action\"), window(col(\"time\"), \"1 hour\")  # Aggregate by action...\n",
    "    )  # ...then by a 1 hour window\n",
    "    .count()  # For each aggregate, produce a coun\n",
    ")\n",
    "\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)\n",
    "\n",
    "\n",
    "streamingQuery = (\n",
    "    countsDF.writeStream.queryName(  # Start with our \"streaming\" DataFrame  # Get the DataStreamWriter\n",
    "        \"stream_4p\"\n",
    "    )  # Name the query\n",
    "    .trigger(processingTime=\"1 seconds\")  # Configure for a 3-second micro-batch\n",
    "    .format(\"json\")  # Specify the sink type, a Parquet file\n",
    "    .option(\n",
    "        \"checkpointLocation\", \"./streaming/checkpointdir3/\"\n",
    "    )  # Specify the location of checkpoint files & W-A logs\n",
    "    .outputMode(\"append\")  # Write only new data to the \"file\"\n",
    "    .start(\n",
    "        \"./streaming/outputdir3/\"\n",
    "    )  # Start the job, writing to the specified directory\n",
    ")\n",
    "\n",
    "streamingQuery.awaitTermination(\n",
    "    6\n",
    ")  # Stream for another 5 seconds while the current thread blocks\n",
    "streamingQuery.stop()  # Stop the stream\n",
    "\n",
    "spark.read.json(\"./streaming/outputdir3/\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
