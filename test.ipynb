{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, col, window, current_timestamp, sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\n",
    "    \"\"\"\n",
    "    select current_timestamp() as date, 1 as value union select current_timestamp() as date, 2 as value;\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----+\n",
      "|date                   |value|\n",
      "+-----------------------+-----+\n",
      "|2024-01-19 12:56:09.791|1    |\n",
      "|2024-01-19 12:56:09.791|2    |\n",
      "+-----------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+---+\n",
      "|start                |end                  |sum|\n",
      "+---------------------+---------------------+---+\n",
      "|2024-01-19 12:56:07.1|2024-01-19 12:56:12.1|3  |\n",
      "|2024-01-19 12:56:09.1|2024-01-19 12:56:14.1|3  |\n",
      "|2024-01-19 12:56:06.1|2024-01-19 12:56:11.1|3  |\n",
      "|2024-01-19 12:56:08.1|2024-01-19 12:56:13.1|3  |\n",
      "|2024-01-19 12:56:05.1|2024-01-19 12:56:10.1|3  |\n",
      "+---------------------+---------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# w = df.groupBy(window(\"current_date\", \"5 seconds\")).agg(sum(\"val\").alias(\"sum\"))\n",
    "# w.select(\n",
    "#     w.window.start.cast(\"string\").alias(\"start\"),\n",
    "#     w.window.end.cast(\"string\").alias(\"end\"),\n",
    "#     \"sum\",\n",
    "# ).collect()\n",
    "\n",
    "\n",
    "df = df.groupBy(window(\"date\", \"5 seconds\", \"1 seconds\", \"0.1 seconds\")).agg(\n",
    "    sum(\"value\").alias(\"sum\")\n",
    ")\n",
    "\n",
    "df.select(\n",
    "    df.window.start.cast(\"string\").alias(\"start\"),\n",
    "    df.window.end.cast(\"string\").alias(\"end\"),\n",
    "    \"sum\",\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Time Windows in Spark Streaming\n",
    "\n",
    "In Spark Streaming, there are several types of time windows available for windowed computations. The available types of time windows are:\n",
    "\n",
    "1. **Tumbling Window**: Tumbling windows are a series of fixed-sized, non-overlapping, and contiguous time intervals. An input can only be bound to a single window[3][4].\n",
    "\n",
    "2. **Sliding Window**: Sliding windows allow you to apply transformations over a sliding window of data. The operation is applied over a specified time period of data and slides by a specified time interval[3].\n",
    "\n",
    "3. **Session Window**: Session windows are a new type of window introduced in Apache Spark 3.2, which works for both streaming and batch queries. They represent windows based on the activity or \"session\" of the data, allowing for more flexible window definitions[4][5].\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
